{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f35687f",
   "metadata": {},
   "source": [
    "In this work, you will test various time series classification methods. You must choose **three** datasets from [the UCR/UEA time series repository](http://timeseriesclassification.com) and perform the tasks by evaluating the models on three selected datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5da5c7",
   "metadata": {},
   "source": [
    "### Task 0: Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b137cc",
   "metadata": {},
   "source": [
    "You need to choose **three** datasets from the UCR/UEA time series repository. Please be careful since some UCR datasets can take a long time to be processed - You do not need to choose heavy datasets since it would slow your training/testing process. Use sktime's `load_UCR_UEA_dataset` function to perform. Please note that **you should use each dataset's original train/test splits** to train and report the test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2, L1\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sktime.datasets import load_UCR_UEA_dataset\n",
    "\n",
    "regularization_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef8034-4362-46ab-8659-e606542c9230",
   "metadata": {},
   "source": [
    "### Task 1: Time series classification using deep learning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28328ff-1ab7-4913-b4b5-d5aebc8f7ffe",
   "metadata": {},
   "source": [
    "Time series classification problems can be solved using networks like CNN, RNN, or FCN. You can even try to merge different networks. In this task, you must test your three chosen datasets on four other models.\n",
    "\n",
    "Try to implement four different models: **1) Fully connected network at least with five dense layers, 2) One directional RNN, 3) 1D-CNN only, and 4) 1D-CNN +\n",
    "GRU**. Note that you can always link the network to a fully connected layer to match the output size. You can freely construct any structure you want. **Report the average test scores of four models on three datasets you chose**. **`It would be four scores in total. Mark the best model in terms of the average test score`**. Briefly explain the structure you constructed. There is no definitive answer, and it is up to your own model. Here it would be best if you keep the following rules:\n",
    "\n",
    "- When initially loading the dataset, use sktime's `load_UCR_UEA_dataset` function. This is for our grading purpose.\n",
    "- You should use at least **two** Tensorflow callbacks when you fit your model. These can be built-in ones or your personalized callback.\n",
    "- You should use Tensorflow's data API (`tf.data`) to manage your dataset and use `shuffle`, `batch,` and `prefetch` functions. This means that you need to convert the data format using the `from_tensor_slices` function. This also means that you need to create your own validation set. You are not limited to using any methods to do this, but you may also need to shuffle the dataset before (for that, you can use `np.random.permutation`). If you use Torch, explain how you implement the equivalent operations.\n",
    "- You need to clearly report the **test accuracy** of the four models. Training and validation accuracy scores are not enough.\n",
    "- You may need to deal with datasets of different sizes. In this case, it might be helpful to make a function to create a model that can receive different input sizes as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b7cf3a-d04b-4a53-8e68-7a6fe31882b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "scheduler_callback = LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "es_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    "    start_from_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876fa23-5417-49e3-bf1a-5f872083ae52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3cbba0-e6c2-4c77-9a3a-0e738ea78367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_fully_connected(X_train , y_train, X_test, y_test):\n",
    "    \n",
    "    # Encode class values as integer\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # convert integer to dummy variables (i.e. one hot encoded)\n",
    "\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    \n",
    "\n",
    "    # number of classes\n",
    "    num_classes = y_train.shape[-1]\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "\n",
    "    num_classes =  y_train.shape[-1]\n",
    "    input_shape= X_train.shape[1]\n",
    "    input_shape = X_train.shape[1:]\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "    # create tensorflow dataset from training data\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    \n",
    "    \n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(len(X_train) * validation_split)\n",
    "    train_ds = train_ds.skip(num_validation_samples)\n",
    "    val_ds = train_ds.take(num_validation_samples)\n",
    "    \n",
    "    train_ds =(\n",
    "        train_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate = 0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=[es_callback, scheduler_callback])\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "    return test_acc\n",
    "\n",
    "def train_and_evaluate_1D_CNN(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # Encoding class values as Interger\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # converting integers into dummy varibles (i.e. one hot encoded)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    num_classes = y_train.shape[-1]\n",
    "    \n",
    "    # Add new dimension to the input data to create 3D input shape\n",
    "    X_train =np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    # creating Tensorflow dataset\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=len(X_train))\n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(len(X_train) * validation_split)\n",
    "    train_ds = train_data.skip(num_validation_samples)\n",
    "    val_ds = train_data.take(num_validation_samples)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "    batch_size = 32\n",
    "    train_ds =(\n",
    "        train_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv1D(filters=6, kernel_size=3, activation='relu', padding='same' , input_shape=input_shape, kernel_regularizer=L2(regularization_rate)),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(filters=16, kernel_size=3, activation='relu',padding='same', kernel_regularizer=L2(regularization_rate)),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(filters=32, kernel_size=3, activation='relu',padding='same', kernel_regularizer=L2(regularization_rate)),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu',padding='same', kernel_regularizer=L2(regularization_rate)),\n",
    "        \n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_ds, epochs=10, batch_size=32, validation_data=val_ds, callbacks=[es_callback, scheduler_callback])\n",
    "\n",
    "    # evaluating model\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a2a806-e793-4920-84fd-2efa64df895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_1d_RNN(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # encode target values as integer\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # converting integers to dummy variables(i.e. one hot encoded)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    # numberof classes\n",
    "    num_classes =  y_train.shape[-1]\n",
    "    input_shape=(X_train.shape[1], X_train.shape[-1])\n",
    "\n",
    "    # add a new dimention \n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "    input_shape=(X_train.shape[1], X_train.shape[-1])\n",
    "    \n",
    "    # creating Tensorflow dataset\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=len(X_train))\n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(len(X_train) * validation_split)\n",
    "    train_ds = train_data.skip(num_validation_samples)\n",
    "    val_ds = train_data.take(num_validation_samples)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "    batch_size = 32\n",
    "    train_ds =(\n",
    "        train_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.SimpleRNN(units=64, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(units=64, activation='relu'),\n",
    "        layers.Dense(units=128, activation='relu'),\n",
    "        layers.Dense(units=num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # build model \n",
    "    model.build(input_shape=X_train.shape)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_ds, epochs=10, batch_size=32, validation_data=val_ds, callbacks=[es_callback, scheduler_callback])\n",
    "\n",
    "    #evaluate model\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "    return test_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165aaa91-082b-4a80-9025-c49dbcd268a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_1D_CNN_and_gru(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # encode target values as integer\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # converting integers to dummy variables(i.e. one hot encoded)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    # numberof classes\n",
    "    num_classes =  y_train.shape[-1]\n",
    "\n",
    "    # add a new dimention \n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    # creating Tensorflow dataset\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=len(X_train))\n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(len(X_train) * validation_split)\n",
    "    train_ds = train_data.skip(num_validation_samples)\n",
    "    val_ds = train_data.take(num_validation_samples)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "    batch_size = 32\n",
    "    train_ds =(\n",
    "        train_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=X_train.shape[1:], kernel_regularizer=L2(regularization_rate)),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.GRU(32, return_sequences=True , kernel_regularizer=L2(regularization_rate)),\n",
    "        layers.GRU(32 , kernel_regularizer=L2(regularization_rate)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_ds, epochs=10, batch_size=32, validation_data=val_ds, callbacks=[es_callback, scheduler_callback])\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99afc5e2-1a76-47dc-ad80-1707f5ceab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 2.0327 - accuracy: 0.3125 - val_loss: 3.2741 - val_accuracy: 0.1250 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.0488 - accuracy: 0.2812 - val_loss: 4.3741 - val_accuracy: 0.3750 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.7998 - accuracy: 0.1875 - val_loss: 2.4778 - val_accuracy: 0.2500 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.2833 - accuracy: 0.4062 - val_loss: 1.5834 - val_accuracy: 0.6250 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.5683 - accuracy: 0.6250 - val_loss: 2.1597 - val_accuracy: 0.3750 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.8236 - accuracy: 0.5000 - val_loss: 1.3337 - val_accuracy: 0.7500 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6543 - accuracy: 0.4688 - val_loss: 1.4665 - val_accuracy: 0.6250 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.4301 - accuracy: 0.5938 - val_loss: 1.4265 - val_accuracy: 0.6250 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.3003 - accuracy: 0.7500 - val_loss: 1.1064 - val_accuracy: 0.8750 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.2262 - accuracy: 0.7500 - val_loss: 1.2797 - val_accuracy: 0.6250 - lr: 0.0050\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.1815 - accuracy: 0.7750\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4849 - accuracy: 0.3125 - val_loss: 1.0685 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 1.1654 - accuracy: 0.4688 - val_loss: 0.8133 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 1.0058 - accuracy: 0.3750 - val_loss: 1.0422 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 1.0153 - accuracy: 0.3750 - val_loss: 1.0249 - val_accuracy: 0.3750 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.9138 - accuracy: 0.4375 - val_loss: 0.7812 - val_accuracy: 0.6250 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.8210 - accuracy: 0.6562 - val_loss: 0.5401 - val_accuracy: 0.8750 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.7886 - accuracy: 0.7188 - val_loss: 0.5565 - val_accuracy: 0.8750 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.6831 - accuracy: 0.7188 - val_loss: 0.6506 - val_accuracy: 0.7500 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.6979 - accuracy: 0.8125 - val_loss: 0.6528 - val_accuracy: 0.8750 - lr: 0.0055\n",
      "Epoch 9: early stopping\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.0995 - accuracy: 0.6500\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4551 - accuracy: 0.2188 - val_loss: 0.8003 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.3863 - accuracy: 0.3125 - val_loss: 0.5169 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.6880 - accuracy: 0.4062 - val_loss: 5.7139 - val_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.4461 - accuracy: 0.5938 - val_loss: 1.5415 - val_accuracy: 0.5000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8485 - accuracy: 0.7500 - val_loss: 0.4864 - val_accuracy: 0.7500 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4934 - accuracy: 0.8750 - val_loss: 0.4366 - val_accuracy: 0.7500 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.4700 - accuracy: 0.8438 - val_loss: 0.4333 - val_accuracy: 0.7500 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4102 - accuracy: 0.8438 - val_loss: 0.4384 - val_accuracy: 0.7500 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3301 - accuracy: 0.9062 - val_loss: 0.4454 - val_accuracy: 0.7500 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2564 - accuracy: 0.9375 - val_loss: 0.4508 - val_accuracy: 0.7500 - lr: 0.0050\n",
      "Epoch 10: early stopping\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.1301 - accuracy: 0.6000\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.3407 - accuracy: 0.2188 - val_loss: 1.9965 - val_accuracy: 0.3750 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 2.0306 - accuracy: 0.3438 - val_loss: 1.8374 - val_accuracy: 0.3750 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 1.9553 - accuracy: 0.3438 - val_loss: 1.6896 - val_accuracy: 0.6250 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.6707 - accuracy: 0.7500 - val_loss: 1.6334 - val_accuracy: 0.6250 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 576ms/step - loss: 1.5744 - accuracy: 0.7500 - val_loss: 1.2685 - val_accuracy: 1.0000 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 1.3946 - accuracy: 0.9375 - val_loss: 1.4930 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 638ms/step - loss: 1.2765 - accuracy: 0.9062 - val_loss: 1.1064 - val_accuracy: 0.8750 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 687ms/step - loss: 1.1748 - accuracy: 0.9062 - val_loss: 0.7578 - val_accuracy: 1.0000 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 0.9704 - accuracy: 0.9375 - val_loss: 0.9280 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 0.9473 - accuracy: 0.9375 - val_loss: 0.8264 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.8759 - accuracy: 1.0000\n",
      "Test accuracy on BasicMotions using 1D CNN: 0.7749999761581421\n",
      "Test accuracy on BasicMotions using one directional RNN: 0.6499999761581421\n",
      "Test accuracy on BasicMotions using fully connected network: 0.6000000238418579\n",
      "Test accuracy on BasicMotions using CNN-GRU: 1.0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 4s 575ms/step - loss: 1.3889 - accuracy: 0.4630 - val_loss: 1.2010 - val_accuracy: 0.9231 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 1.1952 - accuracy: 0.5741 - val_loss: 1.0079 - val_accuracy: 0.9231 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.0134 - accuracy: 0.7222 - val_loss: 0.9107 - val_accuracy: 0.6923 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.8711 - accuracy: 0.7593 - val_loss: 0.7128 - val_accuracy: 1.0000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.7002 - accuracy: 1.0000 - val_loss: 0.5842 - val_accuracy: 0.9231 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5676 - accuracy: 0.9259 - val_loss: 0.3847 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4238 - accuracy: 1.0000 - val_loss: 0.4408 - val_accuracy: 0.9231 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3593 - accuracy: 0.9815 - val_loss: 0.2742 - val_accuracy: 1.0000 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.2934 - accuracy: 0.9444 - val_loss: 0.2796 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.2939 - accuracy: 0.9815 - val_loss: 0.2143 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.5190 - accuracy: 0.8465\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 5s 645ms/step - loss: 0.6688 - accuracy: 0.6481 - val_loss: 0.5034 - val_accuracy: 0.8462 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5291 - accuracy: 0.7407 - val_loss: 0.3944 - val_accuracy: 0.6923 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3007 - accuracy: 0.8519 - val_loss: 0.1478 - val_accuracy: 0.9231 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.2145 - accuracy: 0.9630 - val_loss: 0.0244 - val_accuracy: 1.0000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0622 - accuracy: 0.9815 - val_loss: 0.4560 - val_accuracy: 0.9231 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3616 - accuracy: 0.9630 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0541 - accuracy: 0.9815 - val_loss: 9.5760e-04 - val_accuracy: 1.0000 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1138 - accuracy: 0.9815 - val_loss: 0.3603 - val_accuracy: 0.9231 - lr: 0.0050\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.1477 - accuracy: 0.9504\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 3s 395ms/step - loss: 0.6430 - accuracy: 0.6111 - val_loss: 0.8797 - val_accuracy: 0.5385 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5966 - accuracy: 0.6667 - val_loss: 0.1464 - val_accuracy: 0.9231 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1222 - accuracy: 0.9815 - val_loss: 0.0333 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 9.7253e-04 - val_accuracy: 1.0000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 7.1788e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.3454e-04 - val_accuracy: 1.0000 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 7.3903e-05 - accuracy: 1.0000 - val_loss: 1.9053e-05 - val_accuracy: 1.0000 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 6.4831e-06 - accuracy: 1.0000 - val_loss: 2.5675e-06 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 9.6249e-07 - accuracy: 1.0000 - val_loss: 5.6853e-07 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.004493290092796087.\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2.2959e-07 - accuracy: 1.0000 - val_loss: 1.7423e-07 - val_accuracy: 1.0000 - lr: 0.0045\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.004065697081387043.\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 7.2850e-08 - accuracy: 1.0000 - val_loss: 7.3360e-08 - val_accuracy: 1.0000 - lr: 0.0041\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.003678794950246811.\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 3.0906e-08 - accuracy: 1.0000 - val_loss: 3.6680e-08 - val_accuracy: 1.0000 - lr: 0.0037\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00332871126011014.\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 1.5453e-08 - accuracy: 1.0000 - val_loss: 1.8340e-08 - val_accuracy: 1.0000 - lr: 0.0033\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00301194260828197.\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 8.8303e-09 - accuracy: 1.0000 - val_loss: 9.1699e-09 - val_accuracy: 1.0000 - lr: 0.0030\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.002725318307057023.\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.4152e-09 - accuracy: 1.0000 - val_loss: 9.1699e-09 - val_accuracy: 1.0000 - lr: 0.0027\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.002465970115736127.\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.4152e-09 - accuracy: 1.0000 - val_loss: 9.1699e-09 - val_accuracy: 1.0000 - lr: 0.0025\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.002231301972642541.\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.4152e-09 - accuracy: 1.0000 - val_loss: 9.1699e-09 - val_accuracy: 1.0000 - lr: 0.0022\n",
      "Epoch 18: early stopping\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5128 - accuracy: 0.9388\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 14s 2s/step - loss: 1.6399 - accuracy: 0.3148 - val_loss: 1.3953 - val_accuracy: 0.6923 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.4059 - accuracy: 0.4630 - val_loss: 1.2485 - val_accuracy: 0.4615 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.1953 - accuracy: 0.6481 - val_loss: 1.0504 - val_accuracy: 0.7692 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.0136 - accuracy: 0.7222 - val_loss: 0.9904 - val_accuracy: 0.3846 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.9109 - accuracy: 0.5741 - val_loss: 0.7826 - val_accuracy: 0.9231 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7474 - accuracy: 0.9074 - val_loss: 0.7320 - val_accuracy: 0.7692 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6970 - accuracy: 0.7593 - val_loss: 0.5428 - val_accuracy: 0.9231 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5781 - accuracy: 0.8519 - val_loss: 0.6283 - val_accuracy: 0.8462 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4465 - accuracy: 0.9074 - val_loss: 0.4191 - val_accuracy: 0.9231 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3351 - accuracy: 0.9444 - val_loss: 0.2561 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3630 - accuracy: 0.9291\n",
      "Test accuracy on ItalyPowerDemand using 1D CNN: 0.8464528918266296\n",
      "Test accuracy on ItalyPowerDemand using one directional RNN: 0.9504373073577881\n",
      "Test accuracy on ItalyPowerDemand using fully connected network: 0.9387755393981934\n",
      "Test accuracy on ItalyPowerDemand using CNN-GRU: 0.9290573596954346\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.8478 - accuracy: 0.4167 - val_loss: 1.2256 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.5652 - accuracy: 0.6667 - val_loss: 1.9622 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.4779 - accuracy: 0.5000 - val_loss: 1.1845 - val_accuracy: 0.6667 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.1251 - accuracy: 0.7500 - val_loss: 0.4924 - val_accuracy: 1.0000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9677 - accuracy: 0.6667 - val_loss: 1.0413 - val_accuracy: 0.3333 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7856 - accuracy: 0.7500 - val_loss: 0.7683 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.5932 - accuracy: 1.0000 - val_loss: 0.5332 - val_accuracy: 1.0000 - lr: 0.0067\n",
      "Epoch 7: early stopping\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.1708 - accuracy: 0.2667\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.0972 - accuracy: 0.2500 - val_loss: 1.1094 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 1.1014 - accuracy: 0.2500 - val_loss: 1.1605 - val_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 1.0276 - accuracy: 0.4167 - val_loss: 1.0573 - val_accuracy: 0.6667 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.9856 - accuracy: 0.5000 - val_loss: 1.0921 - val_accuracy: 0.0000e+00 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 1.0356 - accuracy: 0.3333 - val_loss: 1.1027 - val_accuracy: 0.3333 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 0.9905 - accuracy: 0.4167 - val_loss: 1.1155 - val_accuracy: 0.3333 - lr: 0.0074\n",
      "Epoch 6: early stopping\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 1.0898 - accuracy: 0.4000\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0980 - accuracy: 0.5000 - val_loss: 1.1994 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9534 - accuracy: 0.6667 - val_loss: 1.1085 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6008 - accuracy: 0.7500 - val_loss: 0.8802 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3634 - accuracy: 0.7500 - val_loss: 0.4818 - val_accuracy: 1.0000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1962 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 1.0000 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1157 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 4.6981e-04 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 9.6226e-04 - accuracy: 1.0000 - val_loss: 6.9909e-04 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.004493290092796087.\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6920e-04 - accuracy: 1.0000 - val_loss: 9.6101e-04 - val_accuracy: 1.0000 - lr: 0.0045\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.004065697081387043.\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5985e-04 - accuracy: 1.0000 - val_loss: 7.3626e-04 - val_accuracy: 1.0000 - lr: 0.0041\n",
      "Epoch 12: early stopping\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.9744 - accuracy: 0.2667\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.0865 - accuracy: 0.3333 - val_loss: 1.9653 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 980ms/step - loss: 1.9395 - accuracy: 0.4167 - val_loss: 1.8503 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8346 - accuracy: 0.3333 - val_loss: 1.6449 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7693 - accuracy: 0.2500 - val_loss: 1.6098 - val_accuracy: 0.3333 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6281 - accuracy: 0.3333 - val_loss: 1.4670 - val_accuracy: 0.3333 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5507 - accuracy: 0.5833 - val_loss: 1.5309 - val_accuracy: 0.3333 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4906 - accuracy: 0.5833 - val_loss: 1.5219 - val_accuracy: 0.0000e+00 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4796 - accuracy: 0.5000 - val_loss: 1.4506 - val_accuracy: 0.6667 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4065 - accuracy: 0.3333 - val_loss: 1.1897 - val_accuracy: 0.6667 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3453 - accuracy: 0.6667 - val_loss: 1.3494 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.4306 - accuracy: 0.2667\n",
      "Test accuracy on AtrialFibrillation using 1D CNN: 0.2666666805744171\n",
      "Test accuracy on AtrialFibrillation using one directional RNN: 0.4000000059604645\n",
      "Test accuracy on AtrialFibrillation using fully connected network: 0.2666666805744171\n",
      "Test accuracy on AtrialFibrillation using CNN-GRU: 0.2666666805744171\n",
      "Average test accuracy for 1D CNN: 0.629373182853063\n",
      "Average test accuracy for RNN: 0.6668124298254648\n",
      "Average test accuracy for fully connected network: 0.6018140812714895\n",
      "Average test accuracy for CNN-GRU: 0.7319080134232839\n",
      "The best model is CNN-gru with an average test accuracy of 0.7319080134232839\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"BasicMotions\", \"ItalyPowerDemand\", \"AtrialFibrillation\"]\n",
    "def train_and_evaluate_on_datasets(dataset_names):\n",
    "    fc_test_scores = []\n",
    "    cnn_test_scores = []\n",
    "    rnn_test_scores = []\n",
    "    cnn_gru_test_scores = []\n",
    "    for dataset_name in dataset_names:\n",
    "        # Load dataset\n",
    "        X_train, y_train = load_UCR_UEA_dataset(name=dataset_name, split='train',  return_type=\"numpy2D\", extract_path=None)\n",
    "        X_test, y_test = load_UCR_UEA_dataset(name=dataset_name, split='test',  return_type=\"numpy2D\", extract_path=None)\n",
    "\n",
    "        # Train and evaluate 1D CNN model\n",
    "        cnn_test_acc = train_and_evaluate_1D_CNN(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        # Train and evaluate rnn model\n",
    "        rnn_test_acc = train_and_evaluate_1d_RNN(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        # Train and evaluate fully connected network model\n",
    "        fc_test_acc = train_and_evaluate_fully_connected(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        # Train and evaluate CNN-LSTM model\n",
    "        cnn_gru_test_acc = train_and_evaluate_1D_CNN_and_gru(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        print(f'Test accuracy on {dataset_name} using 1D CNN:', cnn_test_acc)\n",
    "        print(f'Test accuracy on {dataset_name} using one directional RNN:', rnn_test_acc)\n",
    "        print(f'Test accuracy on {dataset_name} using fully connected network:', fc_test_acc)\n",
    "        print(f'Test accuracy on {dataset_name} using CNN-GRU:', cnn_gru_test_acc)\n",
    "        cnn_test_scores.append(cnn_test_acc)\n",
    "        rnn_test_scores.append(rnn_test_acc)\n",
    "        fc_test_scores.append(fc_test_acc)\n",
    "        cnn_gru_test_scores.append(cnn_gru_test_acc)\n",
    "\n",
    "    # Calculate the average test score for each model\n",
    "    cnn_average_test_score = np.mean(cnn_test_scores)\n",
    "    rnn_average_test_score = np.mean(rnn_test_scores)\n",
    "    fc_average_test_score = np.mean(fc_test_scores)\n",
    "    cnn_gru_average_test_score = np.mean(cnn_gru_test_scores)\n",
    "    \n",
    "    # Determine the best model\n",
    "    models = ['1D CNN', 'RNN', 'Fully Connected Network', 'CNN-gru']\n",
    "    scores = [cnn_average_test_score, rnn_average_test_score, fc_average_test_score, cnn_gru_average_test_score]\n",
    "    best_model = models[np.argmax(scores)]\n",
    "    best_score = np.max(scores)\n",
    "\n",
    "    print(f'Average test accuracy for 1D CNN: {cnn_average_test_score}')\n",
    "    print(f'Average test accuracy for RNN: {rnn_average_test_score}')\n",
    "    print(f'Average test accuracy for fully connected network: {fc_average_test_score}')\n",
    "    print(f'Average test accuracy for CNN-GRU: {cnn_gru_average_test_score}')\n",
    "    print(f'The best model is {best_model} with an average test accuracy of {best_score}')\n",
    "\n",
    "train_and_evaluate_on_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd45ec-ecfb-4dfc-ab9b-6e265d995a21",
   "metadata": {},
   "source": [
    "### Task 2: Time series classification using deep learning 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee355a-5271-48fa-bfcc-3a2b9679c981",
   "metadata": {},
   "source": [
    "There has been several neural network models dedicated to time series classification. Besides your own models that you developed in Lab 4, now you will develop such dedicated models by referring to some papers, and test if they indeed perform better than your rough models. There are two famous papers as follows:\n",
    " - [Convolutional neural networks for time series classification (2017)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870510)\n",
    " - [Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline (2017)](https://arxiv.org/abs/1611.06455)\n",
    "\n",
    "First paper's idea is already implemented in sktime, with the name `CNNClassifier`. Second paper has three models and those are easy to develop using tensorflow. Now the task is to develop two models (MLP and FCN) in the second paper and test it together with `CNNClassifier`.\n",
    "\n",
    "Use the same four datasets, and test sktime's `CNNClassifier` and MLP and FCN models you develop. Report test scores of three models (`CNNClassifier`, MLP, and FCN) on three datasets you chose. It would be nine scores in total. For MLP and FCN, you may need to satisfy the following requirement:\n",
    "\n",
    "- You should use at least **two** Tensorflow callbacks when you fit your model. These can be built-in ones or your personalized callback. If you use Torch, explain how you implement the equivalent operations.\n",
    "- You should run the model at least 10 epochs.\n",
    "- You can use the same processed datasets in Task 1. For `CNNClassifier`, as you cannot use `tf.Data`, you may put the training set directly.\n",
    "- For `CNNClassifier`, you can run it with the default parameters or reduce the number of epoch (default is 2000).\n",
    "- Please use the predefined test dataset to report the test scores.\n",
    "\n",
    "Note that the main purpose of this task is to check if you can develop a similar network structure with description. If the detail of the specific part (e.g., size of one layer or some custom parameters like epoch) is missing in the paper, you can set it on your own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3949f5af-01b7-4219-828c-845fe4b36b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # encoding class values\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # converting into dummy data\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    num_classes = y_train.shape[-1]\n",
    "    batch_size = 32\n",
    "\n",
    "    # creating Tensorflow dataset\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=len(X_train))\n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(len(X_train) * validation_split)\n",
    "    train_ds = train_data.skip(num_validation_samples)\n",
    "    val_ds = train_data.take(num_validation_samples)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "\n",
    "    batch_size = 32\n",
    "    train_ds =(\n",
    "        train_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    input_shape= input_shape = X_train.shape[1:]\n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        layers.Dropout(0.1),\n",
    "\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_ds, epochs=10, batch_size=32, validation_data=val_ds, callbacks=[es_callback, scheduler_callback])\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddc71d-0ef8-49cb-8000-ba3845932e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0640e553-d354-4108-9c9e-993c4f5f4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_fcn(X_train, y_train, X_test, y_test):\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # converting into dummy data\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    num_classes = y_train.shape[-1]\n",
    "    batch_size = 32\n",
    "\n",
    "    # creating Tensorflow dataset\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=len(X_train))\n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(len(X_train) * validation_split)\n",
    "    train_ds = train_data.skip(num_validation_samples)\n",
    "    val_ds = train_data.take(num_validation_samples)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "    batch_size = 32\n",
    "    train_ds =(\n",
    "        train_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    input_shape = X_train.shape[1:]\n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Conv1D(filters=128, kernel_size=8, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Conv1D(filters=256, kernel_size=5, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(units=num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001) , loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_ds, epochs=10, batch_size=32, validation_data=val_ds, callbacks=[es_callback, scheduler_callback])\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c4f962-1490-430e-a738-65487c631fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_evaluate_CNNclassifier(X_train, y_train, X_test, y_test):\n",
    "    # enocoding the data\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # creating cnn classifer with reduced epochs\n",
    "    cnn_classifier = CNNClassifier(n_epochs=50)\n",
    "\n",
    "    cnn_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # prediction \n",
    "    y_pred = cnn_classifier.predict(X_test)\n",
    "\n",
    "    # evaluationg the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "586e8b0e-d342-4b77-b999-b18649f25809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 7.1471 - accuracy: 0.3125 - val_loss: 29.9361 - val_accuracy: 0.1250 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 20.9136 - accuracy: 0.1875 - val_loss: 164.5700 - val_accuracy: 0.2500 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 167.5061 - accuracy: 0.2188 - val_loss: 19.8928 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 47.2762 - accuracy: 0.3750 - val_loss: 4.9289 - val_accuracy: 0.5000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.5822 - accuracy: 0.3438 - val_loss: 0.8300 - val_accuracy: 0.6250 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.8986 - accuracy: 0.6875 - val_loss: 0.8221 - val_accuracy: 0.5000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.7913 - accuracy: 0.6562 - val_loss: 0.6357 - val_accuracy: 0.7500 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7703 - accuracy: 0.7812 - val_loss: 0.4903 - val_accuracy: 0.8750 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4971 - accuracy: 0.8125 - val_loss: 0.4357 - val_accuracy: 0.7500 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3687 - accuracy: 0.8750 - val_loss: 0.0899 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7538 - accuracy: 0.6750\n",
      "3/3 [==============================] - 0s 10ms/step\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.7541 - accuracy: 0.0312 - val_loss: 1.3657 - val_accuracy: 0.1250 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 0.6494 - accuracy: 0.6250 - val_loss: 1.4450 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 0.2625 - accuracy: 1.0000 - val_loss: 1.1675 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 0.2180 - accuracy: 1.0000 - val_loss: 1.0765 - val_accuracy: 0.3750 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 589ms/step - loss: 0.1040 - accuracy: 1.0000 - val_loss: 1.0705 - val_accuracy: 0.3750 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0007408182718791068.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 589ms/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 1.3414 - val_accuracy: 0.1250 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 619ms/step - loss: 0.0914 - accuracy: 1.0000 - val_loss: 1.1157 - val_accuracy: 0.3750 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0006065307534299791.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.0744 - accuracy: 1.0000 - val_loss: 1.1870 - val_accuracy: 0.3750 - lr: 6.0653e-04\n",
      "Epoch 8: early stopping\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.2355 - accuracy: 0.2500\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 4s 676ms/step - loss: 1.3116 - accuracy: 0.4259 - val_loss: 12.4867 - val_accuracy: 0.3846 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 7.7237 - accuracy: 0.4815 - val_loss: 0.5979 - val_accuracy: 0.6154 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5788 - accuracy: 0.6111 - val_loss: 0.5332 - val_accuracy: 0.5385 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6720 - accuracy: 0.5185 - val_loss: 0.4252 - val_accuracy: 0.6154 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5158 - accuracy: 0.7407 - val_loss: 0.4304 - val_accuracy: 0.9231 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3176 - accuracy: 0.9630 - val_loss: 0.2673 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.2299 - accuracy: 0.9259 - val_loss: 0.1501 - val_accuracy: 1.0000 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.1481 - accuracy: 0.9630 - val_loss: 0.0348 - val_accuracy: 1.0000 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.1144 - accuracy: 0.9815 - val_loss: 0.0808 - val_accuracy: 0.9231 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0517 - accuracy: 0.9815 - val_loss: 0.0020 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.1174 - accuracy: 0.9708\n",
      "65/65 [==============================] - 1s 4ms/step\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 5s 625ms/step - loss: 0.8174 - accuracy: 0.6111 - val_loss: 0.6849 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.1596 - accuracy: 0.9259 - val_loss: 0.6710 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.1626 - accuracy: 0.9630 - val_loss: 0.6609 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1401 - accuracy: 0.9630 - val_loss: 0.6619 - val_accuracy: 0.4615 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0008187307976186275.\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0898 - accuracy: 0.9630 - val_loss: 0.6795 - val_accuracy: 0.3846 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0007408182718791068.\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 0.6280 - val_accuracy: 0.6154 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0681 - accuracy: 0.9630 - val_loss: 0.6049 - val_accuracy: 0.6154 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0006065307534299791.\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 0.6527 - val_accuracy: 0.3846 - lr: 6.0653e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0005488117458298802.\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0718 - accuracy: 0.9630 - val_loss: 0.6389 - val_accuracy: 0.5385 - lr: 5.4881e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0004965853877365589.\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0552 - accuracy: 0.9815 - val_loss: 0.5688 - val_accuracy: 0.7692 - lr: 4.9659e-04\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.6305 - accuracy: 0.5199\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0995 - accuracy: 0.3333 - val_loss: 0.1540 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6331 - accuracy: 0.6667 - val_loss: 2.8321 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.1458 - accuracy: 0.5000 - val_loss: 0.3363 - val_accuracy: 0.6667 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.1173 - accuracy: 0.9167 - val_loss: 0.4100 - val_accuracy: 0.6667 - lr: 0.0090\n",
      "Epoch 4: early stopping\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 10.1246 - accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.2522 - accuracy: 0.0833 - val_loss: 1.1012 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 0.8597 - accuracy: 0.5833 - val_loss: 1.1008 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 0.7748 - accuracy: 0.6667 - val_loss: 1.0868 - val_accuracy: 0.6667 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 464ms/step - loss: 0.6027 - accuracy: 0.6667 - val_loss: 1.0939 - val_accuracy: 0.3333 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 438ms/step - loss: 0.6591 - accuracy: 0.7500 - val_loss: 1.1004 - val_accuracy: 0.3333 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0007408182718791068.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 483ms/step - loss: 0.5716 - accuracy: 0.8333 - val_loss: 1.1011 - val_accuracy: 0.3333 - lr: 7.4082e-04\n",
      "Epoch 6: early stopping\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 1.1014 - accuracy: 0.4000\n",
      "Test accuracy of (CNNClassifier) on BasicMotions : 1.0\n",
      "Test accuracy of (MLP) on BasicMotions : 0.675000011920929\n",
      "Test accuracy of (FCn) on BasicMotions : 0.25\n",
      "Test accuracy of (CNNClassifier) on ItalyPowerDemand : 0.9387755102040817\n",
      "Test accuracy of (MLP) on ItalyPowerDemand : 0.9708454608917236\n",
      "Test accuracy of (FCn) on ItalyPowerDemand : 0.5199222564697266\n",
      "Test accuracy of (CNNClassifier) on AtrialFibrillation : 0.4\n",
      "Test accuracy of (MLP) on AtrialFibrillation : 0.3333333432674408\n",
      "Test accuracy of (FCn) on AtrialFibrillation : 0.4000000059604645\n",
      "Rank 1: CNNClassifier - Average accuracy score: 0.7795918367346939\n",
      "Rank 2: MLPClassifier - Average accuracy score: 0.6597262720266978\n",
      "Rank 3: FCN model - Average accuracy score: 0.38997408747673035\n"
     ]
    }
   ],
   "source": [
    "mlp_scores = []\n",
    "cnn_scores = []\n",
    "fcn_scores = []\n",
    "for dataset in datasets:\n",
    "    X_train, y_train = load_UCR_UEA_dataset(name=dataset, split='train',  return_type=\"numpy2D\", extract_path=None)\n",
    "    X_test, y_test = load_UCR_UEA_dataset(name=dataset, split='test',  return_type=\"numpy2D\", extract_path=None)\n",
    "\n",
    "    # mlp classifier\n",
    "    mlp_accuracy = mlp(X_train, y_train, X_test, y_test)\n",
    "    mlp_scores.append(mlp_accuracy)\n",
    "\n",
    "    # cnn\n",
    "    cnn_accuracy = train_and_evaluate_CNNclassifier(X_train, y_train, X_test, y_test)\n",
    "    cnn_scores.append(cnn_accuracy)\n",
    "\n",
    "    # fcn\n",
    "    fcn_accuracy = train_and_evaluate_fcn(X_train, y_train, X_test, y_test)\n",
    "    fcn_scores.append(fcn_accuracy)\n",
    "cnn_average = np.mean(cnn_scores)\n",
    "fcn_average = np.mean(fcn_scores)\n",
    "mlp_average = np.mean(mlp_scores)\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    print(\"Test accuracy of (CNNClassifier) on {} : {}\".format(datasets[i], cnn_scores[i]))\n",
    "    print(\"Test accuracy of (MLP) on {} : {}\".format(datasets[i], mlp_scores[i]))\n",
    "    print(\"Test accuracy of (FCn) on {} : {}\".format(datasets[i], fcn_scores[i]))\n",
    "\n",
    "# Report the rank of average accuracy scores\n",
    "scores = [(cnn_average, \"CNNClassifier\"), (mlp_average, \"MLPClassifier\"), (fcn_average, \"FCN model\")]\n",
    "scores.sort(reverse=True)  # Sort in descending order\n",
    "for rank, (score, model) in enumerate(scores, 1):\n",
    "    print(f\"Rank {rank}: {model} - Average accuracy score: {score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2da517-fb02-4f00-bfce-c15b40227440",
   "metadata": {},
   "source": [
    "### Task 3: Time series classification using deep learning 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f566a0-4640-479e-b70a-b8a4d7c36ec2",
   "metadata": {},
   "source": [
    "Next, you can try to further improve your model by selecting **two** of the following ideas:\n",
    "- `Use Bi-Direction LSTM and CNN networks separately`, create two to three layers individually, and concatenate them. This means that until the third (or second) layer, you have two different networks handling the same dataset, and after that, you concatenate the output and finish with any FCN layer. Check [this post](https://stackoverflow.com/questions/59168306/how-to-combine-lstm-and-cnn-in-timeseries-classification) to get inspired.\n",
    "- Apply any sktime's transformer (not attention transformer) first to the dataset and run any deep learning model you already developed in Tasks 2 and 3. In this case, you need to choose at least two transformers and apply them together.\n",
    "- Train the model on multiple similar datasets and test it on one specific test set. Check if the model can be improved if it is trained on multiple datasets (at least five datasets). However, for this, you also need to choose the similar datasets based on their classification and motivate your choise in the report (UCR repository has a specific dataset type such as **AUDIO** or **MOTION**). You could try to crop or pad the time series if you would like to match the sizes.\n",
    "\n",
    "Choose one model you want from the models you have developed in Tasks 1 and 2. Select one idea, try implementing it, and check if you can improve the performance. Note that you do not need to prove that the accuracy scores increase but must explain your trials. Report test scores on three datasets you chose.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a03d0a-6bb1-4e7a-9a3e-9e8fddc282c1",
   "metadata": {},
   "source": [
    "**Answer:** I have implemented first and second approach seperately. and show the trials below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d73db6-c697-46a1-a067-4a879a5da874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sktime.transformations.series.difference import Differencer\n",
    "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "\n",
    "\n",
    "from sktime.transformations.series.exponent import ExponentTransformer\n",
    "\n",
    "\n",
    "def transform_x_by_transformer(X_train, X_test):\n",
    "    exponent = ExponentTransformer(power=2)\n",
    "    X_train  = exponent.fit_transform(X_train)\n",
    "    X_test = exponent.fit_transform(X_test)\n",
    "    \n",
    "    differencer = Differencer()\n",
    "    X_train = differencer.fit_transform(X_train)\n",
    "    X_test = differencer.fit_transform(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def train_and_evaluate_combined_cnn_lstm(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Encoding\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # converting into dummy variables(i.e one hot encoded)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    num_classes = y_train.shape[-1]\n",
    "    batch_size = 32\n",
    "    \n",
    "    # creating Tensorflow dataset\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=len(X_train))\n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(len(X_train) * validation_split)\n",
    "    train_ds = train_data.skip(num_validation_samples)\n",
    "    val_ds = train_data.take(num_validation_samples)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "    train_ds =(\n",
    "        train_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    input_shape=  (X_train.shape[1], X_train.shape[-1])\n",
    "\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "\n",
    "\n",
    "    # LSTM Network functional model\n",
    "    lstm = layers.Bidirectional(layers.LSTM(64, return_sequences=True, activation='tanh'))(input_layer)\n",
    "    lstm = layers.Bidirectional(layers.LSTM(32, return_sequences=True, activation='tanh'))(lstm)\n",
    "    lstm = layers.Bidirectional(layers.LSTM(16, return_sequences=True, activation='tanh'))(lstm)\n",
    "    lstm = layers.Bidirectional(layers.LSTM(8, activation='tanh'))(lstm)\n",
    "\n",
    "    # CNN Network functional model\n",
    "    cnn = layers.Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=L2(regularization_rate))(input_layer)\n",
    "    cnn = layers.MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = layers.Conv1D(filters=32, kernel_size=3, activation='relu', kernel_regularizer=L2(regularization_rate))(cnn)\n",
    "    cnn = layers.MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = layers.Conv1D(filters=16, kernel_size=3, activation='relu', kernel_regularizer=L2(regularization_rate))(cnn)\n",
    "    cnn = layers.MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = layers.Flatten()(cnn)\n",
    "    \n",
    "\n",
    "    output = layers.concatenate([lstm, cnn])\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(output)\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    combined_model =tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    combined_model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # model\n",
    "    combined_model.fit(train_ds, epochs=10, batch_size=32, validation_data=val_ds, callbacks=[es_callback, scheduler_callback])\n",
    "\n",
    "    # Evaluate model on test \n",
    "    test_loss, test_acc = combined_model.evaluate(test_ds)\n",
    "\n",
    "\n",
    "    return test_acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0408c1a5-f2a7-40ac-880a-5739d8bbfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## second ideas\n",
    "def transform_x_by_transformer(X_train, X_test):\n",
    "    exponent = ExponentTransformer(power=2)\n",
    "    X_train  = exponent.fit_transform(X_train)\n",
    "    X_test = exponent.fit_transform(X_test)\n",
    "    \n",
    "    differencer = Differencer()\n",
    "    X_train = differencer.fit_transform(X_train)\n",
    "    X_test = differencer.fit_transform(X_test)\n",
    "    \n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb6052e-e6f3-4355-9851-df3de1fe2fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 46s 46s/step - loss: 3.0290 - accuracy: 0.1875 - val_loss: 4.5521 - val_accuracy: 0.2500 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 4.0749 - accuracy: 0.2188 - val_loss: 3.6963 - val_accuracy: 0.3750 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 5.3325 - accuracy: 0.2188 - val_loss: 1.9962 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9843 - accuracy: 0.4688 - val_loss: 1.5063 - val_accuracy: 0.8750 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4486 - accuracy: 0.7812 - val_loss: 1.5380 - val_accuracy: 0.6250 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6292 - accuracy: 0.5312 - val_loss: 1.1597 - val_accuracy: 0.8750 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4483 - accuracy: 0.5938 - val_loss: 1.2468 - val_accuracy: 0.8750 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.3124 - accuracy: 0.7188 - val_loss: 1.2968 - val_accuracy: 0.6250 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1712 - accuracy: 0.7812 - val_loss: 1.2612 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "Epoch 9: early stopping\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 1.1636 - accuracy: 0.8750\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 43s 8s/step - loss: 1.3016 - accuracy: 0.5556 - val_loss: 1.1401 - val_accuracy: 0.5385 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 1.1147 - accuracy: 0.6667 - val_loss: 0.9890 - val_accuracy: 0.5385 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.9360 - accuracy: 0.6111 - val_loss: 0.9592 - val_accuracy: 0.3846 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.7951 - accuracy: 0.6481 - val_loss: 0.6184 - val_accuracy: 0.9231 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6324 - accuracy: 0.7963 - val_loss: 0.4812 - val_accuracy: 1.0000 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.5255 - accuracy: 0.9259 - val_loss: 0.3633 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.4217 - accuracy: 0.9630 - val_loss: 0.3531 - val_accuracy: 0.9231 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.3338 - accuracy: 0.9444 - val_loss: 0.2843 - val_accuracy: 0.9231 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2677 - accuracy: 0.9815 - val_loss: 0.2125 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2286 - accuracy: 0.9815 - val_loss: 0.1740 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.2821 - accuracy: 0.9679\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 46s 46s/step - loss: 1.7521 - accuracy: 0.4167 - val_loss: 1.5605 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4759 - accuracy: 0.9167 - val_loss: 1.6322 - val_accuracy: 0.6667 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4296 - accuracy: 0.6667 - val_loss: 0.5744 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.1398 - accuracy: 0.5833 - val_loss: 0.5804 - val_accuracy: 1.0000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5604 - accuracy: 0.9167 - val_loss: 0.7316 - val_accuracy: 1.0000 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5052 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3945 - accuracy: 1.0000 - val_loss: 0.3577 - val_accuracy: 1.0000 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3706 - accuracy: 1.0000 - val_loss: 0.3518 - val_accuracy: 1.0000 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.3371 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 6.0706 - accuracy: 0.2667\n",
      "test accuracy of BasicMotions dataset is 0.875\n",
      "test accuracy of ItalyPowerDemand dataset is 0.967930018901825\n",
      "test accuracy of AtrialFibrillation dataset is 0.2666666805744171\n"
     ]
    }
   ],
   "source": [
    "# training model on multiple datasets\n",
    "datasets = [\"BasicMotions\", \"ItalyPowerDemand\", \"AtrialFibrillation\"]\n",
    "test_accuracy = []\n",
    "for data in datasets:\n",
    "    X_train, y_train = load_UCR_UEA_dataset(name=data, split='train',  return_type=\"numpy2D\", extract_path=None)\n",
    "    X_test, y_test = load_UCR_UEA_dataset(name=data, split='test',  return_type=\"numpy2D\", extract_path=None)\n",
    "    accuracy = train_and_evaluate_combined_cnn_lstm(X_train, y_train, X_test, y_test)\n",
    "    test_accuracy.append(accuracy)\n",
    "for index, acc in enumerate(test_accuracy):\n",
    "    print(\"test accuracy of {} dataset is {}\".format(datasets[index], acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cea90047-d045-4d91-bee3-a44c1694cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 184.0564 - accuracy: 0.0938 - val_loss: 2336.8887 - val_accuracy: 0.3750 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2427.7410 - accuracy: 0.2812 - val_loss: 5215.9141 - val_accuracy: 0.2500 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3556.2427 - accuracy: 0.2188 - val_loss: 1439.1749 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 988.1449 - accuracy: 0.4062 - val_loss: 143.5804 - val_accuracy: 0.5000 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 144.9196 - accuracy: 0.4062 - val_loss: 122.7213 - val_accuracy: 0.1250 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 104.0824 - accuracy: 0.4062 - val_loss: 204.2823 - val_accuracy: 0.2500 - lr: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 102.5335 - accuracy: 0.3750 - val_loss: 126.7370 - val_accuracy: 0.3750 - lr: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 63.8813 - accuracy: 0.4688 - val_loss: 50.1018 - val_accuracy: 0.2500 - lr: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488116759806871.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 62.6029 - accuracy: 0.4062 - val_loss: 37.4843 - val_accuracy: 0.1250 - lr: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965853411704302.\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 42.3749 - accuracy: 0.3438 - val_loss: 1.7875 - val_accuracy: 0.7500 - lr: 0.0050\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 16.9757 - accuracy: 0.4000\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 2s 371ms/step - loss: 2.1700 - accuracy: 0.5556 - val_loss: 2.0138 - val_accuracy: 0.5385 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.7566 - accuracy: 0.7037 - val_loss: 0.2273 - val_accuracy: 0.8462 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.0090 - accuracy: 0.7593 - val_loss: 0.3692 - val_accuracy: 0.8462 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.5533 - accuracy: 0.7963 - val_loss: 0.5057 - val_accuracy: 0.7692 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4413 - accuracy: 0.8333 - val_loss: 0.4287 - val_accuracy: 0.7692 - lr: 0.0082\n",
      "Epoch 5: early stopping\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.4347 - accuracy: 0.7677\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1131 - accuracy: 0.3333 - val_loss: 1.7240 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3571 - accuracy: 0.7500 - val_loss: 26.5394 - val_accuracy: 0.3333 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 23.3123 - accuracy: 0.2500 - val_loss: 0.2694 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 12.6617 - accuracy: 0.5000 - val_loss: 13.0150 - val_accuracy: 0.0000e+00 - lr: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 3.5047 - accuracy: 0.6667 - val_loss: 7.2688 - val_accuracy: 0.6667 - lr: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.2148 - accuracy: 0.7500 - val_loss: 0.3459 - val_accuracy: 1.0000 - lr: 0.0074\n",
      "Epoch 6: early stopping\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.0861 - accuracy: 0.4667\n",
      "test accuracy of BasicMotions dataset is 0.4000000059604645\n",
      "test accuracy of ItalyPowerDemand dataset is 0.7677356600761414\n",
      "test accuracy of AtrialFibrillation dataset is 0.46666666865348816\n"
     ]
    }
   ],
   "source": [
    "### second ideas\n",
    "## first transform the data using sktime transformer then used a mlp model which implemented  in task 2.\n",
    "\n",
    "datasets = [\"BasicMotions\", \"ItalyPowerDemand\", \"AtrialFibrillation\"]\n",
    "test_accuracy = []\n",
    "for data in datasets:\n",
    "    X_train, y_train = load_UCR_UEA_dataset(name=data, split='train',  return_type=\"numpy2D\", extract_path=None)\n",
    "    X_test, y_test = load_UCR_UEA_dataset(name=data, split='test',  return_type=\"numpy2D\", extract_path=None)\n",
    "    X_train, X_test = transform_x_by_transformer(X_train, X_test)\n",
    "    test_acc = mlp(X_train, y_train, X_test, y_test)\n",
    "    test_accuracy.append(test_acc)\n",
    "for index, accuracy in enumerate(test_accuracy):\n",
    "    print(\"test accuracy of {} dataset is {}\".format(datasets[index], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acb475-a545-4168-8bc6-4dc032da433e",
   "metadata": {},
   "source": [
    "### Task 4: Time series classification using sktime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a7d53-1b21-4709-9813-ea99329447b9",
   "metadata": {},
   "source": [
    "We can use `RandomizedSearch` to find optimal parameter options on pipelines. However, sktime's pipeline does not support scikit-learn's classifiers such as DecisionTree or RandomForest well. However, sometimes we would like to use the output of the sktime transformer (e.g., catch22) to train scikit-learn models such as RandomForest. Sktime supports this with `SklearnClassifierPipeline` to put a  scikit-learn classifier and sktime's transformer together and you need to implement it.\n",
    "\n",
    "Pick one classifier from scikit-learn (that can be anything! e.g., Decision Tree or Logistic Regressor) and two transformers from sktime and create `SklearnClassifierPipeline.` As we tried in this lab, that can be **Rocket with RandomForest** or **Catch22 with DecisionTree**. Pick one parameter from each module (in total, three, one from the classifier and two from two transformers) and run a randomized search on the pipeline you define and report the test score of the best model found by the randomized Search. Compare your best score to the score from the same model with the default setting.\n",
    "\n",
    "Task 4 involves a time-consuming process, so you can only choose **one dataset** to perform the task above. Also, note that you do not need to perform better by conducting a randomized search for this task (but still good to try!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3ca2214-027d-4445-9418-aeeb6e109e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.summarize import RandomIntervalFeatureExtractor\n",
    "from sktime.transformations.panel.reduce import Tabularizer\n",
    "from sktime.transformations.panel.pca import PCATransformer\n",
    "\n",
    "\n",
    "from sktime.classification.compose import SklearnClassifierPipeline\n",
    "\n",
    "from sktime.transformations.series.exponent import ExponentTransformer\n",
    "from sktime.transformations.series.difference import Differencer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ad2b075-7c6c-4944-9c7c-1f7af0f8bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transformer_and_model(X_train, y_train, X_test, y_test):\n",
    "    exponentTransformer = ExponentTransformer()\n",
    "    differencer = Differencer()\n",
    "    clf = KNeighborsClassifier()\n",
    "\n",
    "    # pipeline\n",
    "    pipeline = SklearnClassifierPipeline(\n",
    "        transformers=[\n",
    "            ('exponentTransformer', exponentTransformer),\n",
    "            ('differencer', differencer)\n",
    "            ]\n",
    "        ,\n",
    "        classifier=clf\n",
    "        )\n",
    "    param_grid = {'exponentTransformer__power':[0.1,0.3,0.5, 0.7,0.9],\n",
    "                  'differencer__lags': [1,2,3,4,5],\n",
    "                  'clf__n_neighbors':[5,10,15,20] \n",
    "                 }\n",
    "    model = RandomizedSearchCV(pipeline, param_grid,n_iter=10, cv=3, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    best_model = model.best_estimator_\n",
    "    score = best_model.score(X_test, y_test)\n",
    "\n",
    "    # best score and best estimator\n",
    "    print(\"Best Score: {}\".format( model.best_score_))\n",
    "    print(\"best Estimator: {}\".format( model.best_estimator_))\n",
    "\n",
    "    print(\"test score of the best model is {}\".format(score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf917e3c-ffe7-408f-8b0c-d80bcc7c48e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Testing using Randomized searchcv hyperparameter tuning on BasicMotions dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tensorflow-learn\\tensorenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: nan\n",
      "best Estimator: SklearnClassifierPipeline(classifier=KNeighborsClassifier(),\n",
      "                          transformers=[('exponentTransformer',\n",
      "                                         ExponentTransformer(power=0.7)),\n",
      "                                        ('differencer', Differencer(lags=2))])\n",
      "test score of the best model is 0.325\n",
      "Training Testing using default parameter on BasicMotions dataset\n",
      "Test score of the default model is 0.25\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"BasicMotions\"]\n",
    "\n",
    "for data in datasets:\n",
    "    X_train, y_train=load_UCR_UEA_dataset(name=data, split='train', return_type=\"numpy2D\", extract_path=None)\n",
    "    X_test, y_test =load_UCR_UEA_dataset(name=data, split='test', return_type=\"numpy2D\",   extract_path=None)\n",
    "\n",
    "    print(\"Training Testing using Randomized searchcv hyperparameter tuning on {} dataset\".format(data))\n",
    "    transformer_and_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(\"Training Testing using default parameter on {} dataset\".format(data))\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"transform1\", ExponentTransformer()),\n",
    "        (\"transform2\", Differencer()),\n",
    "        (\"classifier\", KNeighborsClassifier())\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    score = pipe.score(X_test, y_test)\n",
    "    print(\"Test score of the default model is {}\".format(score))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251db31-6649-44b9-89a2-b1d533e07246",
   "metadata": {},
   "source": [
    "### Task 5: Multivariate time series classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb98f-77b8-4099-9d97-088be39407be",
   "metadata": {},
   "source": [
    "Time series can be **multivariate**, which means there can be many values (= data points) describing one time point. In this task, you will use one **multivariate** dataset (**Eplipsy**) and try to run one deep learning model and one sktime model to see if those models work well on multivariate time series.\n",
    "\n",
    "- Use Eplipsy dataset in the UCR/UEA repository.\n",
    "- `Run two classifiers of your choice in sktime`, such as TapNet, Rocket, or MiniRocket, together with **the best tensorflow deep learning model from the previous tasks** on Eplipsy. You need to adjust the deep learning model's input layer to handle this multivariate dataset.\n",
    "- Use sktime's `load_UCR_UEA_dataset` function to perform. You should use each dataset's original train/test splits.\n",
    "- For the deep learning model, you should transform it using TensorFlow data API (`tf.data`) to manage your dataset and use `shuffle`, `batch`, and `prefetch` functions. This means that you need to create the validation set first. If you use Torch, explain how you implement the equivalent operations.\n",
    "- For training, you need to run at least 10 epochs for your deep learning model. `For TapNet, you can keep the default parameter options`.\n",
    "- Report the test scores of three models on the predefined test set.\n",
    "- **Do the same task on one more chosen multivariate time series dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f807646d-3b85-4c69-8c48-1a8f2ff21724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.deep_learning.tapnet import TapNetClassifier\n",
    "from sktime.classification.deep_learning.resnet import ResNetClassifier\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "def test_resnet(X_train, y_train, X_test, y_test):\n",
    "    # tapnet classifer\n",
    "    # tapnet = TapNetClassifier(layers=(50, 30),  n_epochs=10, batch_size=32)\n",
    "    restnet = ResNetClassifier(n_epochs=15, batch_size=32)\n",
    "    restnet.fit(X_train, y_train)\n",
    "    # evaluate the classifier on test data\n",
    "    test_acc = restnet.score(X_test, y_test)\n",
    "    print(\"ResNet Test Accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "def test_rocket(X_train, y_train, X_test, y_test):\n",
    "    #rocket classifier\n",
    "    rocket = Rocket(random_state=42)\n",
    "    rocket.fit(X_train)\n",
    "    X_train_transform_rocket = rocket.transform(X_train)\n",
    "    X_test_transform_rocket = rocket.transform(X_test)\n",
    "    classifier_rocket = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "    classifier_rocket.fit(X_train_transform_rocket, y_train)\n",
    "    test_acc = classifier_rocket.score(X_test_transform_rocket, y_test)\n",
    "    print(\"Rocket test Accuracy:\", test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18b94c0c-7795-42d6-a461-fbfa8f9098a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## deep learning model\n",
    "def deep_learning(X_train, y_train, X_test, y_test):\n",
    "    # encoding the data\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # converting into dummy data\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    num_classes = y_train.shape[-1]\n",
    "    batch_size = 32\n",
    "    # creating Tensorflow dataset\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=len(X_train))\n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(len(X_train) * validation_split)\n",
    "    train_ds = train_data.skip(num_validation_samples)\n",
    "    val_ds = train_data.take(num_validation_samples)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    \n",
    "\n",
    "    batch_size = 32\n",
    "    train_ds =(\n",
    "        train_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    input_shape = X_train.shape[1:]\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Example layers; adjust according to your dataset\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        layers.LSTM(32, return_sequences=True),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_ds, epochs=10, batch_size=32, validation_data=val_ds)\n",
    "\n",
    "    # Evaluate model on test data\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "    print('Deep Learning Test accuracy:', test_acc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf690e3d-692c-4d5e-9807-878163924d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 84ms/step\n",
      "ResNet Test Accuracy: 0.2753623188405797\n",
      "Rocket test Accuracy: 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_UCR_UEA_dataset(\"Epilepsy\", split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_UCR_UEA_dataset(\"Epilepsy\", split=\"test\", return_X_y=True)\n",
    "\n",
    "test_resnet(X_train,y_train, X_test, y_test)\n",
    "test_rocket(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b88bbc-e75c-4d47-9f04-2414c131cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 10s 969ms/step - loss: 0.6083 - accuracy: 0.3273 - val_loss: 0.4829 - val_accuracy: 0.5556\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 496ms/step - loss: 0.4769 - accuracy: 0.5455 - val_loss: 0.3602 - val_accuracy: 0.8519\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 493ms/step - loss: 0.3736 - accuracy: 0.8091 - val_loss: 0.2881 - val_accuracy: 0.8889\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 510ms/step - loss: 0.2922 - accuracy: 0.8909 - val_loss: 0.2258 - val_accuracy: 0.8889\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 520ms/step - loss: 0.2171 - accuracy: 0.9000 - val_loss: 0.1833 - val_accuracy: 0.9259\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 532ms/step - loss: 0.1436 - accuracy: 0.9273 - val_loss: 0.1572 - val_accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 2s 515ms/step - loss: 0.0866 - accuracy: 0.9818 - val_loss: 0.1262 - val_accuracy: 0.9259\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 2s 530ms/step - loss: 0.0704 - accuracy: 0.9636 - val_loss: 0.0414 - val_accuracy: 0.9630\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 2s 519ms/step - loss: 0.0404 - accuracy: 0.9909 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 2s 511ms/step - loss: 0.0360 - accuracy: 0.9818 - val_loss: 0.0417 - val_accuracy: 0.9630\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.0810 - accuracy: 0.9565\n",
      "Deep Learning Test accuracy: 0.95652174949646\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_UCR_UEA_dataset(\"Epilepsy\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test, y_test = load_UCR_UEA_dataset(\"Epilepsy\", split=\"test\", return_type=\"numpy2D\")\n",
    "deep_learning(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
