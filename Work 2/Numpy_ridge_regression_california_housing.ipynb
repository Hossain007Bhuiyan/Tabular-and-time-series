{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7vPaIoU4pqD"
      },
      "source": [
        "Practice the neural network algorithm by creating a model from scratch only with NumPy. You will learn how forward/backpropagation and weight normalization/activation of the simple single-layer neural network work. You will use the famous MNIST dataset, and you will need to compare the result under different normalization/activation settings of the same architecture using Pandas pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtWBHOcoqnKH"
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA351vjdly0R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "RANDOM_STATE = 12345"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCm_Zo2W8Q22"
      },
      "source": [
        "The MNIST dataset was constructed from two datasets of the US National Institute\n",
        "of Standards and Technology (NIST). The training dataset consists of handwritten\n",
        "digits from 250 different people, 50 percent high school students and 50 percent\n",
        "employees from the Census Bureau. Note that the test dataset contains handwritten digits from different people following the same split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPqJ7jTk7aaG"
      },
      "source": [
        "![alt text](https://dezyre.gumlet.net/images/Exploring+MNIST+Dataset+using+PyTorch+to+Train+an+MLP/MNIST+Dataset.png?w=900&dpr=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V29SZpPn8S69"
      },
      "source": [
        "This MNIST dataset can be directly downloaded via various routes including tensorflow dataset repository, keras dataset, and scikit-learn. This time, you can use `sklearn.datasets` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmAMpQXwdq4t",
        "outputId": "9873a6a0-b6de-45e9-9ac9-69dcfb53fca1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUngFc88XsU"
      },
      "source": [
        "It has 70,000 different handwriting instances. It usually has 60,000 instances in the training set and the remaining ones in the test set, but scikit-learn loads it as a whole. You may need to divide it into two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVuTqRrtlb44",
        "outputId": "f742c3df-15e4-4e45-e64a-d59f742ded5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((70000, 784), (70000,))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kILkvez98bVl"
      },
      "source": [
        "You can also check the class distribution. Is it balanced or not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "YNAjbM-Z8ixF",
        "outputId": "ae7d7c84-a6e4-49fd-a071-ddb55b4312c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.bar(np.unique(y, return_counts = True)[0], np.unique(y, return_counts = True)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEc2na5f9tRg"
      },
      "source": [
        "You may also need to apply normalization for better performance. Since it is clear that its maximum value is 255, you can simply normalize it by dividing the whole value by 255 (then the dataset will range from 0 to 1). You can use NumPy's broadcasting to divide the matrix by one scalar value. It is also possible to further standardize it to have a centeralized mean, but this time it is optional.\n",
        "- Apply normalization to X to have the range [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzXvdYtlmNqi"
      },
      "outputs": [],
      "source": [
        "#X_normalized = None # CHANGE IT\n",
        "X_normalized = X / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmuNtw2d8dX"
      },
      "source": [
        "If you take a look at `y`, it has string labels! It might be disturbing when you need to handle them later, so let's also convert them to an integer form.\n",
        "- Change the type of the labels to integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfx0EL0wmoyU"
      },
      "outputs": [],
      "source": [
        "#y_integer = None # CHANGE IT\n",
        "y_integer = y.astype(int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqeW4C00l5se"
      },
      "source": [
        "Next, you need to split the dataset into two parts using scikit-learn's `train_test_split` method.\n",
        "- Use scikit-learn's `train_test_split` to create training and test sets.\n",
        "- Set **test_size** to 20%.\n",
        "- Enable stratification and shuffling.\n",
        "- use `X_normalized` and `y_integer`.\n",
        "- set `random_state` to the pre-defined variable `RANDOM_STATE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiERfT0gddu1"
      },
      "outputs": [],
      "source": [
        "#X_train, X_test, y_train, y_test = None # CHANGE IT\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, stratify=y, shuffle=True, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcUJo3uj6sCa"
      },
      "source": [
        "Please print the mean and the standard deviation of `X_train` here (for validation purpose)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mujB3I56rjw",
        "outputId": "a19041c1-667e-459a-9626-2da374dc98c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.13086888228505666, 0.30836916270386544)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.mean(), X_train.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMldZza-faxL"
      },
      "source": [
        "Here you can check some of the instances that you get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "TzRs98dodeAF",
        "outputId": "e85d93f4-82a3-4627-f604-bebe94698eea"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
        "ax = ax.flatten()\n",
        "for i in range(10):\n",
        "  img = X_train[i].reshape(28, 28)\n",
        "  ax[i].imshow(img, cmap='Greys')\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baT1IBRFqrBJ"
      },
      "source": [
        "## 2. Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVROBu9_py1h"
      },
      "source": [
        "First, you need to fix the labels, which are now in a sparse form, to have a one-hot encoded form for better computation of error terms using NumPy's vectorization. Let's create a function that receives a label vector and transform it into a one-hot encoded label matrix.\n",
        "\n",
        "- Complete the `one_hot` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msvnKGsR0yUS"
      },
      "outputs": [],
      "source": [
        "def one_hot(y):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - y : set of labels\n",
        "\n",
        "  Output:\n",
        "    - onehot: a one-hot-encoded array\n",
        "\n",
        "  This function creates an one-hot encoded representation of the labels.\n",
        "  This means that you will have a set of binary columns indicading each possible class.\n",
        "\n",
        "  You have to develop this one hot encoding strategy without using Python for loop\n",
        "\n",
        "  Expected outcome:\n",
        "    one_hot(np.array([1,0,2,3]))\n",
        "\n",
        "    array([[0., 1., 0., 0.],\n",
        "          [1., 0., 0., 0.],\n",
        "          [0., 0., 1., 0.],\n",
        "          [0., 0., 0., 1.]])\n",
        "  \"\"\"\n",
        "  n_label = np.unique(y).size\n",
        "  onehot = np.zeros((len(y), n_label))\n",
        "  onehot[np.arange(len(y)), y] = 1\n",
        "  return onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dAPaf1708Bs"
      },
      "source": [
        "Test your code here (see the expected outcome above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGFbZ_tN00WK",
        "outputId": "ff170996-93b0-4bc2-dbf3-10588c35e9d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_hot(np.array([1,0,2,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7niKihIdpYa5"
      },
      "source": [
        "Next, you may also need a sigmoid function for the output values as you are dealing with a classification problem. Sigmoid can be represented as follows:\n",
        "\n",
        "$$ h_ \\theta (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} }  $$\n",
        "\n",
        "\n",
        "- Complete the sigmoid function below that supports both vectors and scalars (this can be automatically handled if you use NumPy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFSCiidT1Drz"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - z: input vector or scalar value\n",
        "\n",
        "  Output:\n",
        "    - sigmoid: output sigmoid-transformed vector or scalar value\n",
        "\n",
        "  Calculate the sigmoid value of the input.\n",
        "\n",
        "  Expected outcome:\n",
        "    sigmoid(np.array([np.inf, -np.inf, 0]))\n",
        "\n",
        "    array([1. , 0. , 0.5])\n",
        "  \"\"\"\n",
        "  return  1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlugwi8r0O-H"
      },
      "source": [
        "Test your code here (see the expected outcome above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fJNnW22qMQ0",
        "outputId": "21030ca1-90cb-4249-b782-b77de166c542"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1. , 0. , 0.5])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sigmoid(np.array([np.inf, -np.inf, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnfdCK6Fqtls"
      },
      "source": [
        "## 3. Our FCN classifier with the class structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2YtPQk0pXK"
      },
      "source": [
        "Now it is time to create your neural network model from scratch! You eventually need to integrate everything into scikit-learn's pipeline, so it's important to have an appropriate class structure. To do this, you may need to extend `BaseEstimator` and `TransformerMixin` to make scikit-learn recognize that your class is a valid classifier.\n",
        "\n",
        "You are going to develop a neural network with one layer for simplicity. That means you will have two different sets of weights.\n",
        "\n",
        "- First layer: [input size (number of features), hidden layer size]\n",
        "- Second layer (or output layer): [hidden layer size, output size (number of classes)]\n",
        "\n",
        "In the class structure `FullyConnectedNetwork` below, you will develop five different methods as follows:\n",
        " - `compile`: Given parameters, this function will initialize weight and bias values needed for our neural network model.\n",
        "   - Here, you will initialize bias and weights based on a chosen initialization technique.\n",
        "        - You need to implement three different options: normal, Xavier, and he\n",
        "        - Each technique initializes the weight using the normal distribution but different standard deviation. The mean value remains the same.\n",
        "          - Normal:\n",
        "$ \\mu = 0, \\sigma = 0.1 $\n",
        "          - Xavier:\n",
        "$ \\mu = 0, \\sigma = \\sqrt{\\frac{2}{n_{in} + n_{out}}}$\n",
        "          - He:\n",
        "$ \\mu = 0, \\sigma = \\sqrt{\\frac{2}{n_{in}}}$\n",
        " - `forward`: Perform a forward propagation with the weights saved in the model.\n",
        " - `back_propagation`: Perform a back propagation (training the model).\n",
        "    - Most of the derivative terms are already provided. You will only need to finish some part of it.\n",
        "      - Weight and bias update\n",
        "      - Derivative of the sigmoid function\n",
        "        - $σ(x)=σ(x)(1−σ(x))$.\n",
        "\n",
        " - `fit`: Run the whole fitting process (forward and backpropagation for each batch).\n",
        " - `cost`: Calculate the cost (cross-entropy) together with the elastic net (l1/l2)\n",
        "   - cross-entropy loss can be calculated as follows:\n",
        " - `predict`: With a trained model, perform a prediction of unseen data by running the forward propagation with the trained weight and bias.\n",
        " - `evaluate`: With trained weight and bias, perform a prediction of test data and calculate the performance metric (in our case, those are training and validation accuracy scores).\n",
        "\n",
        "**This exercise is based on *Chapter 12* of the coursebook Python Machine Learning with some modification and additional tasks - so please note that the structure and requirements are not the same.**\n",
        "\n",
        "You are free to check out the coursebook for reference. However, to solve the tasks, you should understand the logic clearly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mLxK2KXJ1UY"
      },
      "outputs": [],
      "source": [
        "class FullyConnectedNetwork(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_hidden=30, l2=0., l1=0., epochs=100, eta=0.001, validation_rate = 0.3,\n",
        "                 shuffle=True, batch_size=1, init_technique = \"normal\", seed=None, debug=True):\n",
        "\n",
        "        \"\"\"\n",
        "        The class structure receive the following parameters to construct and test the model:\n",
        "\n",
        "        Input:\n",
        "          - n_hidden: Number of hidden nodes.\n",
        "          - l2: Lambda value for L2-regularization.\n",
        "          - l1: Lambda value for L1-regularization.\n",
        "          - epochs: Number of passes over the training set.\n",
        "          - eta: Learning rate.\n",
        "          - validation_rate: size of the validation set.\n",
        "          - shuffle: Enabling shuffling option of the dataset every epoch.\n",
        "          - batch_size: Number of training examples per batch.\n",
        "          - init_technique: Indicator for an initialization technique.\n",
        "          - seed: Random seed for initializing weights and shuffling.\n",
        "        \"\"\"\n",
        "        self.seed = seed\n",
        "\n",
        "        # DEFINE YOUR RANDOM NUMBER GENERATOR USING THE INPUT SEED\n",
        "        # WARNING! It is strictly required to use \"np.random.default_rng\" to generate random numbers.\n",
        "        #self.random = np.random.default_rng(seed) # CHANGE IT\n",
        "        self.random = None\n",
        "        self.n_hidden = n_hidden\n",
        "        self.l2 = l2\n",
        "        self.l1 = l1\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.validation_rate = validation_rate\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.debug = debug\n",
        "        self.init_technique = init_technique\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compile(self, n_features, n_outputs):\n",
        "        \"\"\"\n",
        "        Initializing the weights of the model\n",
        "\n",
        "        - Here you will initialize bias and weights based on chosen initialization technique.\n",
        "        - The classifier has three different options: normal, xavier, and he\n",
        "        - Each technique initializes the weight using the normal distribution but different standard deviation.\n",
        "        - Use self.init_technique to check the chosen technique and use self.random to perform the sampling.\n",
        "\n",
        "        Input:\n",
        "          - n_features: input size of the network\n",
        "          - n_outputs: output size of the network\n",
        "          - Unit size of the layer is given as self.n_hidden\n",
        "\n",
        "        Steps:\n",
        "          1. Check if you have created self.random using NumPy's random number generator.\n",
        "             You will use this generator throughout this function.\n",
        "          2. Create lists self.W and self.B which will keep the weight values for each layer.\n",
        "          3. Set mean and standard deviation for different initialization technique.\n",
        "          4. Create weights and bias for the linkage between inputs and the first layer.\n",
        "            - Weight should have the size [n_features, self.n_hidden].\n",
        "            - Bias should have the size [self.n_hidden].\n",
        "            - Weight initialization should be applied to the weights only.\n",
        "            - Bias should be initizalied by zeros.\n",
        "          5. Create weights and bias for the linkage between the first layer and the output layer.\n",
        "            - Weight should have the size [self.n_hidden, n_outputs].\n",
        "            - Bias should have the size [n_outputs].\n",
        "            - Weight initialization should be applied to the weights only.\n",
        "            - Bias should be initizalied by zeros.\n",
        "          5. Save the weights to self.W and biases to self.B. Each list should have two elements for each layer.\n",
        "\n",
        "          WARNING! It is strictly required to use \"np.random.default_rng\" to generate random numbers.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        self.random=np.random.default_rng(self.seed)\n",
        "\n",
        "        #self.B = None # CHANGE IT\n",
        "        #self.W = None # CHANGE IT\n",
        "        self.B =[]\n",
        "        self.W = []\n",
        "\n",
        "        # 1. Creating weights and bias for [input -> hidden]\n",
        "        # Use specific initialization techniques for weights\n",
        "        # Weights should have the size (n_features, self.n_hidden)\n",
        "        # Use np.zeros for bias with the size 'self.n_hidden'\n",
        "        if self.init_technique==\"normal\":\n",
        "              mean,std_dev=0, 0.1\n",
        "        elif self.init_technique==\"xavier\":\n",
        "              mean,std_dev=0,np.sqrt(2/(n_features+n_outputs))\n",
        "        elif self.init_technique==\"he\":\n",
        "              mean,std_dev=0,np.sqrt(2/n_features)\n",
        "        else:\n",
        "            raise ValueError(\"Void initialization technique\")\n",
        "\n",
        "        b_h =np.zeros(self.n_hidden)\n",
        "        w_h=self.random.normal(mean,scale=std_dev,size=(n_features, self.n_hidden))\n",
        "\n",
        "        # 2. Append bias to self.B and weights to self.W\n",
        "\n",
        "        # 3. Creating weights and bias for [hidden -> output]\n",
        "        # Use specific initialization techniques for weights\n",
        "        # Weights should have the size (self.n_hidden, n_outputs)\n",
        "        # Use np.zeros for bias with the size 'n_outputs'\n",
        "\n",
        "        #b_out = None # CHANGE IT\n",
        "        #w_out = None # CHANGE IT\n",
        "        b_out = np.zeros(n_outputs)\n",
        "        w_out= self.random.normal(mean,scale=std_dev,size=(self.n_hidden,n_outputs))\n",
        "\n",
        "        self.B.append(b_h)\n",
        "        self.B.append(b_out)\n",
        "        self.W.append(w_h)\n",
        "        self.W.append( w_out)\n",
        "\n",
        "        # 4. Append bias to self.B and weights to self.W\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Given the dataset X, compute forward propagation step with the weights and bias saved in the list.\n",
        "        This process eventually outputs ten numbers in our case as the dataset has ten outputs.\n",
        "        (Please refer to the lecture slides for detailed computation process)\n",
        "        Forward propagation is performed by multiple chained dot products of inputs and weights.\n",
        "\n",
        "        Input:\n",
        "          - X: features\n",
        "\n",
        "        Output:\n",
        "          - Z: Result of dot product of the weights and the previous output for each phase\n",
        "          - A: A list that contains sigmoided values of A\n",
        "        Steps:\n",
        "          1. Create two lists Z and A.\n",
        "          2. Take a dot product of X and the first weight self.W[0] - save the result into Z\n",
        "          3. Apply sigmoid function to the first Z - save the result into A\n",
        "          4. Take a dot product of A and the second weight self.W[1] - save the result into Z\n",
        "          5. Apply sigmoid function to the second Z - save the result into A\n",
        "          6. Return Z and A\n",
        "\n",
        "        WARNING! Be careful when you multiply two matrices - think about which rows you are multiplying.\n",
        "                 Wrong order in .dot() function can lead completely wrong result.\n",
        "        \"\"\"\n",
        "\n",
        "        #Z = None # CHANGE IT\n",
        "        #A = None # CHANGE IT\n",
        "        Z = []\n",
        "        A = []\n",
        "\n",
        "        # Step 1: net input of hidden layer\n",
        "        # - You are calculating the first XW+b.\n",
        "        # - Take a dot product of the input features and the initial weights.\n",
        "        # - Add the outcome to list Z.\n",
        "\n",
        "        #Z.append(None) # CHANGE IT\n",
        "        Z.append(X.dot(self.W[0])+self.B[0])\n",
        "\n",
        "        # Step 2: activation of hidden layer\n",
        "        # - Apply the sigmoid function to the dot producted outcome.\n",
        "        # - Add the outcome to list A.\n",
        "        #A.append(None) # CHANGE IT\n",
        "        A.append(self.sigmoid(Z[0]))\n",
        "\n",
        "        # Step 3: net input of output layer\n",
        "        # - You are calculating the second XW+b.\n",
        "        # - Take a dot product of the intermediate features and the weights of the output layer.\n",
        "        # - Add the outcome to list Z.\n",
        "        #Z.append(None) # CHANGE IT\n",
        "        Z.append(A[0].dot(self.W[1])+self.B[1])\n",
        "\n",
        "        # Step 4: activation output layer\n",
        "        # - Apply the sigmoid function to the dot producted outcome.\n",
        "        # - For simplicity, here the network uses sigmoid instead of softmax.\n",
        "        # - Add the outcome to list A.\n",
        "        #A.append(None) # CHANGE IT\n",
        "        A.append(self.sigmoid(Z[1]))\n",
        "\n",
        "        return Z, A\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "\n",
        "        Predict class labels by performing forward propagation.\n",
        "\n",
        "        Input:\n",
        "          - X: Feature matrix.\n",
        "        Output:\n",
        "          - y_pred: Predicted class labels for all data instances.\n",
        "\n",
        "        Steps:\n",
        "          1. Run forward proparation on X and get Z, a.\n",
        "          2. Calculate y_pred by using the final output (A[-1]) and with np.argmax\n",
        "            - You have to choose the index of the one with the highest value, which means the highest probability.\n",
        "          3. Return the prediction. You SHOULD perform the operation using NumPy's vectorization feature.\n",
        "             This means that if you put many instances at once as an input, this function should calculate the result also at once.\n",
        "\n",
        "        \"\"\"\n",
        "        #y_pred = None # CHANGE IT\n",
        "\n",
        "        Z, A=self.forward(X)\n",
        "        y_pred=np.argmax(A[-1], axis=1)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def cost(self, y_truth, y_pred):\n",
        "        \"\"\"\n",
        "\n",
        "        This function computes the cost for the classification task.\n",
        "        The network supports Elastic net (combination of l1 and l2 with corresponding weights).\n",
        "\n",
        "        Input:\n",
        "          - y_truth: \"One-hot encoded\" class labels.\n",
        "          - y_pred: Activation of the output layer (= output of the forward propagation function).\n",
        "          - The weights for l1 and l2 are saved into self.l1 and self.l2.\n",
        "\n",
        "        Output:\n",
        "          - cost: Regularized cost\n",
        "\n",
        "        Steps:\n",
        "          1. Calculate the cross entropy between the truth (y) and predicted values (y*).\n",
        "             - y * log(y*) - (1 - y) * log(1 - y*)\n",
        "          2. Add l1 and l2 terms to the cost.\n",
        "            - L1 term is the sum of absolute weight values.\n",
        "            - L2 term is the sum of squared weight values.\n",
        "            - You should multiply l1 and l2 ratio saved in self.l1 and self.l2 (this will decide the degree of regularization).\n",
        "            - You should NOT include weights that belong to the bias values.\n",
        "          3. Return the total cost (cross entropy + L1 term + L2 term).\n",
        "\n",
        "        \"\"\"\n",
        "        #cost = None # CHANGE IT\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        cross_entropy_loss = -np.sum(y_truth * np.log(y_pred) - (1 - y_truth) * np.log(1 - y_pred))\n",
        "        L1 = sum([np.sum(np.abs(w)) for w in self.W]) * self.l1\n",
        "        L2 = sum([np.sum(w**2) for w in self.W]) * self.l2\n",
        "        cost = cross_entropy_loss + L1 + L2\n",
        "\n",
        "        return cost\n",
        "\n",
        "\n",
        "    def back_propagation(self, X_train, batch_idx, A, y_truth):\n",
        "      \"\"\"\n",
        "      Perform back propagation based on the result of forward propagation and true labels (for each batch).\n",
        ",\n",
        "      Input:\n",
        "        X_train: Training features.\n",
        "        batch_idx: The current batch indices from the fit function.\n",
        "        A: Sigmoided output values - the result of forward propagation.\n",
        "        y_truth: One-hot encoded true labels.\n",
        "\n",
        "      Output:\n",
        "        None\n",
        "        You should update the weights and biases in self.W/self.B\n",
        "\n",
        "      **** You only need to fill in some required parts marked as \"CHANGE THIS PART\" ****\n",
        "      **** To get more information about the backpropagation process:\n",
        "           https://towardsdatascience.com/deriving-backpropagation-with-cross-entropy-loss-d24811edeaf9 ****\n",
        "      \"\"\"\n",
        "\n",
        "      # OUTPUT WEIGHTS (LAYER-OUTPUT)\n",
        "\n",
        "      # δC/δA * δA/δZ\n",
        "      delta_out = A[-1] - y_truth[batch_idx]\n",
        "      # δC/δA * δA/δZ * δZ/δW\n",
        "      grad_w_out = np.dot(A[0].T, delta_out)\n",
        "      # δC/δA * δA/δZ * δZ/δB\n",
        "      grad_b_out = np.sum(delta_out, axis=0)\n",
        "\n",
        "      #############################################\n",
        "      # CHANGE THIS PART\n",
        "\n",
        "      # Using the final gradients of the weight and bias (grad_w_out, grad_b_out), the network needs to update its current weight values.\n",
        "      # The gradient of w and b are already calculated and all you need to do is to merge it with l1/l2 terms.\n",
        "      # Change the values of self.W[1], self.B[1] (output weight and bias).\n",
        "      # - You should also apply l1 and l2 normalization to the weight (not to the bias).\n",
        "      # - You should use the learning rate (self.eta) when changing the value.\n",
        "\n",
        "      #delta_w_out = None # CHANGE IT\n",
        "      delta_w_out=grad_w_out+self.l1*np.sign(self.W[1])+self.l2*self.W[1]\n",
        "      #delta_b_out = None # CHANGE IT\n",
        "      delta_b_out=grad_b_out\n",
        "      #self.W[1] = None # CHANGE IT\n",
        "      self.W[1]-=self.eta*delta_w_out\n",
        "      #self.B[1] = None # CHANGE IT\n",
        "      self.B[1]-=self.eta*delta_b_out\n",
        "\n",
        "\n",
        "      # END OF CHANGE\n",
        "      #############################################\n",
        "\n",
        "      # HIDDEN WEIGHTS (INPUT-LAYER)\n",
        "\n",
        "      #############################################\n",
        "      # CHANGE THIS PART\n",
        "\n",
        "      # To continue to take derivatives backwards, you need to take a derivative of the sigmoid function.\n",
        "      # Here you are trying to take derivative of a sigmoided output A[0].\n",
        "      # Derivative of sigmoid σ(x) can be represented as σ(x)(1−σ(x)).\n",
        "\n",
        "      #sigmoid_derivative_h = None # CHANGE IT\n",
        "      sigmoid_derivative_h=A[0]*(1-A[0])\n",
        "\n",
        "      # END OF CHANGE\n",
        "      #############################################\n",
        "\n",
        "      delta_h = (np.dot(delta_out, self.W[1].T) * sigmoid_derivative_h)\n",
        "      grad_w_h = np.dot(X_train[batch_idx].T, delta_h)\n",
        "      grad_b_h = np.sum(delta_h, axis=0)\n",
        "\n",
        "      #############################################\n",
        "      # CHANGE THIS PART\n",
        "\n",
        "      # Using the final gradients of the weight and bias (grad_w_h, grad_b_h).\n",
        "      # The gradient of w and b are already calculated and all you need to do is to merge it with l1/l2 terms.\n",
        "      # Change the values of self.W[0], self.B[0] (output weight and bias).\n",
        "      # - You should also apply l1 and l2 normalization to the weight (not to the bias).\n",
        "      # - You should use the learning rate (self.eta) when changing the value.\n",
        "\n",
        "\n",
        "      #delta_w_h = None # CHANGE IT\n",
        "      #delta_b_h = None # CHANGE IT\n",
        "      delta_w_h=grad_w_h + self.l1 * np.sign(self.W[0]) + self.l2 * self.W[0]\n",
        "      delta_b_h=grad_b_h\n",
        "\n",
        "      #self.W[0] = None # CHANGE IT\n",
        "      #self.B[0] = None # CHANGE IT\n",
        "      self.W[0] -= self.eta * delta_w_h\n",
        "      self.B[0] -= self.eta * delta_b_h\n",
        "\n",
        "      # END OF CHANGE\n",
        "      #############################################\n",
        "\n",
        "    def one_hot(self,y):\n",
        "      n_label = np.unique(y).size\n",
        "      onehot = np.zeros((len(y), n_label))\n",
        "      onehot[np.arange(len(y)), y] = 1\n",
        "      return onehot\n",
        "\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    def evaluate(self, epoch, X_train, X_valid, y_train, y_valid):\n",
        "      \"\"\"\n",
        "      Evaluate performances on the training and validation sets per epoch\n",
        "\n",
        "      Input:\n",
        "        - epoch: Current epoch number.\n",
        "        - X_train: Training features\n",
        "        - X_valid: Validation features\n",
        "        - y_train: Training labels\n",
        "        - y_valid: Validation labels\n",
        "\n",
        "      Output:\n",
        "        - None\n",
        "        Append the cost and performance metrics of current epoch to self.history\n",
        "      \"\"\"\n",
        "\n",
        "      # Step 1. Call self.forward on X_train to calculate the output with current weights and bias of the model.\n",
        "      #Z, A = None # CHANGE IT\n",
        "      Z, A = self.forward(X_train)\n",
        "\n",
        "      # Step 2. call predict functions with both X_train and X_valid and save the predicted values accordingly.\n",
        "      #y_train_pred = None # CHANGE IT\n",
        "      #y_valid_pred = None # CHANGE IT\n",
        "      y_train_pred = self.predict(X_train)\n",
        "      y_valid_pred = self.predict(X_valid)\n",
        "      # Step 2. Call self.cost with one hot encoded y_train and the probability of the output prediction.\n",
        "      # Save it into the variable 'cost'.\n",
        "      #cost = None # CHANGE IT\n",
        "      cost = self.cost(self.one_hot(y_train), A[-1])\n",
        "\n",
        "\n",
        "      # Step 4. Calculate accuracy scores.\n",
        "      # - between y_train_pred and y_train.\n",
        "      # - between y_valid_pred and y_valid.\n",
        "      #train_acc = None # CHANGE IT\n",
        "      #valid_acc = None # CHANGE IT\n",
        "\n",
        "      train_acc = (y_train_pred == y_train).mean()\n",
        "      valid_acc = (y_valid_pred == y_valid).mean()\n",
        "\n",
        "\n",
        "\n",
        "      # Step 5. Save the results into the dictionary.\n",
        "      # This part is already complete.\n",
        "      if self.debug == True:\n",
        "        print('%d/%d | Cost: %.2f '\n",
        "                        '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
        "                        (epoch+1, self.epochs, cost,\n",
        "                          train_acc*100, valid_acc*100))\n",
        "        self.history['cost'].append(cost)\n",
        "        self.history['train_acc'].append(train_acc)\n",
        "        self.history['valid_acc'].append(valid_acc)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "\n",
        "        Learn weights from training data.\n",
        "\n",
        "        Input\n",
        "          - X: features (training+validation)\n",
        "          - y: labels\n",
        "\n",
        "        Output\n",
        "          - self.history: information about cost and accuracy scores\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.history = {'cost': [], 'train_acc': [], 'valid_acc': []}\n",
        "\n",
        "        # Step 1: Select different training and test sets. Use scikit-learn's train_test_split.\n",
        "        # Turn on the stratification option and use self.validation_rate\n",
        "        #X_train, X_valid, y_train, y_valid = None # CHANGE IT\n",
        "        y = y.astype(int)\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=self.validation_rate, stratify=y, random_state=self.seed)\n",
        "\n",
        "\n",
        "        # Step 2: Compile (initialize) the parameters by running self.compile with correct number of features and outputs\n",
        "        # WRITE YOUR CODE HERE\n",
        "        self.compile(X_train.shape[1],len(np.unique(y)))\n",
        "\n",
        "\n",
        "\n",
        "        #\n",
        "\n",
        "        # Step 3: Prepare one-hot encoded training labels by using one_hot function on y_train\n",
        "        #y_train_enc = None # CHANGE IT\n",
        "        y_train_enc=self.one_hot(y_train)\n",
        "\n",
        "        # Step 4: iterate over training epochs\n",
        "        #for i in range(None):\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            # Step 5: set the indices\n",
        "            # - if self.shuffle is True, shuffle the indices using self.random.shuffle or permutation\n",
        "            #indices = None # CHANGE IT\n",
        "            indices = np.arange(X_train.shape[0])\n",
        "            if self.shuffle:\n",
        "              self.random.shuffle(indices)\n",
        "\n",
        "            # Step 6: iterate over the data\n",
        "            # - For each iteration, you need to choose the data\n",
        "            for start_idx in range(0, indices.shape[0] - self.batch_size +\n",
        "                                   1, self.batch_size):\n",
        "                #batch_idx = None # CHANGE IT\n",
        "                batch_idx = indices[start_idx:start_idx+self.batch_size]\n",
        "\n",
        "                # Step 7: Run a forward propagation\n",
        "                #Z, A = None # CHANGE IT\n",
        "                Z, A = self.forward(X_train[batch_idx])\n",
        "                # Step 8: Run back propagation\n",
        "                # - Use X_train, batch_idx, A, and y_train_enc\n",
        "                # WRITE YOUR CODE HERE\n",
        "                self.back_propagation(X_train, batch_idx, A, y_train_enc)\n",
        "\n",
        "\n",
        "                #\n",
        "\n",
        "            # call evaluate function after inner loop (whole batch cycles) is complete\n",
        "            # WRITE YOUR CODE HERE\n",
        "            self.evaluate(i, X_train,X_valid,y_train,y_valid)\n",
        "\n",
        "            #\n",
        "\n",
        "        # Step 9: After all loops are complete, return self.history\n",
        "        return self.history\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      \"\"\"\n",
        "      Do not need to complete this function.\n",
        "      Leave as it is!\n",
        "      \"\"\"\n",
        "      return self.history\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "      \"\"\"\n",
        "      Score function for pipeline\n",
        "      Leave as it is!\n",
        "      \"\"\"\n",
        "      y_pred = self.predict(X)\n",
        "      acc = np.sum(y == y_pred) / X.shape[0]\n",
        "      return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QAzKAjhhwHu"
      },
      "source": [
        "After you finish developing the methods in the class structure, you can create a new instance by calling 'FullyConnectedNetwork' class. Create your model using the following parameters:\n",
        "\n",
        "- n_hidden = 100\n",
        "- l2 = 0.01\n",
        "- epochs = 50\n",
        "- eta = 0.0005\n",
        "- batch_size = 100\n",
        "- shuffle = True\n",
        "- seed = `RANDOM_STATE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEkq3fydrmVw"
      },
      "outputs": [],
      "source": [
        "#nn = None # CHANGE IT\n",
        "nn = FullyConnectedNetwork(n_hidden=100, l2=0.01, epochs=50, eta=0.0005,\n",
        "                           batch_size=100, shuffle=True, seed=RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXBBoj_oh5Vt"
      },
      "source": [
        "Then the fit methods will run the model for 50 epochs, and depending on your computing power, it might take a few minutes to an hour.\n",
        " - Fit your network on `X_train` and `y_train` and save the output to `history`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSKfzn3czNrU",
        "outputId": "3694ea83-942c-4ce9-8652-5dce9f5d71fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/50 | Cost: 17095.08 | Train/Valid Acc.: 78.01%/78.12% \n",
            "2/50 | Cost: 11089.21 | Train/Valid Acc.: 83.89%/84.21% \n",
            "3/50 | Cost: 8854.16 | Train/Valid Acc.: 86.81%/87.02% \n",
            "4/50 | Cost: 7809.15 | Train/Valid Acc.: 87.96%/87.98% \n",
            "5/50 | Cost: 7138.61 | Train/Valid Acc.: 88.82%/88.82% \n",
            "6/50 | Cost: 6741.27 | Train/Valid Acc.: 89.39%/89.22% \n",
            "7/50 | Cost: 6479.38 | Train/Valid Acc.: 89.85%/89.62% \n",
            "8/50 | Cost: 6844.54 | Train/Valid Acc.: 90.24%/89.93% \n",
            "9/50 | Cost: 6293.83 | Train/Valid Acc.: 90.56%/90.18% \n",
            "10/50 | Cost: 5932.25 | Train/Valid Acc.: 90.82%/90.48% \n",
            "11/50 | Cost: 5751.38 | Train/Valid Acc.: 91.12%/90.70% \n",
            "12/50 | Cost: 5506.55 | Train/Valid Acc.: 91.28%/90.93% \n",
            "13/50 | Cost: 5132.85 | Train/Valid Acc.: 91.54%/91.22% \n",
            "14/50 | Cost: 4953.72 | Train/Valid Acc.: 91.71%/91.40% \n",
            "15/50 | Cost: 5103.13 | Train/Valid Acc.: 91.95%/91.51% \n",
            "16/50 | Cost: 5265.87 | Train/Valid Acc.: 92.12%/91.68% \n",
            "17/50 | Cost: 5011.20 | Train/Valid Acc.: 92.26%/91.87% \n",
            "18/50 | Cost: 4878.71 | Train/Valid Acc.: 92.37%/92.02% \n",
            "19/50 | Cost: 4619.86 | Train/Valid Acc.: 92.62%/92.21% \n",
            "20/50 | Cost: 4806.56 | Train/Valid Acc.: 92.68%/92.20% \n",
            "21/50 | Cost: 4390.79 | Train/Valid Acc.: 92.85%/92.37% \n",
            "22/50 | Cost: 4578.42 | Train/Valid Acc.: 92.98%/92.43% \n",
            "23/50 | Cost: 4541.67 | Train/Valid Acc.: 93.11%/92.63% \n",
            "24/50 | Cost: 4365.64 | Train/Valid Acc.: 93.17%/92.65% \n",
            "25/50 | Cost: 4268.53 | Train/Valid Acc.: 93.36%/92.81% \n",
            "26/50 | Cost: 4055.90 | Train/Valid Acc.: 93.52%/92.90% \n",
            "27/50 | Cost: 3797.74 | Train/Valid Acc.: 93.54%/92.98% \n",
            "28/50 | Cost: 3794.42 | Train/Valid Acc.: 93.66%/93.08% \n",
            "29/50 | Cost: 4233.38 | Train/Valid Acc.: 93.73%/93.14% \n",
            "30/50 | Cost: 3946.96 | Train/Valid Acc.: 93.85%/93.24% \n",
            "31/50 | Cost: 3841.45 | Train/Valid Acc.: 93.97%/93.33% \n",
            "32/50 | Cost: 4055.15 | Train/Valid Acc.: 94.04%/93.41% \n",
            "33/50 | Cost: 3559.38 | Train/Valid Acc.: 94.11%/93.47% \n",
            "34/50 | Cost: 3608.27 | Train/Valid Acc.: 94.17%/93.52% \n",
            "35/50 | Cost: 3637.15 | Train/Valid Acc.: 94.27%/93.62% \n",
            "36/50 | Cost: 3540.94 | Train/Valid Acc.: 94.34%/93.65% \n",
            "37/50 | Cost: 3263.91 | Train/Valid Acc.: 94.43%/93.70% \n",
            "38/50 | Cost: 3209.50 | Train/Valid Acc.: 94.49%/93.78% \n",
            "39/50 | Cost: 3223.20 | Train/Valid Acc.: 94.57%/93.86% \n",
            "40/50 | Cost: 3367.67 | Train/Valid Acc.: 94.66%/93.92% \n",
            "41/50 | Cost: 3129.22 | Train/Valid Acc.: 94.69%/93.92% \n",
            "42/50 | Cost: 3169.33 | Train/Valid Acc.: 94.76%/93.98% \n",
            "43/50 | Cost: 2905.31 | Train/Valid Acc.: 94.81%/94.04% \n",
            "44/50 | Cost: 3162.96 | Train/Valid Acc.: 94.88%/94.08% \n",
            "45/50 | Cost: 3286.73 | Train/Valid Acc.: 94.94%/94.12% \n",
            "46/50 | Cost: 3043.08 | Train/Valid Acc.: 94.97%/94.15% \n",
            "47/50 | Cost: 3025.43 | Train/Valid Acc.: 95.09%/94.20% \n",
            "48/50 | Cost: 2885.00 | Train/Valid Acc.: 95.08%/94.21% \n",
            "49/50 | Cost: 2925.20 | Train/Valid Acc.: 95.16%/94.23% \n",
            "50/50 | Cost: 2605.41 | Train/Valid Acc.: 95.23%/94.29% \n"
          ]
        }
      ],
      "source": [
        "#history = None # CHANGE IT\n",
        "history = nn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGuc6mti3lIp"
      },
      "source": [
        "After the training is done, you should be able to plot the training and validation accuracy scores over time using the `history` dictionary returned by the fit function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "n79B9p-OzPIA",
        "outputId": "86843651-9bf6-464e-ca15-9997c54e5b72"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVhklEQVR4nO3deXhU9d3//+fMJJN9X0kIBAKCIptBYkTRSloq3tSFVqpWKVWs9w3UkvZuQUFc7hp7+70RW7H016LeraVSW9TbDSux0qIIGkREIbJJgOwJ2SbJJJk5vz9OGBgTlkAmk+X1uK5zzcyZc868z6maV8/5LBbDMAxERERE+jirvwsQERER6Q4KNSIiItIvKNSIiIhIv6BQIyIiIv2CQo2IiIj0Cwo1IiIi0i8o1IiIiEi/oFAjIiIi/UKAvwvoKW63m+LiYiIiIrBYLP4uR0RERM6CYRjU19eTkpKC1Xr6ezEDJtQUFxeTlpbm7zJERETkHBw+fJjBgwefdpsBE2oiIiIA86JERkb6uRoRERE5G3V1daSlpXn+jp/OgAk1xx85RUZGKtSIiIj0MWfTdEQNhUVERKRfUKgRERGRfkGhRkRERPoFhRoRERHpFxRqREREpF9QqBEREZF+QaFGRERE+gWFGhEREekXFGpERESkX1CoERERkX5BoUZERET6BYUaERER6RcGzISWIiIi0n0Mw6Ci3sn+CgcHKx0cqGggPT6M71021G81KdSIiIjIKTW1uDhQ2cCBCgcHKhwcrGzgQKWDgxUO6p1tXttOGRHn11BzTo+fVq1aRXp6OsHBwWRlZbFt27ZTbtva2srDDz9MRkYGwcHBjB8/ng0bNnht8+CDD2KxWLyW0aNHe23T3NzM/PnziYuLIzw8nFmzZlFWVnYu5YuIiMhX1DW3sr3oGH/56DCPvrGbHzz3IVf+9ztctHwD1/1qMwv//DFPbPyCl3cUs/NILfXONqwWGBoXytWjEvjBlGHcPCnNr+fQ5Ts169atIzc3l9WrV5OVlcXKlSuZPn06hYWFJCYmdth+6dKlPP/88/zud79j9OjRvPXWW9x44428//77TJw40bPdmDFj2Lhx44nCArxLW7RoEa+//jovvvgiUVFRLFiwgJtuuon33nuvq6cgIiLSL1U7Wtiyv4rN+yp5f38lZXXNBAXYCAqwEhRoJTjARlCg1bMuONCGs83F/nIHpXXNpzxudGggGQnhDIsPY3hCGMPjw8lICGNIXChBAbYePMPTsxiGYXRlh6ysLC699FKeeuopANxuN2lpaSxcuJDFixd32D4lJYX777+f+fPne9bNmjWLkJAQnn/+ecC8U/Pyyy+zY8eOTn+ztraWhIQE1q5dy7e//W0A9uzZw4UXXsiWLVu47LLLzlh3XV0dUVFR1NbWEhkZ2ZVTFhER6ZWaWlx8+GU17+2rZPO+Sj4vqaNrf9W9JUUGMSIxnJGJEWQkhjMyMZwRieHEhdmxWCzdV3gXdOXvd5fu1LS0tFBQUMCSJUs866xWKzk5OWzZsqXTfZxOJ8HBwV7rQkJC2Lx5s9e6vXv3kpKSQnBwMNnZ2eTl5TFkyBAACgoKaG1tJScnx7P96NGjGTJkyClDjdPpxOl0ej7X1dV15VRFRET8qqXNTU1TC7WNrdQ0tVLT2EpNYwu1Ta1UOVrYUVRDwaFjtLjcXvuNSopgyoh4rhgZx8jECJxtbpxtLvO11U1zmwtna/u6VjcWCwxPMMNLVEign862e3Qp1FRWVuJyuUhKSvJan5SUxJ49ezrdZ/r06axYsYKpU6eSkZFBfn4+69evx+VyebbJysriueeeY9SoUZSUlPDQQw9x5ZVXsmvXLiIiIigtLcVutxMdHd3hd0tLSzv93by8PB566KGunJ6IiEiPaHW5Kalp5khNI0eONbUv5vvimiaOOVpwtLjOfCBgUFQwV4yI54qR8WRnxJEYEXzmnfopn/d+evLJJ5k3bx6jR4/GYrGQkZHB3LlzeeaZZzzbXHvttZ7348aNIysri6FDh/KXv/yFO++885x+d8mSJeTm5no+19XVkZbm3wZMIiIyMBiGQZWjhaLqRoqqGjlU1UhRdSOHqxs5cqyR0rpm3GfxmMhigaiQQKJDAokKtRMdEkh0aCBRIYGMTAxnyoh4hsWH+e3RUG/TpVATHx+PzWbr0OuorKyM5OTkTvdJSEjg5Zdfprm5maqqKlJSUli8eDHDhw8/5e9ER0dzwQUXsG/fPgCSk5NpaWmhpqbG627N6X43KCiIoKCgrpyeiIjIWTEMg2pHCyW1zRTXmHdXimubOVTl4FCVGV7OdKfFHmBlcEwIg2NCSY0OaX9vLvHhQUSH2IkIDsBqVWA5W10KNXa7nczMTPLz87nhhhsAs6Fwfn4+CxYsOO2+wcHBpKam0trayt/+9jduvvnmU27b0NDA/v37uf322wHIzMwkMDCQ/Px8Zs2aBUBhYSFFRUVkZ2d35RRERETOyO02KK938mWVg0NVDg5XN1Fc20RJTTMltU2U1DbjbHOf9hgWCyRHBjMkNpShcaEMiQ0lrX0ZHBNCfFiQAks36/Ljp9zcXObMmcOkSZOYPHkyK1euxOFwMHfuXADuuOMOUlNTycvLA2Dr1q0cPXqUCRMmcPToUR588EHcbjc/+9nPPMf86U9/ysyZMxk6dCjFxcUsX74cm83GLbfcAkBUVBR33nknubm5xMbGEhkZycKFC8nOzj6rnk8iIiInMwyDBmcb1Y4Wjhxrag8vjXxZab4eqnbQ3Hr60AIQHx5ESnQwg6KCSYkOYWhsKEPiQhkSG8bgmBCCA3tPd+eBoMuhZvbs2VRUVPDAAw9QWlrKhAkT2LBhg6fxcFFREVbriTH9mpubWbp0KQcOHCA8PJwZM2bwxz/+0esx0pEjR7jllluoqqoiISGBK664gg8++ICEhATPNk888QRWq5VZs2bhdDqZPn06Tz/99HmcuoiI9EcNzjYKS+vZW1ZPeb2TakcLVY4WjrW/VjucHHO0dug19FU2q4W0mBCGxoWRFhvCoKiQ9gATQkpUCElRQb1qjBY5h3Fq+iqNUyMi0r+43AZfVjnYU1LPntI69pSar4erm876GCGBNgZFBzMsLoyhcWGkx4ear3GhpESHEGjTvM/+5rNxakRERHqay21QVN3I3rJ69lU0sK+sgb3lDXxRVn/Kdi1JkUGMSo4kJSqY2DC71xIXFkRsuJ3YUDshdt1p6U8UakREpFeobWrlcLXZ/XlveT37yhvYV25OpHiqR0UhgTYuSI7gwuQIRiVHMDo5ktHJEcSE2Xu4eukNFGpERKRHtLS5OXzM7O58+FgTR6rbx2451sjh6iZqm1pPuW9QgJWMhHBGJoUzov11VHIkQ2JDsakHkbRTqBERkW7X0ubmi7J6Pj1ay6dHa9l1tJY9JfVnbJwbF2YnLTbUM+eQGWIiSI0JUXiRM1KoERGR81LX3MqXlQ4+K647Y4AJtdsYEhvK4JhQ0mJDSIs5MX7L4JgQwoL0Z0nOnf7pERGRM2pqcfFllYMvKx0cqDRfv6xycLDSQWVDS6f7RIUEMjY1iotToxjbvqTFhmhI//7G2QA1ReYSFAHpU/xWikKNiIh4qWlsYecR87HRJ4dr2HW0luLa5tPukxARxAVJ4YxNjVaA6S/cbmhpMJfmWqg5DDWH2gPMoRNBprHqxD4XXKtQIyIi/tHgbGPX0Vo+PVLLJ0dq2HmklqLqxk63jQ4NJD0ujOHxYaTHhzGsfRkaF0pEcGAPVy5eDAOc9eCoMJeGcvPVWQeuVmhzgqvlxNJ2/L3T/M7ZAC317a8N5mur4+x/PzgaoodA7KnndewJCjUiIv2Yy21QVtfs6XFUVN3IkZN6HJXWdX4HJj0ulHGDoxk32LzrckGSukn3CMOAFgc015h3R44vTV/53FwLjZXt4aUSHOXQdvq7aefMYoWgSIhKM4PL8SVm6In3wVG++e0uUqgREenjDMOgrM7JgYoGDlQ6OFDh4EBlA4eqGjlyrJFW1+kHjk+JCmbc4GjGDo5i/GDz8VFUqO68dCtnPdSXQn2JdxDxet9+l6Xt7EdE7iAwDMITICwRwhLMsBFgB1sQ2AIhIAhs9hPL8c9B4WCPaH8N9/4cEGzOztkHKNSIiPQRTS0uDlQ2sL/Cwf5yM8AcrGzgYIUDR4vrlPsFWC2kxpg9jdJiQ8yZomPMHkdDYkOJ1R2Yc+dqhbpiqDtqvh4PLvWl3u+78igHwBoIIdFmKPEsJ3+OhNB4M7iEJ0JY+3t7mC/Oss9QqBER6UUMw6CyoYX9FQ3mUu5gX0UD+8sbOFpz6v8Hb7NaGBIbyvD2di7DE8JJjzdDy6AojfFyTgzDbAR77BDUHobaI2Z48bwehYYy4CynUAyKhPCk9hCScOL1q++Ph5M+cnekN1GoERHxI5fbYHdJHdsOVrPtYDUffllNlaPzLtJgNtYdkRBORkI4wxNOBJghsaHYAzT5YgduNxgucLs6vh5/31Bm9uI5dlKPnuO9e1o7bzTtxWaHyNT2ZZAZXCIGQUTyidfwJPNRjviUQo2ISA9qdbn59GitV4ipb27z2sZigbSYUDISwhiRaAaYjPZXPSo6idt9IpDUFEHNlyfeHztk3lFxn3rqhbMWMchsJBs1GKJSIfL4a6q5LjQerAqUvYFCjYiIDznbXOw8UsvWA1V8cKCagkPHaGr1bv8SHhTApPQYJg+LJWtYHGNSIgkO1OzRHi0OqPwCKgqhYo/5WvmFOW6Ky3keB7aA1QahcSf16jmpR09MuhlaAoK660zExxRqRES6kbPNxSeHa/ngQBVbD1ZRcOgYza3e0wVEhwYyOT3WE2IuHBRBgE3/Tx9nPVR80R5c2sNLxW7zzsupWKzmnZOvdjGOHmoGEnu4eRfFYjMDjNerrnl/o1AjInKOnG0uiqoa2V/hYE9pHVsPVLO96BjONu8QExdmJ2u4GWAuGx7HyMRwrAO54W7TsZPCy0l3X+qOnHqf0HhIGA0Jo9pfLzDvpESmml2VRVCoERE5Lc8YMJUN5vgv7WPAHKhwcORYI+5OOr7Eh9vJGh7HZcNiuWx4HCMSwwfGdAHHR7WtL4WGr3RpPv5afdD87lTCk83AknDhSQFmlNllWeQMFGpERE5iGAaHq5vYcqCSLfur2HKgirK6U7fbCLPbGJ4QTkZCGJPSY7lseCwZCf0oxDgqoWQHNFafNKptTcfRbZtrzMHjznY8lsjB3qHl+N2XkBjfnYv0ewo1IjLgHa1pMgPM/io+OFDVYTwYqwXS2seAGd7elXp4vPmaGBHUfwJMazOU7oQjH8HRj8zXmkNdP05QVHt35k66NkelQfwF5uBxIt1MoUZE+h3DMNhdUk9JbROOFhcOZxsOZxuNx9+3tOFwumhwtlFYWt9hAsdAm4Xxg6PJzogje3gclwyN6V+9kVxt5iOg2qNQfQCOFpghpnRX512g40aaXZg7Hdn2pPdh8WZwGeCj2or/KNSISL9RWFrPq58U8+rOYg5VncWgae1sVgtjU6M8IWZSegyh9j76n0fDMBvi1hS1j4J71Hw9PgJu3VGzfYvh7nz/sARInQSDM83X1Et6zWSFImfSR/+tFRExHapymEHmkxIKy+o960MCbYxMCifMHkBYkI2woABC7QGEB9naXwMIDbKREh3CpemxhAf1of8cOhvMOyxfHf32+OKsO/MxrAEQkQLRaTBowokQEz1Ew/NLn9WH/i0WETGV1TW335Ep4ZPDNZ71gTYLV12QyLcmpJBzYWLfvdtynKvNDC9lu6D8cyj7HMo/g2Nfnnnf8KT2UXA7GQE3MtWca8jajx6piaBQIyJ9RKvLTf7ucl74sIhNX1RgtHeltlrg8ox4vjU+heljkokK7aNjljRWQ+mn5lL2mRlkKgpPPWJuaPxXBpsbAtHp7a9pEBjSo+WL9AYKNSLSqx2qcvDCh4f5a8ERKupP/IHPHBrDt8ancO3YZBIjgv1YYRcZhvmIqPRTs6fR8SBTe7jz7QPDIPFCSLoIEsdAUvsSGtuzdYv0AQo1ItLrONtc/P2zMv68rYj391d51seH25mVOZjvXjqEYfF9oIeNYZiPj4o/Nsd6Kd5hBpnm2s63j0mH5HGQdHF7eLnIvPui4fxFzopCjYj4XX1zK4eqGjlc3chHh46xfvsRjjWaXYstFrhyZAK3XJrGtAuTsAf00j/whmE22C3+uH3ZYQaZzgKMNRASR5sBJnkcJI+F5IvVy0jkPCnUiEiPqGtuZXdxHYeqGymqaqSoupFD1WaQqXa0dNg+OTKYmycN5juT0kiLDfVDxZ0wDLPtS/UBOHbQfK1uf63aa3al/ipbkBlYUiaavYwGjTdHzw2w93j5Iv2dQo2I+ERzq4uPvjzGe/sreX9/FZ8eqel0nqTj4sLsnlF7rxs3iKsuSPDvzNVNNVC8HY4UmI12jx00A8zpuktbA83HRikT25cJ5hxGCjAiPUKhRkS6RavLzc4jNby3r4r391ey/VANLS7vAd5So0MYnhBGWmwoQ2NDGRoXSlpsKENiQ4kI9mOvJVer2ePo6EdmiDn6EVR+certI1MhZhjEHl+Gm0vCaAgI6rm6RcSLQo2InLPD1Y28W1jOu4UVfHCgCkeLy+v75MhgLh8Rx+UZ8VyeEUdKtJ+7Gbvd5oi6VXuhcp/5WrLTbPvS1txx++ihMHiS+dgoboQZXGKGqru0SC+lUCMiZ83Z5uLDg8f4R2E57xaWs7/Ce0bmmNBAsjNOhJhh8WH+meyxzQnlu81xXqr2QuVeqNoHVfuhranzfYKjIDWzfZlkvoYn9GzdInJeFGpE5LSO1jTxjz1miHl/fxWNJ92NsVktZA6N4epRCVx1QQIXJkditfZwiHG1mqPtHu9xVPyx+Sips4kZwWz3EjvMnKQxLgMSLzLvxsRmqOu0SB+nUCMiHdQ3t/Lmp6X8dfsRth2s9vouMSKIq0clcPWoRKaMiCcqpIfbwjgq4Yu3zJmljweYzkbdDYkxB6uLH2EGmPiR5iOk6KFg03/6RPoj/ZstIgC0udxs3lfJ+u1HeeuzUpxtZiNfiwUyh8TwtdGJXD0qgYsGRfb8I6WmY7D7NfhsPRzYBIZ32x2Co050mT7e80gTM4oMOAo1IgPcntI6/lZwhJd3FHtNQ5CREMaszMHcODGVQVF+aBjbXAeFb8Cu9bD/He/HSYPGw7CrzC7TKRPNnkgKMCIDnkKNSD/ndhtUOpyU1DRTXNNEcW0zJTVNlNQ2s7e8ni/KGjzbxoQGcv2EVG66JJWxqVG+uyNjGOBqgdZGaG1uf20yl5pD8NlLsPdt78dKiWPg4hthzE1mWxgRka84p1CzatUqHn/8cUpLSxk/fjy//vWvmTx5cqfbtra2kpeXx//+7/9y9OhRRo0axS9/+Uu++c1verbJy8tj/fr17Nmzh5CQEC6//HJ++ctfMmrUKM82V199NZs2bfI69g9/+ENWr159Lqcg0m+1udz8teAIL+84ytGaJkprm2l1nXrUu0CbhWmjk7jpklSuHpXY/dMQuN1w4B348Bk4sg1aGs0eSIb7zPvGjYSLbzKDTOLo7q1LRPqdLoeadevWkZuby+rVq8nKymLlypVMnz6dwsJCEhMTO2y/dOlSnn/+eX73u98xevRo3nrrLW688Ubef/99Jk6cCMCmTZuYP38+l156KW1tbdx333184xvf4PPPPycs7MSkdfPmzePhhx/2fA4N7SVDp4v0AoZh8NZnpfz3W4Uc+EpXa6sFEiOCGRQdTEpUCIOighkUHUJqdDBZw+KICfPBiLdNx2DHWvjw9+Y0AqdisUFgKAQGm+O/BEfBiBy4eJY5saMeK4nIWbIYhnGagcs7ysrK4tJLL+Wpp54CwO12k5aWxsKFC1m8eHGH7VNSUrj//vuZP3++Z92sWbMICQnh+eef7/Q3KioqSExMZNOmTUydOhUw79RMmDCBlStXdqVcj7q6OqKioqitrSUyMvKcjiHSW23ZX8UvN+xhx+EawHyMdM9VGWQOjWFQdAiJEUEE9tSUAyWfwLbfwad/PTEmTFAkTLgVxt4MobFmeAkMgYAQsAUquIjIKXXl73eX7tS0tLRQUFDAkiVLPOusVis5OTls2bKl032cTifBwcFe60JCQti8efMpf6e21pzVNjY21mv9n/70J55//nmSk5OZOXMmy5YtO+XdGqfTidN54nl8Xd1p5msR6aM+L67jlxv2sOmLCgBCAm3cdeUw5k0dTmRPTjvQ2gyfvwIf/g6OfHhifeIYmHyXGWaCwnuuHhEZkLoUaiorK3G5XCQlJXmtT0pKYs+ePZ3uM336dFasWMHUqVPJyMggPz+f9evX43K5Ot3e7Xbz4x//mClTpnDxxRd71t96660MHTqUlJQUdu7cyc9//nMKCwtZv359p8fJy8vjoYce6srpifQZh6sb+Z+/F/LKJ8UYBgRYLdwyeQgLp40gMSL4zAc4Fy2NJyZ19Jql+gDUHjnRRsYaCBd9Cy6dB0Mu010YEekxPu/99OSTTzJv3jxGjx6NxWIhIyODuXPn8swzz3S6/fz589m1a1eHOzl333235/3YsWMZNGgQ06ZNY//+/WRkdOwJsWTJEnJzcz2f6+rqSEtL66azEulZrS43nxfX8dGhY3x4sJr8PWWexr8zx6fwk69fQHp82BmO0kVNx2D3q/DZy+aIvfUlp98+MhUy58Ild0BE0um3FRHxgS6Fmvj4eGw2G2VlZV7ry8rKSE5O7nSfhIQEXn75ZZqbm6mqqiIlJYXFixczfPjwDtsuWLCA1157jX/+858MHjz4tLVkZWUBsG/fvk5DTVBQEEFBmi1X+qb65la2F9VQ8GU1H355jB2Ha2hq9b67eeXIeH42fTRjB0d13w+3Npmj9X76Iuz9u9nt+mTB0SdmpY45Pjt1+2t4ku7KiIhfdSnU2O12MjMzyc/P54YbbgDMx0X5+fksWLDgtPsGBweTmppKa2srf/vb37j55ps93xmGwcKFC3nppZd49913GTZs2Blr2bFjBwCDBg3qyimI9EqtLjcfHqxm4+5yPjhQxZ7SOtxfacIfFRLIpKExZKbHcHlGPBPSorvnx90uOLjJbNi7+1VwntT+LHEMjPsOpE81w0to7KmPIyLiZ11+/JSbm8ucOXOYNGkSkydPZuXKlTgcDubOnQvAHXfcQWpqKnl5eQBs3bqVo0ePMmHCBI4ePcqDDz6I2+3mZz/7meeY8+fPZ+3atbzyyitERERQWloKQFRUFCEhIezfv5+1a9cyY8YM4uLi2LlzJ4sWLWLq1KmMGzeuO66DSI+rbWzl3S/K2bjbnCyyvrnN6/shsaFMSo9h0tBYLk2PISMh/Owmi6wrgY+fN4MKQEAQ2OxmLyNb+/sAu/na1gx73gBH+Yn9o9Jg7Ldh7HcgaUw3nrGIiG91OdTMnj2biooKHnjgAUpLS5kwYQIbNmzwNB4uKirCetJMt83NzSxdupQDBw4QHh7OjBkz+OMf/0h0dLRnm9/85jeA2W37ZM8++yzf//73sdvtbNy40ROg0tLSmDVrFkuXLj2HUxbxn0NVDt7+vIz83eVs+7Ia10m3Y+LC7FwzOpGrRyVyaXoMiZFdaPDrdsOBf8BHz0Dhmx3nRjqTkFgYc6MZZNKyNFu1iPRJXR6npq/SODXiL4eqHPzfjmJe3VnsNSUBwAVJ4eRcmMS0C5OYkBaN7WzuxJysody8K7P9f+HYlyfWD8mG8d8Fezi4Ws22MceXNmf7Oqf56GlINmRcY969ERHpZXw2To2InJ3y+mZe31nCKzuKPQPigdn1evKwWHIuTCLnwiSGxJ3DqNiGAV/+y7wrs/u1ExM9BkWZQWbSXEi8sHtORESkD1GoEekmdc2tvLWrlP/7pJj39lV6GvpaLTBlRDzfGp/CN8YkExVyDoPiud1w9CNzgLvP/w9qi058lzrJDDJjbgK7pg4RkYFLoUbkPH1cdIzf/esAG3eX09J2YpLGCWnRXD8hhevGDTq3AfHcLji89USQqS8+8Z09HMbdbI4LM0iN5UVEQKFG5JztOFzDyo1f8G5hhWddRkIYN0xI5VsTUhgadw6D4bldcOg9M8jsfhUaThoTyh4Bo74JF10PGdN0V0ZE5CsUakS6aOeRGp54+wv+0R5mbFYLN01MZc7l6YxJicRyLgPQ1RyG7X8wl4bSE+uDomD0DDPIDP+aOZO1iIh0SqFG5Cx9eqSWlRu/IH+POaaLzWrhxompLPjaiHObosDtgr1vQ8Gz5ui9x+dOCo6G0f8GY26AYVepV5KIyFlSqBE5g11Ha1m5cS8bd5uPgqwWuGFiKj+6ZuS5hZm6Evj4j1Dwv1B35MT69CvNBr+jZyrIiIicA4UakU7UNrXy5qclrP/4KNsOVgPtYWZCKguuGcHwhPCuHdDtah8c71nvwfFCYmDCbZD5fYgf2b0nISIywCjUiLRraXOz6YsKXvr4iFdPJqsFvjU+hYXTRpLR1TBT9hl88oI5QeTJs1wPyTZ7Ll10vdrJiIh0E4UaGdAMw+DjwzW8tP0or+0s5lhjq+e7C5LCuXHiYK6fkEJKdMjZH7S+DHb9FT75M5R+emJ9SAyMvVmD44mI+IhCjQxIzjYXz39QxPMfHOJgpcOzPiEiiOvHp3DjJalcNKgLPZlam2DP6+Zdmf3vnHi8ZA2EC6abI/2O/IY5uaSIiPiEQo0MKIZh8OauUh57cw9F1Y0AhATa+ObFydw4MZXLM+IIsJ3FZI7NdeYIv4e3mQPkHd4GLSfN6zT4UjPIjLkJQmN9dDYiInIyhRoZMLYXHeMXr++m4NAxwLwr8+OckdwwIZWwoNP8q2AY5mSRJweY8s9OdME+LnoIjPsujJsN8SN8dyIiItIphRrp9w5XN/LLDXt4bafZUDck0MbdU4dz99Thpw4zhgFHt8OO581JIx3lHbeJHgppWZA22XxNuhisZ3GXR0REfEKhRvqt2qZWVv1jH8+99yUtLjcWC3z7ksH85BujSI46RY+j+jLYuQ52/Akq9pxYbw2ElAntIaY9yEQk98h5iIjI2VGokX6ntLaZl3ccZfWm/dS092aaMiKO+2ZcyJiUqI47tLXAFxvMILP37RONfAOC4cJvwYRbYMjl6notItLLKdRIv3DM0cIbu0r4vx3FbPuyGsMw149IDOf+GRdy9aiEjj2ZqvbDtt/Bp3+BxqoT6wdfag6Id/FNENxJCBIRkV5JoUb6rAZnG3//rJT/+6SYzXsraXMbnu8yh8Zw86TBzLpkcMfeTC2N8M/H4f1fg7t9XJrwJLO30oTbIGFUD56FiIh0F4Ua6VMMw2Dj7nJe+vgI+bvLcbad6IE0JiWSmeNT+LdxgxgcE9rZzuZYMhsWQ+1hc13GNMj6oflq078OIiJ9mf4rLn3GgYoG7n9pF1sOnHhUNDw+jJnjU5g5PoURiaeZwqD6ILz5c9j7lvk5Kg2u/SWMmgFnO8CeiIj0ago10uu1tLlZvWk/T/1jHy1tboIDrdx+2VCun5DKmJQzjPrb2gzvPQmbV0Bbs9mL6fKFMPWnYD+HGbZFRKTXUqiRXu3DL6tZsv5T9pWbo/VeOTKeX9wwliFxnTxe+qq9G+HN/4TqA+bnYVfBjP8HCRf4sGIREfEXhRrplWobW8l7czcvfGi2fYkPt7Ps3y7iW+NTzjwfU+mn8I88KHzd/BwxCKb/wpyyQI+aRET6LYUa6VUMw+D/Pinmkdc+p7KhBYBbJqfx82+OJjrUfvqdiz+GTY+fCDMWG1z273D1YgiK8HHlIiLibwo10mvsr2jgoVc/559fVADmGDOP3jiWycPOMCHkkQLY9MsTjYCxwMWzYOp/QuJo3xYtIiK9hkKN+N0xRwtP5u/l+Q8O0eY2sAdYWfi1EfzwqgzsAaeZS6loqxlm9uebny1WGHszXPkTtZsRERmAFGrEb1ra3Pxhy5f8Kn8vdc1tAEwbncj9113I8ITTdM/+8j0zzBzcZH622MyB8678CcRl9EDlIiLSGynUSI8zDIO3PivjsTd382VVIwCjkyNY9m8XMWVE/Kl2gn358K//B0VbzHXWAJhwK1yRC7HDeqh6ERHprRRqpEd9eqSWR17/nG0HqwFIiAjip9+4gG9npmGzdtIzye2GPa/Bv/4HSnaY62x2czqDK3MhekjPFS8iIr2aQo30iPK6Zh7bsIf1248CEBRg5e6pw/nhVRmEB3Xyj6GrDXb9Ff61AioLzXWBoZA5Fy5fAJEpPVi9iIj0BQo14lOGYfDKjmKW/99n1DaZk0feODGV/5w+ipTokI47tDbDJ2th80qoOWSuC4qCyfPgsv+AsLieK15ERPoUhRrxmcoGJ/e/9ClvfVYGwNjUKP7rhosZnxbdcWPDgJ1/gY3Lob7EXBcaD9n/AZfeBcFRPVe4iIj0SQo14hNvflrC/S/votrRQoDVwo+mjeTfr84g0NZJF+3KvfDaIvjyX+bniBSY8iO4ZA7Yz2I6BBERERRqpJvVNLbwwCuf8X+fFANmr6b/uXk8Y1I6udPS2mw2AH5vJbhaICAYrvoZZC+AgKCeLVxERPo8hRrpNvm7y1i8/lMq6p3YrBb+/aoMfjRtZOcD6O1/B17/yYnJJkd8HWY8rq7ZIiJyzhRq5LzVNbfy8Kuf89eCIwBkJITxPzdPYEJnbWfqy+Ct+8yeTWBONvnNx+Ci6zXZpIiInBeFGjkvhaX1zPvDRxRVN2KxwF1XDOMn3xhFcKDNe0O3Cz56BvIfAWetOaXB5Lvha/dDcKR/ihcRkX7lNBPrnNqqVatIT08nODiYrKwstm3bdsptW1tbefjhh8nIyCA4OJjx48ezYcOGLh+zubmZ+fPnExcXR3h4OLNmzaKsrOxcypdusmFXKTc+/R5F1Y0MjgnhLz/M5v7rLvIONIYBezfCb6fCGz81A03KRJj3Dlz7SwUaERHpNl0ONevWrSM3N5fly5ezfft2xo8fz/Tp0ykvL+90+6VLl/Lb3/6WX//613z++efcc8893HjjjXz88cddOuaiRYt49dVXefHFF9m0aRPFxcXcdNNN53DKcr7cboMn3v6Ce54voLHFxeUZcby64AouTf/KbNrFH8MfvgV/mgVlu8zxZq59HO7KN4ONiIhIN7IYhmF0ZYesrCwuvfRSnnrqKQDcbjdpaWksXLiQxYsXd9g+JSWF+++/n/nz53vWzZo1i5CQEJ5//vmzOmZtbS0JCQmsXbuWb3/72wDs2bOHCy+8kC1btnDZZZedse66ujqioqKora0lMlJ3B85Vg7ON3HU7+Pvn5l2yuVPSuX/GhQSc3FW7+iC88wjs+pv52WY3HzVd+RMIje3kqCIiIp3ryt/vLrWpaWlpoaCggCVLlnjWWa1WcnJy2LJlS6f7OJ1OgoODvdaFhISwefPmsz5mQUEBra2t5OTkeLYZPXo0Q4YMOWWocTqdOJ1Oz+e6urqunKp04lCVg3l/+Igvyhqw26z84saL+c6ktBMbOCrhn4/Dh2vAbY4ezNib4ZqlEDPUP0WLiMiA0aVQU1lZicvlIikpyWt9UlISe/bs6XSf6dOns2LFCqZOnUpGRgb5+fmsX78el8t11scsLS3FbrcTHR3dYZvS0tJOfzcvL4+HHnqoK6cnp/GvvRUsWPsxtU2tJEYEsfr2TC4ZEmN+2dIIHzxtTm3QUm+uy7gGch6EQeP9VbKIiAww59RQuCuefPJJRo4cyejRo7Hb7SxYsIC5c+ditfr2p5csWUJtba1nOXz4sE9/r78yDIPf/+sAc57ZRm1TKxPSonl14RUnAs3+f8DTWebjppZ6SB4Ht78Mt7+kQCMiIj2qS3dq4uPjsdlsHXodlZWVkZyc3Ok+CQkJvPzyyzQ3N1NVVUVKSgqLFy9m+PDhZ33M5ORkWlpaqKmp8bpbc7rfDQoKIihIo9KejzaXm8XrP/WMP/PtzMH81w0Xm72bmmvh70th+x/MjaPSYNpyuHgW+DiwioiIdKZLf33sdjuZmZnk5+d71rndbvLz88nOzj7tvsHBwaSmptLW1sbf/vY3rr/++rM+ZmZmJoGBgV7bFBYWUlRUdMbflXPT6nJz7ws7+GvBEWxWC8tnXsTj3x5nBpov3oJVl50INJPvhv/4AMZ9R4FGRET8psuD7+Xm5jJnzhwmTZrE5MmTWblyJQ6Hg7lz5wJwxx13kJqaSl5eHgBbt27l6NGjTJgwgaNHj/Lggw/idrv52c9+dtbHjIqK4s477yQ3N5fY2FgiIyNZuHAh2dnZZ9XzSbqm1eXmR3/+mDd3lWK3WXn6tkvIuSgJGqthw2LYuc7cMHY4fOspSJ/i34JFREQ4h1Aze/ZsKioqeOCBBygtLWXChAls2LDB09C3qKjIq71Mc3MzS5cu5cCBA4SHhzNjxgz++Mc/ej1GOtMxAZ544gmsViuzZs3C6XQyffp0nn766fM4delMS5ubhX/ezluflWG3WVl9+yVcMzoJPn/FnKvJUWGOBpw9H66+T7Noi4hIr9HlcWr6Ko1Tc2YtbW7mr93O25+XYQ+w8tvbM/laKuZIwJ+/Ym6UMBquXwWDJ/m1VhERGRh8Nk6N9F/ONhfz/7SdjbvLsQdY+d0dk7gq+AA8fSs0VoLFBlfmwtT/hAA1wBYRkd5HoUZwtrn49+e3886ecoICrPx+ziSubHoX1s0HlxMSx8CNv1EXbRER6dUUaga45lYX9zxfwLuFFQQHWllzxySmHPkdbPqlucHof4Ob/j+wh/m3UBERkTNQqBnAmltd3P3HAv75hRlonvveWC775Ocn5myaci9Me1DdtEVEpE9QqBmgmltdzPvDR/xrbyUhgTae/246mf/6Phz5EKwB8G9PwCV3+LtMERGRs6ZQMwAZhsGS9Z/yr72VhNptvHB9JOP+/h2oLYLgaJj9Rxg21d9lioiIdIlCzQD0m037eenjo9isFl68poExb91lztsUmwG3/gXiR/i7RBERkS5TqBlg/v5ZKY+/VQjAn8ftYMymx8Fww9ArzDs0obF+rlBEROTcKNQMIJ8X1/HjdTuwGi7+NPglJu9Zb34x8Xtw3RMQYPdvgSIiIudBoWaAqKh3Mu8PH2FrqeelqN8wrrIAsEDOg2YvJ4vF3yWKiIicF4WaAcDZZo5FY639kldDV5DuPAyBoeb4MxfO9Hd5IiIi3UKhpp873tOJog94JWgFse56iBgEt7wAKRP8XZ6IiEi3Uajp5377zwO4dqxjrf3/I4g2SB4Ht66DyBR/lyYiItKtFGr6sbc/K6Hl7Ud40v6SuUJTHoiISD+mUNNP7TlcRttf5vKjgC0AGJffiyXnQU15ICIi/ZZCTT9UXVGC65mZXGvZSxs2mLmSgExNeSAiIv2bQk0/dOD5e5lk7KWOcKzffZ7w0V/zd0kiIiI+p2cR/czBTzczqfYtAEr+7Q8KNCIiMmAo1PQnhoHztSUAbIvIYdSkaX4uSEREpOco1PQju9/9M6OdO2k2AkmZ9ai/yxEREelRCjX9hLvVSdS/HgZgW/ItDE4f5eeKREREepZCTT/x2SsrSHGXUGVEMWb2g/4uR0REpMcp1PQDzvpKhu56CoCdoxYQFxvn54pERER6nkJNP/DFXx4gkgb2WoZw2U0/9nc5IiIifqFQ08fVH9nN6MMvAFB86VJCgu1+rkhERMQ/FGr6uOK//ZxAXGwNmMQV37zZ3+WIiIj4jUJNH1bx6UZGHdtEm2GFrz+MzWrxd0kiIiJ+o1DTV7ndNL9uDrT3Tth1TJ58uZ8LEhER8S+Fmj7qyKZnSWv+gjojhNQbH8Ji0V0aEREZ2BRq+qIWB6GbzRGD/5FwB2NGZvi5IBEREf9TqOmDvnz1l8S6KjlsJHDJzYv9XY6IiEivoFDTx7hqi0n+dDUA2zJ+RFpirJ8rEhER6R0UavqYor8tIxgnO7iAa276ob/LERER6TUUavoQ49gh0opeAuDAxJ8TEx7k54pERER6D4WaPqT8zV8SgIv3jbHkTL/e3+WIiIj0Kgo1fUXtEeK+WAfAzowfEhkc6OeCREREeheFmj6i6d0VBNDGFtdFTLnmW/4uR0REpNdRqOkL6koI3PFHAF6P+R5jB0f5uSAREZHe55xCzapVq0hPTyc4OJisrCy2bdt22u1XrlzJqFGjCAkJIS0tjUWLFtHc3Oz5Pj09HYvF0mGZP3++Z5urr766w/f33HPPuZTf5xjvPUmA0cI29yjGXTnT3+WIiIj0SgFd3WHdunXk5uayevVqsrKyWLlyJdOnT6ewsJDExMQO269du5bFixfzzDPPcPnll/PFF1/w/e9/H4vFwooVKwD48MMPcblcnn127drF17/+db7zne94HWvevHk8/PDDns+hoaFdLb/vqS/D/dEz2IDfW7/Dk+NT/V2RiIhIr9TlULNixQrmzZvH3LlzAVi9ejWvv/46zzzzDIsXdxzd9v3332fKlCnceuutgHlX5pZbbmHr1q2ebRISErz2eeyxx8jIyOCqq67yWh8aGkpycnJXS+7btvwam8vJdvcIUiddS4jd5u+KREREeqUuPX5qaWmhoKCAnJycEwewWsnJyWHLli2d7nP55ZdTUFDgeUR14MAB3njjDWbMmHHK33j++ef5wQ9+0GGSxj/96U/Ex8dz8cUXs2TJEhobG09Zq9PppK6uzmvpcxyVuLf9HoBftd3EbZel+7ceERGRXqxLd2oqKytxuVwkJSV5rU9KSmLPnj2d7nPrrbdSWVnJFVdcgWEYtLW1cc8993Dfffd1uv3LL79MTU0N3//+9zscZ+jQoaSkpLBz505+/vOfU1hYyPr16zs9Tl5eHg899FBXTq/32fIU1rYmPnEPxzn0GkYkhvu7IhERkV6ry4+fuurdd9/l0Ucf5emnnyYrK4t9+/Zx77338sgjj7Bs2bIO269Zs4Zrr72WlJQUr/V333235/3YsWMZNGgQ06ZNY//+/WRkdJylesmSJeTm5no+19XVkZaW1o1n5mON1RjbfocF+HXbjXwvO93fFYmIiPRqXQo18fHx2Gw2ysrKvNaXlZWdsq3LsmXLuP3227nrrrsAM5A4HA7uvvtu7r//fqzWE0/ADh06xMaNG0959+VkWVlZAOzbt6/TUBMUFERQUB+eRuCDp7G0NPC5eyg7QrJ5+qKkM+8jIiIygHWpTY3dbiczM5P8/HzPOrfbTX5+PtnZ2Z3u09jY6BVcAGw2s7GrYRhe65999lkSExO57rrrzljLjh07ABg0aFBXTqFvaDoGW38LwJNtN/LdyUOwB2hIIRERkdPp8uOn3Nxc5syZw6RJk5g8eTIrV67E4XB4ekPdcccdpKamkpeXB8DMmTNZsWIFEydO9Dx+WrZsGTNnzvSEGzDD0bPPPsucOXMICPAua//+/axdu5YZM2YQFxfHzp07WbRoEVOnTmXcuHHnc/6909bfgrOOPe40NhqTeCBriL8rEhER6fW6HGpmz55NRUUFDzzwAKWlpUyYMIENGzZ4Gg8XFRV53ZlZunQpFouFpUuXcvToURISEpg5cya/+MUvvI67ceNGioqK+MEPftDhN+12Oxs3bvQEqLS0NGbNmsXSpUu7Wn7v11wLHzwNmG1prh6VTGp0iJ+LEhER6f0sxlefAfVTdXV1REVFUVtbS2RkpL/LObV/Pg7v/Bf7SeXrzb9kzfez+NrojoMaioiIDARd+futhhq9ibMetqwC4MmWG0iJCWPqBQln2ElERERAoaZ3+fD30HSMo7ZUXnNnc2vWEGxWy5n3ExEREYWaXqXgOQBWNJmNqG+e1IfG1REREfEzhZrewtkAx74EIN89kW9ePIj48D48zo6IiEgPU6jpLSoKAag0oqghgu+pG7eIiEiXKNT0FhW7ASh0D2ZkYjiTh8X6uSAREZG+RaGmtyg3Q80XxmBuyxrSYYZyEREROT2Fml7C3R5q9hqD+fqYzufREhERkVNTqOklXGVmqDlsG0JKVLCfqxEREel7FGp6g+Y6AhuKAWiLH6VHTyIiIudAoaY3aO/5VGZEMyi5H846LiIi0gMUanqD9p5PX7gHMyIx3M/FiIiI9E0KNb1B+R7AbCQ8MjHCz8WIiIj0TQo1vYBxUndu3akRERE5Nwo1vcDxnk8HLWmkxYT4uRoREZG+SaHG35pqCHCUANAWewEBNv1PIiIici70F9Tf2ns+lRixJCdr0D0REZFzpVDjb+09n/a6Uxmp9jQiIiLnTKHG39p7PqmRsIiIyPlRqPEzo0I9n0RERLqDQo2fudt7Pu0zBjMsPszP1YiIiPRdCjX+1HQMm6MMAGfMBQQF2PxckIiISN+lUONP7e1pjhpxpCQl+rkYERGRvk2hxp88PZ/UnkZEROR8KdT408k9nxIUakRERM6HQo0/Hb9TY6QyMkmhRkRE5Hwo1PiR+/hElu7BZOhOjYiIyHlRqPGXxmqsjgrzbeQIwoIC/FyQiIhI36ZQ4y/td2mOGPEMSkrwczEiIiJ9n0KNv5w055MaCYuIiJw/hRp/OannkxoJi4iInD+FGn+pMEPNXs35JCIi0i0UavzEOH6nxq0xakRERLqDQo0/OKqwNJo9n46FphMTZvdzQSIiIn2fQo0/tDcSPuxOICVRPZ9ERES6g0KNPxwfdE+NhEVERLqNQo0/nNxIWO1pREREuoVCjT94GgmnMjIpws/FiIiI9A/nFGpWrVpFeno6wcHBZGVlsW3bttNuv3LlSkaNGkVISAhpaWksWrSI5uZmz/cPPvggFovFaxk9erTXMZqbm5k/fz5xcXGEh4cza9YsysrKzqV8vzMqTjx+UnduERGR7tHlULNu3Tpyc3NZvnw527dvZ/z48UyfPp3y8vJOt1+7di2LFy9m+fLl7N69mzVr1rBu3Truu+8+r+3GjBlDSUmJZ9m8ebPX94sWLeLVV1/lxRdfZNOmTRQXF3PTTTd1tXz/a6jA0liF27BQZh9KYkSQvysSERHpF7o8i+KKFSuYN28ec+fOBWD16tW8/vrrPPPMMyxevLjD9u+//z5Tpkzh1ltvBSA9PZ1bbrmFrVu3ehcSEEBycnKnv1lbW8uaNWtYu3Yt11xzDQDPPvssF154IR988AGXXXZZV0/Df473fDISGJwch8Vi8XNBIiIi/UOX7tS0tLRQUFBATk7OiQNYreTk5LBly5ZO97n88sspKCjwPKI6cOAAb7zxBjNmzPDabu/evaSkpDB8+HBuu+02ioqKPN8VFBTQ2trq9bujR49myJAhp/xdp9NJXV2d19IrnDQ9ghoJi4iIdJ8u3amprKzE5XKRlJTktT4pKYk9e/Z0us+tt95KZWUlV1xxBYZh0NbWxj333OP1+CkrK4vnnnuOUaNGUVJSwkMPPcSVV17Jrl27iIiIoLS0FLvdTnR0dIffLS0t7fR38/LyeOihh7pyej3j+ESW6s4tIiLSrXze++ndd9/l0Ucf5emnn2b79u2sX7+e119/nUceecSzzbXXXst3vvMdxo0bx/Tp03njjTeoqanhL3/5yzn/7pIlS6itrfUshw8f7o7TOX8nT4+gRsIiIiLdpkt3auLj47HZbB16HZWVlZ2yPcyyZcu4/fbbueuuuwAYO3YsDoeDu+++m/vvvx+rtWOuio6O5oILLmDfvn0AJCcn09LSQk1NjdfdmtP9blBQEEFBvawRrmFgVOzGwvExatSdW0REpLt06U6N3W4nMzOT/Px8zzq3201+fj7Z2dmd7tPY2NghuNhsNgAMw+h0n4aGBvbv38+gQYMAyMzMJDAw0Ot3CwsLKSoqOuXv9koN5ViajuEyLBwJGExqTIi/KxIREek3utz7KTc3lzlz5jBp0iQmT57MypUrcTgcnt5Qd9xxB6mpqeTl5QEwc+ZMVqxYwcSJE8nKymLfvn0sW7aMmTNnesLNT3/6U2bOnMnQoUMpLi5m+fLl2Gw2brnlFgCioqK48847yc3NJTY2lsjISBYuXEh2dnaf7PlUZCSSGh+LzaqeTyIiIt2ly6Fm9uzZVFRU8MADD1BaWsqECRPYsGGDp/FwUVGR152ZpUuXYrFYWLp0KUePHiUhIYGZM2fyi1/8wrPNkSNHuOWWW6iqqiIhIYErrriCDz74gISEE5M9PvHEE1itVmbNmoXT6WT69Ok8/fTT53PuPa/8pOkR1J5GRESkW1mMUz0D6mfq6uqIioqitraWyMhI/xTx6r1Q8BxPtV2P8bVlLJw20j91iIiI9BFd+futuZ96kno+iYiI+IxCTU9p7/kEGqNGRETEFxRqekp9KZbmWlyGhUOWFIbGhfm7IhERkX5FoaantN+l+dJIZlBcNIE2XXoREZHupL+sPeWknk8jEzXonoiISHdTqOkp7XdqvjBS1UhYRETEBxRqesrxOzVuNRIWERHxBYWanmAYUNHendsYTEaCQo2IiEh3U6jpCY1V4KwD4EsGKdSIiIj4gEJNT3BUAnDMCCcxJpIQu83PBYmIiPQ/CjU9obEKgGojghG6SyMiIuITCjU9oT3U1BDOyCR15xYREfEFhZqe0FQN6E6NiIiILynU9IT2OzXHjAgyNEaNiIiITyjU9ICWerOhcDURGnhPRETERxRqekBLbQUADmskUSGBfq5GRESkf1Ko6QGu9i7dLUExfq5ERESk/1Ko6QntbWrcwQo1IiIivqJQ0wOszccAMELj/FyJiIhI/6VQ0wPsLWaosYXH+7kSERGR/kuhxtdcrQS1NQBgj0jwczEiIiL9l0KNrzWaA++5DAuhUbF+LkZERKT/UqjxtfZGwrWEERMW4udiRERE+i+FGl9rnyLhmBFBbJjdz8WIiIj0Xwo1vnZ8hm4UakRERHxJocbXTpr3SaFGRETEdxRqfKytoX3eJ4UaERERn1Ko8TFn+7xPtZYIIoM175OIiIivKNT4WGv7DN3NgdFYrRY/VyMiItJ/KdT4mLu9TU2rJrMUERHxKYUaH7O0d+l2h2jgPREREV9SqPGxgGYz1Fg0maWIiIhPKdT4mL2lBoCACE1mKSIi4ksKNb7U1kKQywFAkGboFhER8SmFGl9qOnkySz1+EhER8SWFGl86PpowEcSEB/u5GBERkf5NocaXTpoiIS4syM/FiIiI9G8KNb500mSWMWEaTVhERMSXzinUrFq1ivT0dIKDg8nKymLbtm2n3X7lypWMGjWKkJAQ0tLSWLRoEc3NzZ7v8/LyuPTSS4mIiCAxMZEbbriBwsJCr2NcffXVWCwWr+Wee+45l/J7jOEwQ02NEa47NSIiIj7W5VCzbt06cnNzWb58Odu3b2f8+PFMnz6d8vLyTrdfu3YtixcvZvny5ezevZs1a9awbt067rvvPs82mzZtYv78+XzwwQe8/fbbtLa28o1vfAOHw+F1rHnz5lFSUuJZ/vu//7ur5fcoZ50571O1oTs1IiIivhbQ1R1WrFjBvHnzmDt3LgCrV6/m9ddf55lnnmHx4sUdtn///feZMmUKt956KwDp6enccsstbN261bPNhg0bvPZ57rnnSExMpKCggKlTp3rWh4aGkpyc3NWS/cZZV0Ew4LBFERRg83c5IiIi/VqX7tS0tLRQUFBATk7OiQNYreTk5LBly5ZO97n88sspKCjwPKI6cOAAb7zxBjNmzDjl79TW1gIQG+s9tcCf/vQn4uPjufjii1myZAmNjY2nPIbT6aSurs5r6WltDe2TWdqje/y3RUREBpou3amprKzE5XKRlJTktT4pKYk9e/Z0us+tt95KZWUlV1xxBYZh0NbWxj333OP1+OlkbrebH//4x0yZMoWLL77Y6zhDhw4lJSWFnTt38vOf/5zCwkLWr1/f6XHy8vJ46KGHunJ63e54m5q2YM37JCIi4mtdfvzUVe+++y6PPvooTz/9NFlZWezbt497772XRx55hGXLlnXYfv78+ezatYvNmzd7rb/77rs978eOHcugQYOYNm0a+/fvJyMjo8NxlixZQm5urudzXV0daWlp3XhmZ2Ztn/fJUKgRERHxuS6Fmvj4eGw2G2VlZV7ry8rKTtnWZdmyZdx+++3cddddgBlIHA4Hd999N/fffz9W64knYAsWLOC1117jn//8J4MHDz5tLVlZWQDs27ev01ATFBREUJB/exwFNB8DwBqm0YRFRER8rUttaux2O5mZmeTn53vWud1u8vPzyc7O7nSfxsZGr+ACYLOZjWYNw/C8LliwgJdeeol33nmHYcOGnbGWHTt2ADBo0KCunEKPCm6tASAwQqFGRETE17r8+Ck3N5c5c+YwadIkJk+ezMqVK3E4HJ7eUHfccQepqank5eUBMHPmTFasWMHEiRM9j5+WLVvGzJkzPeFm/vz5rF27lldeeYWIiAhKS0sBiIqKIiQkhP3797N27VpmzJhBXFwcO3fuZNGiRUydOpVx48Z117XoXq3N2N1NAARFJp1hYxERETlfXQ41s2fPpqKiggceeIDS0lImTJjAhg0bPI2Hi4qKvO7MLF26FIvFwtKlSzl69CgJCQnMnDmTX/ziF55tfvOb3wDmAHsne/bZZ/n+97+P3W5n48aNngCVlpbGrFmzWLp06bmcc89on8yyzbASHhXj52JERET6P4tx/BlQP1dXV0dUVBS1tbVERkb6/gdLP4XVV1BhRLFj9od8/SLdrREREemqrvz91txPvuKZzDKc2DC7n4sRERHp/xRqfOV4qCFCoUZERKQHKNT4SGt9+wzdhkKNiIhIT1Co8ZHmOnOCzxoiiQz2+RiHIiIiA55CjY+0tM/Q3RQYhcVi8XM1IiIi/Z9CjY+4GszHTy12decWERHpCQo1vtLeUNgdrFAjIiLSExRqfMTWPpkloZoiQUREpCco1PhIoLMGAGtYvH8LERERGSAUanwkuK0GgMBIhRoREZGeoFDjCy2N2N3NAIREJfq5GBERkYFBocYX2iezbDFshEeqobCIiEhPUKjxhfaeTzVEEBce5OdiREREBgaFGl9oNO/UVBsRxIRqigQREZGeoFDjA27H8Rm6I4gLV6gRERHpCQo1PnB83qdqwokODfRzNSIiIgODQo0PNNea8z41WKMICrD5uRoREZGBQaHGB9rqzVDjtEf7txAREZEBRKHGB1ztbWpaNZmliIhIj1Go8QFLe+8nd0isnysREREZOBRqfMDmPGa+0WSWIiIiPUahxgeC2kNNQLjmfRIREekpCjXdzTAIaZ/M0h6Z4N9aREREBhCFmu7W2kig0QJASLRCjYiISE9RqOlu7Y2EnUYAkRHR/q1FRERkAFGo6W7tk1keI4LYCE1mKSIi0lMUarpb44l5n2I1maWIiEiPUajpZi31lYA5Q3esJrMUERHpMQo13aypxpzMstYSTkRQgJ+rERERGTgUarqZs86c96kxIBqLxeLnakRERAYOhZpu1tZgPn5yBkb7txAREZEBRqGmm7nbJ7NsC9a8TyIiIj1JoaabWZs0maWIiIg/KNR0swCnGWqsYZrMUkREpCcp1HSzoJYaQJNZioiI9DSFmu5kGIS21QIQHKV5n0RERHqSQk13anEQSCsAodGJfi5GRERkYDmnULNq1SrS09MJDg4mKyuLbdu2nXb7lStXMmrUKEJCQkhLS2PRokU0Nzd36ZjNzc3Mnz+fuLg4wsPDmTVrFmVlZedSvu+0T5HQbAQSFRHl52JEREQGli6HmnXr1pGbm8vy5cvZvn0748ePZ/r06ZSXl3e6/dq1a1m8eDHLly9n9+7drFmzhnXr1nHfffd16ZiLFi3i1Vdf5cUXX2TTpk0UFxdz0003ncMp+1B7qKnWZJYiIiI9zmIYhtGVHbKysrj00kt56qmnAHC73aSlpbFw4UIWL17cYfsFCxawe/du8vPzPet+8pOfsHXrVjZv3nxWx6ytrSUhIYG1a9fy7W9/G4A9e/Zw4YUXsmXLFi677LIz1l1XV0dUVBS1tbVERkZ25ZTPmvuLt7Gu/Tafu4cS/9NtJEYG++R3REREBoqu/P3u0p2alpYWCgoKyMnJOXEAq5WcnBy2bNnS6T6XX345BQUFnsdJBw4c4I033mDGjBlnfcyCggJaW1u9thk9ejRDhgw55e/6w/F5n6qNcGLCNJmliIhIT+rSjIuVlZW4XC6SkpK81iclJbFnz55O97n11luprKzkiiuuwDAM2trauOeeezyPn87mmKWlpdjtdqKjoztsU1pa2unvOp1OnE6n53NdXV1XTvWcNNZWEAbU26IItKkNtoiISE/y+V/ed999l0cffZSnn36a7du3s379el5//XUeeeQRn/5uXl4eUVFRniUtLc2nvwfQWm9OZtkcEO3z3xIRERFvXQo18fHx2Gy2Dr2OysrKSE5O7nSfZcuWcfvtt3PXXXcxduxYbrzxRh599FHy8vJwu91ndczk5GRaWlqoqak5699dsmQJtbW1nuXw4cNdOdVz4pnM0h7t898SERERb10KNXa7nczMTK9Gv263m/z8fLKzszvdp7GxEavV+2dsNhsAhmGc1TEzMzMJDAz02qawsJCioqJT/m5QUBCRkZFei8+1935yad4nERGRHtelNjUAubm5zJkzh0mTJjF58mRWrlyJw+Fg7ty5ANxxxx2kpqaSl5cHwMyZM1mxYgUTJ04kKyuLffv2sWzZMmbOnOkJN2c6ZlRUFHfeeSe5ubnExsYSGRnJwoULyc7OPqueTz3l+GSWKNSIiIj0uC6HmtmzZ1NRUcEDDzxAaWkpEyZMYMOGDZ6GvkVFRV53ZpYuXYrFYmHp0qUcPXqUhIQEZs6cyS9+8YuzPibAE088gdVqZdasWTidTqZPn87TTz99Pufe7QKdxwCwhmsySxERkZ7W5XFq+qqeGKem9r+GE9VWxV8nreXb/3adT35DRERkIPHZODVyGoZBmCazFBER8RuFmu7irCeANgDCNJmliIhIj1Oo6S7tPZ+aDDvRUZrMUkREpKcp1HSXRrPnUzURxGqKBBERkR6nUNNNnO2jCR8zFGpERET8QaGmmzjaJ7OsIYLwoC73lBcREZHzpFDTTZprzDs1joAoLBaLn6sREREZeBRquklrQ/tkloExfq5ERERkYFKo6Sbu9sks24Ki/VuIiIjIAKVQ013aez+5gjXvk4iIiD8o1HQTW/u8T4Qq1IiIiPiDQk03sbeHmoBwTZEgIiLiDwo13SSkrQYAe0S8fwsREREZoBRquoNhEO5qn8wyOsnPxYiIiAxMCjXdobkWG24AwmL0+ElERMQfFGq6Q5PZ88lhBBEbGennYkRERAYmhZpu4GowZ+g+RgQxYYF+rkZERGRgUqjpBo5jZQBUGxHEhGoySxEREX9QqOkGTbXmZJb11kgCbbqkIiIi/qC/wN2gudac96kpIMrPlYiIiAxcCjXdwOWZzDLav4WIiIgMYAo13cDVPu9TW5CmSBAREfEXhZpuYG0PNe4QhRoRERF/UajpBgFOM9RYwjRFgoiIiL8o1HSDoJYaAAIi4vxbiIiIyACmUNMNQtsnswyO1BQJIiIi/qJQc77cbsLc9QCERif6uRgREZGBS6HmfDlPTGYZHqNQIyIi4i8KNeervedTvRFCbGSEn4sREREZuBRqzlNz+xQJx4xwYsI075OIiIi/KNScp/pqczLLWksEYXabn6sREREZuAL8XUBfVx49nh+13E9UaDCrLRZ/lyMiIjJgKdScp3JXGFvcY7goItLfpYiIiAxoCjXnaUhsKPdOG0lMaKC/SxERERnQFGrOU0ZCOIu+foG/yxARERnw1FBYRERE+gWFGhEREekXFGpERESkXzinULNq1SrS09MJDg4mKyuLbdu2nXLbq6++GovF0mG57rrrPNt09r3FYuHxxx/3bJOent7h+8cee+xcyhcREZF+qMsNhdetW0dubi6rV68mKyuLlStXMn36dAoLC0lM7Dj30fr162lpafF8rqqqYvz48XznO9/xrCspKfHa58033+TOO+9k1qxZXusffvhh5s2b5/kcEaFpCURERMTU5VCzYsUK5s2bx9y5cwFYvXo1r7/+Os888wyLFy/usH1sbKzX5xdeeIHQ0FCvUJOcnOy1zSuvvMLXvvY1hg8f7rU+IiKiw7YiIiIi0MXHTy0tLRQUFJCTk3PiAFYrOTk5bNmy5ayOsWbNGr773e8SFhbW6fdlZWW8/vrr3HnnnR2+e+yxx4iLi2PixIk8/vjjtLW1nfJ3nE4ndXV1XouIiIj0X126U1NZWYnL5SIpKclrfVJSEnv27Dnj/tu2bWPXrl2sWbPmlNv87//+LxEREdx0001e63/0ox9xySWXEBsby/vvv8+SJUsoKSlhxYoVnR4nLy+Phx566CzOSkRERPqDHh18b82aNYwdO5bJkyefcptnnnmG2267jeDgYK/1ubm5nvfjxo3Dbrfzwx/+kLy8PIKCgjocZ8mSJV771NXVkZaW1g1nISIiIr1Rlx4/xcfHY7PZKCsr81pfVlZ2xrYuDoeDF154odPHSsf961//orCwkLvuuuuMtWRlZdHW1saXX37Z6fdBQUFERkZ6LSIiItJ/dSnU2O12MjMzyc/P96xzu93k5+eTnZ192n1ffPFFnE4n3/ve9065zZo1a8jMzGT8+PFnrGXHjh1YrdZOe1yJiIjIwNPlx0+5ubnMmTOHSZMmMXnyZFauXInD4fD0hrrjjjtITU0lLy/Pa781a9Zwww03EBcX1+lx6+rqePHFF/mf//mfDt9t2bKFrVu38rWvfY2IiAi2bNnCokWL+N73vkdMTExXT0FERET6oS6HmtmzZ1NRUcEDDzxAaWkpEyZMYMOGDZ7Gw0VFRVit3jeACgsL2bx5M3//+99PedwXXngBwzC45ZZbOnwXFBTECy+8wIMPPojT6WTYsGEsWrTIq82MiIiIDGwWwzAMfxfRE2pra4mOjubw4cNqXyMiItJHHO/oU1NTQ1RU1Gm37dHeT/5UX18PoB5QIiIifVB9ff0ZQ82AuVPjdrspLi4mIiICi8XSrcc+niJ1F6hn6Hr3LF3vnqXr3bN0vXvWuVxvwzCor68nJSWlQ/OWrxowd2qsViuDBw/26W+o63jP0vXuWbrePUvXu2fpevesrl7vM92hOe6cZukWERER6W0UakRERKRfUKjpBkFBQSxfvrzT6Rqk++l69yxd756l692zdL17lq+v94BpKCwiIiL9m+7UiIiISL+gUCMiIiL9gkKNiIiI9AsKNSIiItIvKNScp1WrVpGenk5wcDBZWVls27bN3yX1G//85z+ZOXMmKSkpWCwWXn75Za/vDcPggQceYNCgQYSEhJCTk8PevXv9U2wfl5eXx6WXXkpERASJiYnccMMNFBYWem3T3NzM/PnziYuLIzw8nFmzZlFWVuanivu23/zmN4wbN84zAFl2djZvvvmm53tda9967LHHsFgs/PjHP/as0zXvPg8++CAWi8VrGT16tOd7X15rhZrzsG7dOnJzc1m+fDnbt29n/PjxTJ8+nfLycn+X1i84HA7Gjx/PqlWrOv3+v//7v/nVr37F6tWr2bp1K2FhYUyfPp3m5uYerrTv27RpE/Pnz+eDDz7g7bffprW1lW984xs4HA7PNosWLeLVV1/lxRdfZNOmTRQXF3PTTTf5seq+a/DgwTz22GMUFBTw0Ucfcc0113D99dfz2WefAbrWvvThhx/y29/+lnHjxnmt1zXvXmPGjKGkpMSzbN682fOdT6+1Ieds8uTJxvz58z2fXS6XkZKSYuTl5fmxqv4JMF566SXPZ7fbbSQnJxuPP/64Z11NTY0RFBRk/PnPf/ZDhf1LeXm5ARibNm0yDMO8toGBgcaLL77o2Wb37t0GYGzZssVfZfYrMTExxu9//3tdax+qr683Ro4cabz99tvGVVddZdx7772GYeif7+62fPlyY/z48Z1+5+trrTs156ilpYWCggJycnI866xWKzk5OWzZssWPlQ0MBw8epLS01Ov6R0VFkZWVpevfDWprawGIjY0FoKCggNbWVq/rPXr0aIYMGaLrfZ5cLhcvvPACDoeD7OxsXWsfmj9/Ptddd53XtQX98+0Le/fuJSUlheHDh3PbbbdRVFQE+P5aD5gJLbtbZWUlLpeLpKQkr/VJSUns2bPHT1UNHKWlpQCdXv/j38m5cbvd/PjHP2bKlClcfPHFgHm97XY70dHRXtvqep+7Tz/9lOzsbJqbmwkPD+ell17ioosuYseOHbrWPvDCCy+wfft2Pvzwww7f6Z/v7pWVlcVzzz3HqFGjKCkp4aGHHuLKK69k165dPr/WCjUi4mX+/Pns2rXL6xm4dL9Ro0axY8cOamtr+etf/8qcOXPYtGmTv8vqlw4fPsy9997L22+/TXBwsL/L6feuvfZaz/tx48aRlZXF0KFD+ctf/kJISIhPf1uPn85RfHw8NputQ4vtsrIykpOT/VTVwHH8Guv6d68FCxbw2muv8Y9//IPBgwd71icnJ9PS0kJNTY3X9rre585utzNixAgyMzPJy8tj/PjxPPnkk7rWPlBQUEB5eTmXXHIJAQEBBAQEsGnTJn71q18REBBAUlKSrrkPRUdHc8EFF7Bv3z6f//OtUHOO7HY7mZmZ5Ofne9a53W7y8/PJzs72Y2UDw7Bhw0hOTva6/nV1dWzdulXX/xwYhsGCBQt46aWXeOeddxg2bJjX95mZmQQGBnpd78LCQoqKinS9u4nb7cbpdOpa+8C0adP49NNP2bFjh2eZNGkSt912m+e9rrnvNDQ0sH//fgYNGuT7f77Pu6nxAPbCCy8YQUFBxnPPPWd8/vnnxt13321ER0cbpaWl/i6tX6ivrzc+/vhj4+OPPzYAY8WKFcbHH39sHDp0yDAMw3jssceM6Oho45VXXjF27txpXH/99cawYcOMpqYmP1fe9/z7v/+7ERUVZbz77rtGSUmJZ2lsbPRsc8899xhDhgwx3nnnHeOjjz4ysrOzjezsbD9W3XctXrzY2LRpk3Hw4EFj586dxuLFiw2LxWL8/e9/NwxD17onnNz7yTB0zbvTT37yE+Pdd981Dh48aLz33ntGTk6OER8fb5SXlxuG4dtrrVBznn79618bQ4YMMex2uzF58mTjgw8+8HdJ/cY//vEPA+iwzJkzxzAMs1v3smXLjKSkJCMoKMiYNm2aUVhY6N+i+6jOrjNgPPvss55tmpqajP/4j/8wYmJijNDQUOPGG280SkpK/Fd0H/aDH/zAGDp0qGG3242EhARj2rRpnkBjGLrWPeGroUbXvPvMnj3bGDRokGG3243U1FRj9uzZxr59+zzf+/JaWwzDMM7/fo+IiIiIf6lNjYiIiPQLCjUiIiLSLyjUiIiISL+gUCMiIiL9gkKNiIiI9AsKNSIiItIvKNSIiIhIv6BQIyIiIv2CQo2IiIj0Cwo1IiIi0i8o1IiIiEi/oFAjIiIi/cL/D7WzaQJJ1NSfAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['train_acc'])\n",
        "plt.plot(history['valid_acc'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPA_LeMzqyFc"
      },
      "source": [
        "## 4. Integrate our classifier into the scikit-learn pipeline and the randomized search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP7V3OjInmQM"
      },
      "source": [
        "This time, you can use the same strategy as the second lab, trying to integrate everything from the raw dataset to the performance metrics into scikit-learn pipeline.\n",
        "\n",
        "- Task 1: Create a `Normalizer` class that extends BaseEstimator and TransformerMixin.\n",
        " - Your normalizer should do the following job:\n",
        "   - Normalizer: To make the features have the range [0, 1] and also **center the points to zero by subtracting 0.5 from the values.**\n",
        "   - Use NumPy's broadcasting to calculate (X / 255) - 0.5.\n",
        "\n",
        "\n",
        "- Task 2: Create a pipeline that integrates both normalizer and your neural network classifier.\n",
        "  - Your pipeline should contain the following modules.\n",
        "    - 'normalizer': Normalizer class\n",
        "    - 'classifier': `FullyConnectedNetwork` with default parameters but with epochs=10.\n",
        "\n",
        "- Task 3: Fit your pipeline on the datasets (`X_train`, `y_train`).\n",
        "  - You should **not** use `X_normalized` this time as the normalizer is now part of your pipeline. This means you might need to split your dataset again with `train_test_split` by using `X` and `y_integer`. Turn on stratification, and set `test_size` = 20%.\n",
        "  - Fit your pipeline and report the test score on `X_test` and `y_test` to `pipeline_score`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7KQyD1cf1Na8",
        "outputId": "6525985e-5137-47af-f623-94068f85fe7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/50 | Cost: 16930.83 | Train/Valid Acc.: 76.24%/76.32% \n",
            "2/50 | Cost: 9335.34 | Train/Valid Acc.: 85.82%/85.66% \n",
            "3/50 | Cost: 7200.30 | Train/Valid Acc.: 88.32%/88.07% \n",
            "4/50 | Cost: 6488.05 | Train/Valid Acc.: 89.48%/89.11% \n",
            "5/50 | Cost: 5861.72 | Train/Valid Acc.: 90.07%/89.65% \n",
            "6/50 | Cost: 6133.12 | Train/Valid Acc.: 90.63%/90.24% \n",
            "7/50 | Cost: 5089.21 | Train/Valid Acc.: 91.08%/90.66% \n",
            "8/50 | Cost: 4993.01 | Train/Valid Acc.: 91.49%/91.04% \n",
            "9/50 | Cost: 5597.80 | Train/Valid Acc.: 91.74%/91.26% \n",
            "10/50 | Cost: 4539.10 | Train/Valid Acc.: 91.81%/91.47% \n",
            "11/50 | Cost: 4830.63 | Train/Valid Acc.: 92.17%/91.73% \n",
            "12/50 | Cost: 4722.51 | Train/Valid Acc.: 92.36%/91.83% \n",
            "13/50 | Cost: 4837.71 | Train/Valid Acc.: 92.59%/92.01% \n",
            "14/50 | Cost: 4771.24 | Train/Valid Acc.: 92.71%/92.29% \n",
            "15/50 | Cost: 4548.39 | Train/Valid Acc.: 92.81%/92.24% \n",
            "16/50 | Cost: 3988.60 | Train/Valid Acc.: 92.95%/92.49% \n",
            "17/50 | Cost: 4309.41 | Train/Valid Acc.: 93.23%/92.58% \n",
            "18/50 | Cost: 3776.75 | Train/Valid Acc.: 93.26%/92.68% \n",
            "19/50 | Cost: 4088.12 | Train/Valid Acc.: 93.46%/92.98% \n",
            "20/50 | Cost: 3611.44 | Train/Valid Acc.: 93.53%/93.10% \n",
            "21/50 | Cost: 4508.50 | Train/Valid Acc.: 93.66%/93.21% \n",
            "22/50 | Cost: 3771.01 | Train/Valid Acc.: 93.76%/93.23% \n",
            "23/50 | Cost: 3250.30 | Train/Valid Acc.: 93.77%/93.17% \n",
            "24/50 | Cost: 3582.06 | Train/Valid Acc.: 93.88%/93.36% \n",
            "25/50 | Cost: 4116.89 | Train/Valid Acc.: 93.95%/93.42% \n",
            "26/50 | Cost: 3282.46 | Train/Valid Acc.: 94.17%/93.49% \n",
            "27/50 | Cost: 2839.57 | Train/Valid Acc.: 94.23%/93.53% \n",
            "28/50 | Cost: 3653.32 | Train/Valid Acc.: 94.35%/93.68% \n",
            "29/50 | Cost: 3761.03 | Train/Valid Acc.: 94.37%/93.71% \n",
            "30/50 | Cost: 3603.07 | Train/Valid Acc.: 94.48%/93.84% \n",
            "31/50 | Cost: 3057.17 | Train/Valid Acc.: 94.53%/93.70% \n",
            "32/50 | Cost: 3118.16 | Train/Valid Acc.: 94.49%/93.66% \n",
            "33/50 | Cost: 3186.29 | Train/Valid Acc.: 94.63%/93.85% \n",
            "34/50 | Cost: 3253.43 | Train/Valid Acc.: 94.75%/93.92% \n",
            "35/50 | Cost: 2881.96 | Train/Valid Acc.: 94.82%/94.09% \n",
            "36/50 | Cost: 2750.12 | Train/Valid Acc.: 94.84%/93.98% \n",
            "37/50 | Cost: 3526.11 | Train/Valid Acc.: 94.75%/94.03% \n",
            "38/50 | Cost: 3071.27 | Train/Valid Acc.: 94.92%/94.08% \n",
            "39/50 | Cost: 3107.19 | Train/Valid Acc.: 95.02%/94.02% \n",
            "40/50 | Cost: 3276.56 | Train/Valid Acc.: 95.03%/94.12% \n",
            "41/50 | Cost: 3074.40 | Train/Valid Acc.: 95.12%/94.18% \n",
            "42/50 | Cost: 2820.14 | Train/Valid Acc.: 95.07%/94.15% \n",
            "43/50 | Cost: 2906.23 | Train/Valid Acc.: 95.20%/94.32% \n",
            "44/50 | Cost: 2712.43 | Train/Valid Acc.: 95.21%/94.23% \n",
            "45/50 | Cost: 2746.81 | Train/Valid Acc.: 95.20%/94.22% \n",
            "46/50 | Cost: 2676.56 | Train/Valid Acc.: 95.33%/94.30% \n",
            "47/50 | Cost: 2609.10 | Train/Valid Acc.: 95.45%/94.33% \n",
            "48/50 | Cost: 3035.48 | Train/Valid Acc.: 95.37%/94.39% \n",
            "49/50 | Cost: 2528.90 | Train/Valid Acc.: 95.44%/94.41% \n",
            "50/50 | Cost: 2546.88 | Train/Valid Acc.: 95.45%/94.37% \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;normalizer&#x27;, Normalizer()),\n",
              "                (&#x27;classifier&#x27;, FullyConnectedNetwork(epochs=50))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;normalizer&#x27;, Normalizer()),\n",
              "                (&#x27;classifier&#x27;, FullyConnectedNetwork(epochs=50))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Normalizer</label><div class=\"sk-toggleable__content\"><pre>Normalizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FullyConnectedNetwork</label><div class=\"sk-toggleable__content\"><pre>FullyConnectedNetwork(epochs=50)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('normalizer', Normalizer()),\n",
              "                ('classifier', FullyConnectedNetwork(epochs=50))])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create: class Normalizer(BaseEstimator, TransformerMixin)\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pipe = None # CHANGE IT\n",
        "\n",
        "class Normalizer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X_norm = (X / 255.0) - 0.5\n",
        "        return X_norm\n",
        "from sklearn.metrics import accuracy_score\n",
        "normalizer = Normalizer()\n",
        "nn_clf = FullyConnectedNetwork(epochs=50)\n",
        "pipe = Pipeline([\n",
        "    ('normalizer', normalizer),\n",
        "    ('classifier', nn_clf)\n",
        "])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_integer, test_size=0.2, stratify=y)\n",
        "pipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQRQASa5bz89"
      },
      "outputs": [],
      "source": [
        "#pipeline_score = None # CHANGE IT\n",
        "pipeline_score = pipe.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcykfo9x6zY8",
        "outputId": "7f1441bb-d64a-453c-9580-315b9bea9858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.944\n"
          ]
        }
      ],
      "source": [
        "# PRINT YOUR SCORE HERE\n",
        "print(pipeline_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMdqSR8k4hRI"
      },
      "source": [
        "- Task 4: Randomized search\n",
        "  - After constructing your pipeline, you can perform a randomized search on it.\n",
        "  - Define your parameter grid with the following information. Use `np.arange` if neccesary:\n",
        "    - l1 of classifier: 0.001 to 0.01 (included) with step size 0.001.\n",
        "    - l2 of classifier: 0.001 to 0.005 (included) with step size 0.001.\n",
        "    - size of hidden layer of classifier: 30 to 100 with step size 10\n",
        "    - learning rate of classifier: 0.0001 to 0.001 with step size 0.002\n",
        "    - initialization techniques of classifier: [normal, xavier, he]\n",
        "  - Run your randomized search with cv=3. Fit it on your previous `X_train` and `y_train`.\n",
        "    - Make 10 different attempts.\n",
        "    - Set `random_state` = `RANDOM_STATE`\n",
        "  - Report your best classifier and best score into the variables `best_classifier` and `best_score`.\n",
        "\n",
        "  - **Note that this task will take a few hours based on computing power, so you may not need to finish the run. The submission is regarded correct if the logic is correct - if you cannot finish the task before the submission.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rysn4lmhmpBb",
        "outputId": "430e7f5e-0273-4d86-8dc5-30ce4dca103c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/50 | Cost: 32833.07 | Train/Valid Acc.: 14.24%/14.16% \n",
            "2/50 | Cost: 32328.62 | Train/Valid Acc.: 23.79%/23.22% \n",
            "3/50 | Cost: 31869.19 | Train/Valid Acc.: 24.27%/23.52% \n",
            "4/50 | Cost: 31472.54 | Train/Valid Acc.: 38.57%/37.95% \n",
            "5/50 | Cost: 31145.17 | Train/Valid Acc.: 41.27%/40.55% \n",
            "6/50 | Cost: 30789.25 | Train/Valid Acc.: 38.15%/37.72% \n",
            "7/50 | Cost: 30559.12 | Train/Valid Acc.: 39.51%/38.68% \n",
            "8/50 | Cost: 30309.14 | Train/Valid Acc.: 46.09%/45.62% \n",
            "9/50 | Cost: 30118.30 | Train/Valid Acc.: 40.95%/40.29% \n",
            "10/50 | Cost: 29892.38 | Train/Valid Acc.: 47.83%/47.66% \n",
            "11/50 | Cost: 29602.38 | Train/Valid Acc.: 44.38%/44.38% \n",
            "12/50 | Cost: 29258.20 | Train/Valid Acc.: 42.32%/41.79% \n",
            "13/50 | Cost: 28853.80 | Train/Valid Acc.: 42.39%/41.83% \n",
            "14/50 | Cost: 28300.27 | Train/Valid Acc.: 46.05%/45.72% \n",
            "15/50 | Cost: 27719.05 | Train/Valid Acc.: 45.11%/44.95% \n",
            "16/50 | Cost: 27076.52 | Train/Valid Acc.: 45.84%/45.51% \n",
            "17/50 | Cost: 26426.05 | Train/Valid Acc.: 45.96%/45.62% \n",
            "18/50 | Cost: 25761.04 | Train/Valid Acc.: 50.12%/49.62% \n",
            "19/50 | Cost: 24960.82 | Train/Valid Acc.: 50.15%/49.89% \n",
            "20/50 | Cost: 24195.90 | Train/Valid Acc.: 50.65%/50.30% \n",
            "21/50 | Cost: 23530.88 | Train/Valid Acc.: 53.41%/53.01% \n",
            "22/50 | Cost: 22754.26 | Train/Valid Acc.: 55.22%/54.80% \n",
            "23/50 | Cost: 22049.56 | Train/Valid Acc.: 55.16%/54.64% \n",
            "24/50 | Cost: 21292.00 | Train/Valid Acc.: 57.00%/56.74% \n",
            "25/50 | Cost: 20588.84 | Train/Valid Acc.: 57.00%/56.79% \n",
            "26/50 | Cost: 19955.15 | Train/Valid Acc.: 59.02%/58.71% \n",
            "27/50 | Cost: 19394.83 | Train/Valid Acc.: 60.17%/60.15% \n",
            "28/50 | Cost: 18806.14 | Train/Valid Acc.: 59.77%/59.85% \n",
            "29/50 | Cost: 18181.54 | Train/Valid Acc.: 61.07%/61.12% \n",
            "30/50 | Cost: 17788.00 | Train/Valid Acc.: 62.36%/62.29% \n",
            "31/50 | Cost: 17272.86 | Train/Valid Acc.: 62.61%/62.62% \n",
            "32/50 | Cost: 16758.28 | Train/Valid Acc.: 63.14%/63.21% \n",
            "33/50 | Cost: 16431.02 | Train/Valid Acc.: 63.92%/63.96% \n",
            "34/50 | Cost: 16009.49 | Train/Valid Acc.: 64.43%/64.68% \n",
            "35/50 | Cost: 15650.62 | Train/Valid Acc.: 64.87%/65.00% \n",
            "36/50 | Cost: 15295.89 | Train/Valid Acc.: 65.76%/65.64% \n",
            "37/50 | Cost: 14840.55 | Train/Valid Acc.: 66.72%/66.49% \n",
            "38/50 | Cost: 14686.48 | Train/Valid Acc.: 66.89%/66.69% \n",
            "39/50 | Cost: 14276.56 | Train/Valid Acc.: 67.79%/67.40% \n",
            "40/50 | Cost: 14068.31 | Train/Valid Acc.: 68.31%/67.87% \n",
            "41/50 | Cost: 13658.44 | Train/Valid Acc.: 69.07%/68.71% \n",
            "42/50 | Cost: 13487.33 | Train/Valid Acc.: 69.59%/69.28% \n",
            "43/50 | Cost: 13224.72 | Train/Valid Acc.: 70.11%/69.77% \n",
            "44/50 | Cost: 12954.78 | Train/Valid Acc.: 70.10%/69.80% \n",
            "45/50 | Cost: 12737.63 | Train/Valid Acc.: 70.98%/70.65% \n",
            "46/50 | Cost: 12572.69 | Train/Valid Acc.: 71.43%/70.83% \n",
            "47/50 | Cost: 12385.18 | Train/Valid Acc.: 71.80%/71.21% \n",
            "48/50 | Cost: 12217.11 | Train/Valid Acc.: 72.04%/71.53% \n",
            "49/50 | Cost: 11869.60 | Train/Valid Acc.: 72.63%/72.39% \n",
            "50/50 | Cost: 11716.58 | Train/Valid Acc.: 73.04%/72.96% \n",
            "1/50 | Cost: 32842.46 | Train/Valid Acc.: 12.33%/12.38% \n",
            "2/50 | Cost: 32271.94 | Train/Valid Acc.: 18.69%/18.88% \n",
            "3/50 | Cost: 31809.34 | Train/Valid Acc.: 23.27%/23.14% \n",
            "4/50 | Cost: 31421.77 | Train/Valid Acc.: 31.78%/31.78% \n",
            "5/50 | Cost: 31123.65 | Train/Valid Acc.: 32.60%/32.44% \n",
            "6/50 | Cost: 30870.37 | Train/Valid Acc.: 42.28%/42.48% \n",
            "7/50 | Cost: 30579.29 | Train/Valid Acc.: 44.32%/44.21% \n",
            "8/50 | Cost: 30344.63 | Train/Valid Acc.: 44.32%/44.87% \n",
            "9/50 | Cost: 30106.36 | Train/Valid Acc.: 41.55%/41.41% \n",
            "10/50 | Cost: 29802.28 | Train/Valid Acc.: 46.67%/46.83% \n",
            "11/50 | Cost: 29454.19 | Train/Valid Acc.: 44.96%/44.91% \n",
            "12/50 | Cost: 29018.76 | Train/Valid Acc.: 43.59%/43.44% \n",
            "13/50 | Cost: 28554.28 | Train/Valid Acc.: 43.91%/44.26% \n",
            "14/50 | Cost: 28019.40 | Train/Valid Acc.: 41.82%/42.07% \n",
            "15/50 | Cost: 27385.66 | Train/Valid Acc.: 43.55%/43.88% \n",
            "16/50 | Cost: 26752.35 | Train/Valid Acc.: 41.62%/42.04% \n",
            "17/50 | Cost: 26059.51 | Train/Valid Acc.: 45.34%/45.78% \n",
            "18/50 | Cost: 25268.93 | Train/Valid Acc.: 47.30%/47.74% \n",
            "19/50 | Cost: 24584.00 | Train/Valid Acc.: 47.99%/48.36% \n",
            "20/50 | Cost: 23869.12 | Train/Valid Acc.: 49.78%/50.23% \n",
            "21/50 | Cost: 23179.96 | Train/Valid Acc.: 50.87%/50.87% \n",
            "22/50 | Cost: 22446.95 | Train/Valid Acc.: 52.51%/52.73% \n",
            "23/50 | Cost: 21805.73 | Train/Valid Acc.: 52.78%/53.30% \n",
            "24/50 | Cost: 21119.09 | Train/Valid Acc.: 54.64%/54.87% \n",
            "25/50 | Cost: 20437.79 | Train/Valid Acc.: 57.31%/57.82% \n",
            "26/50 | Cost: 19859.83 | Train/Valid Acc.: 57.75%/58.29% \n",
            "27/50 | Cost: 19275.64 | Train/Valid Acc.: 59.12%/59.46% \n",
            "28/50 | Cost: 18725.39 | Train/Valid Acc.: 59.86%/60.61% \n",
            "29/50 | Cost: 18157.10 | Train/Valid Acc.: 60.49%/60.93% \n",
            "30/50 | Cost: 17587.46 | Train/Valid Acc.: 62.36%/62.81% \n",
            "31/50 | Cost: 17140.51 | Train/Valid Acc.: 63.49%/63.76% \n",
            "32/50 | Cost: 16698.10 | Train/Valid Acc.: 62.85%/63.21% \n",
            "33/50 | Cost: 16300.50 | Train/Valid Acc.: 64.18%/64.37% \n",
            "34/50 | Cost: 15981.29 | Train/Valid Acc.: 64.33%/64.62% \n",
            "35/50 | Cost: 15506.80 | Train/Valid Acc.: 65.23%/65.64% \n",
            "36/50 | Cost: 15099.36 | Train/Valid Acc.: 66.53%/66.68% \n",
            "37/50 | Cost: 14839.87 | Train/Valid Acc.: 66.64%/67.03% \n",
            "38/50 | Cost: 14632.91 | Train/Valid Acc.: 67.45%/67.77% \n",
            "39/50 | Cost: 14217.63 | Train/Valid Acc.: 68.07%/68.34% \n",
            "40/50 | Cost: 13966.56 | Train/Valid Acc.: 68.88%/69.27% \n",
            "41/50 | Cost: 13789.36 | Train/Valid Acc.: 69.60%/69.66% \n",
            "42/50 | Cost: 13644.29 | Train/Valid Acc.: 70.15%/70.29% \n",
            "43/50 | Cost: 13427.69 | Train/Valid Acc.: 70.41%/70.68% \n",
            "44/50 | Cost: 13086.98 | Train/Valid Acc.: 70.98%/71.37% \n",
            "45/50 | Cost: 12940.34 | Train/Valid Acc.: 71.47%/71.76% \n",
            "46/50 | Cost: 12699.43 | Train/Valid Acc.: 71.95%/72.11% \n",
            "47/50 | Cost: 12647.42 | Train/Valid Acc.: 72.20%/72.38% \n",
            "48/50 | Cost: 12394.77 | Train/Valid Acc.: 72.73%/72.88% \n",
            "49/50 | Cost: 12250.43 | Train/Valid Acc.: 73.11%/73.30% \n",
            "50/50 | Cost: 11941.11 | Train/Valid Acc.: 74.03%/74.05% \n",
            "1/50 | Cost: 32835.72 | Train/Valid Acc.: 26.73%/27.49% \n",
            "2/50 | Cost: 32284.71 | Train/Valid Acc.: 17.85%/18.20% \n",
            "3/50 | Cost: 31699.28 | Train/Valid Acc.: 17.10%/17.44% \n",
            "4/50 | Cost: 31335.59 | Train/Valid Acc.: 23.45%/23.96% \n",
            "5/50 | Cost: 31030.78 | Train/Valid Acc.: 41.47%/41.72% \n",
            "6/50 | Cost: 30650.98 | Train/Valid Acc.: 48.40%/48.59% \n",
            "7/50 | Cost: 30360.85 | Train/Valid Acc.: 43.99%/44.04% \n",
            "8/50 | Cost: 30133.76 | Train/Valid Acc.: 47.11%/47.64% \n",
            "9/50 | Cost: 29882.51 | Train/Valid Acc.: 41.76%/41.89% \n",
            "10/50 | Cost: 29649.87 | Train/Valid Acc.: 47.42%/47.70% \n",
            "11/50 | Cost: 29285.90 | Train/Valid Acc.: 47.46%/47.92% \n",
            "12/50 | Cost: 28857.90 | Train/Valid Acc.: 47.68%/47.83% \n",
            "13/50 | Cost: 28314.97 | Train/Valid Acc.: 49.06%/49.03% \n",
            "14/50 | Cost: 27656.40 | Train/Valid Acc.: 49.39%/49.68% \n",
            "15/50 | Cost: 26877.81 | Train/Valid Acc.: 47.46%/47.67% \n",
            "16/50 | Cost: 26052.39 | Train/Valid Acc.: 50.12%/50.20% \n",
            "17/50 | Cost: 25244.77 | Train/Valid Acc.: 51.81%/52.21% \n",
            "18/50 | Cost: 24422.39 | Train/Valid Acc.: 51.43%/51.71% \n",
            "19/50 | Cost: 23556.67 | Train/Valid Acc.: 53.03%/53.30% \n",
            "20/50 | Cost: 22720.48 | Train/Valid Acc.: 53.76%/54.12% \n",
            "21/50 | Cost: 21910.29 | Train/Valid Acc.: 54.60%/55.01% \n",
            "22/50 | Cost: 21110.01 | Train/Valid Acc.: 55.73%/56.16% \n",
            "23/50 | Cost: 20380.94 | Train/Valid Acc.: 57.87%/57.92% \n",
            "24/50 | Cost: 19716.40 | Train/Valid Acc.: 58.55%/58.46% \n",
            "25/50 | Cost: 19017.97 | Train/Valid Acc.: 59.15%/59.28% \n",
            "26/50 | Cost: 18443.12 | Train/Valid Acc.: 59.54%/59.72% \n",
            "27/50 | Cost: 17888.81 | Train/Valid Acc.: 60.80%/60.88% \n",
            "28/50 | Cost: 17324.70 | Train/Valid Acc.: 61.87%/62.07% \n",
            "29/50 | Cost: 17031.19 | Train/Valid Acc.: 62.81%/62.68% \n",
            "30/50 | Cost: 16425.65 | Train/Valid Acc.: 63.49%/63.47% \n",
            "31/50 | Cost: 15992.30 | Train/Valid Acc.: 64.17%/64.18% \n",
            "32/50 | Cost: 15731.11 | Train/Valid Acc.: 65.05%/65.01% \n",
            "33/50 | Cost: 15248.81 | Train/Valid Acc.: 65.45%/65.38% \n",
            "34/50 | Cost: 14968.50 | Train/Valid Acc.: 66.36%/66.35% \n",
            "35/50 | Cost: 14713.93 | Train/Valid Acc.: 67.16%/66.94% \n",
            "36/50 | Cost: 14481.08 | Train/Valid Acc.: 67.29%/67.54% \n",
            "37/50 | Cost: 14141.44 | Train/Valid Acc.: 68.46%/68.41% \n",
            "38/50 | Cost: 13877.66 | Train/Valid Acc.: 69.09%/68.81% \n",
            "39/50 | Cost: 13610.16 | Train/Valid Acc.: 69.64%/69.46% \n",
            "40/50 | Cost: 13398.79 | Train/Valid Acc.: 70.12%/69.82% \n",
            "41/50 | Cost: 13239.80 | Train/Valid Acc.: 70.90%/70.40% \n",
            "42/50 | Cost: 13026.09 | Train/Valid Acc.: 71.16%/70.80% \n",
            "43/50 | Cost: 12692.87 | Train/Valid Acc.: 71.66%/71.51% \n",
            "44/50 | Cost: 12614.36 | Train/Valid Acc.: 71.86%/71.60% \n",
            "45/50 | Cost: 12391.54 | Train/Valid Acc.: 72.51%/72.34% \n",
            "46/50 | Cost: 12158.79 | Train/Valid Acc.: 73.12%/73.00% \n",
            "47/50 | Cost: 12169.02 | Train/Valid Acc.: 73.29%/73.08% \n",
            "48/50 | Cost: 12012.36 | Train/Valid Acc.: 73.58%/73.37% \n",
            "49/50 | Cost: 11747.59 | Train/Valid Acc.: 73.99%/73.72% \n",
            "50/50 | Cost: 11624.22 | Train/Valid Acc.: 74.48%/74.13% \n",
            "1/50 | Cost: 31668.19 | Train/Valid Acc.: 25.95%/25.91% \n",
            "2/50 | Cost: 30583.49 | Train/Valid Acc.: 33.95%/33.92% \n",
            "3/50 | Cost: 29411.12 | Train/Valid Acc.: 38.99%/38.79% \n",
            "4/50 | Cost: 28376.13 | Train/Valid Acc.: 47.12%/46.74% \n",
            "5/50 | Cost: 27252.60 | Train/Valid Acc.: 46.41%/46.07% \n",
            "6/50 | Cost: 26350.97 | Train/Valid Acc.: 55.55%/55.12% \n",
            "7/50 | Cost: 25525.65 | Train/Valid Acc.: 54.64%/53.89% \n",
            "8/50 | Cost: 24792.80 | Train/Valid Acc.: 56.24%/55.60% \n",
            "9/50 | Cost: 24036.72 | Train/Valid Acc.: 57.97%/57.55% \n",
            "10/50 | Cost: 23436.82 | Train/Valid Acc.: 54.78%/54.11% \n",
            "11/50 | Cost: 22965.96 | Train/Valid Acc.: 58.13%/57.50% \n",
            "12/50 | Cost: 22448.05 | Train/Valid Acc.: 59.69%/59.13% \n",
            "13/50 | Cost: 22032.95 | Train/Valid Acc.: 58.63%/58.12% \n",
            "14/50 | Cost: 21543.02 | Train/Valid Acc.: 61.77%/61.12% \n",
            "15/50 | Cost: 21064.38 | Train/Valid Acc.: 59.28%/58.55% \n",
            "16/50 | Cost: 20709.90 | Train/Valid Acc.: 60.37%/60.00% \n",
            "17/50 | Cost: 20239.56 | Train/Valid Acc.: 60.76%/60.41% \n",
            "18/50 | Cost: 19885.90 | Train/Valid Acc.: 62.69%/62.20% \n",
            "19/50 | Cost: 19496.36 | Train/Valid Acc.: 62.59%/62.21% \n",
            "20/50 | Cost: 19154.53 | Train/Valid Acc.: 63.19%/62.83% \n",
            "21/50 | Cost: 18836.87 | Train/Valid Acc.: 64.06%/63.61% \n",
            "22/50 | Cost: 18520.03 | Train/Valid Acc.: 63.66%/63.34% \n",
            "23/50 | Cost: 18125.59 | Train/Valid Acc.: 64.25%/64.04% \n",
            "24/50 | Cost: 17847.68 | Train/Valid Acc.: 63.98%/63.80% \n",
            "25/50 | Cost: 17516.72 | Train/Valid Acc.: 64.65%/64.41% \n",
            "26/50 | Cost: 17216.79 | Train/Valid Acc.: 64.64%/64.43% \n",
            "27/50 | Cost: 16920.52 | Train/Valid Acc.: 65.43%/65.15% \n",
            "28/50 | Cost: 16703.96 | Train/Valid Acc.: 65.53%/65.30% \n",
            "29/50 | Cost: 16416.72 | Train/Valid Acc.: 66.39%/66.26% \n",
            "30/50 | Cost: 16182.65 | Train/Valid Acc.: 66.15%/66.03% \n",
            "31/50 | Cost: 16022.27 | Train/Valid Acc.: 66.99%/66.89% \n",
            "32/50 | Cost: 15780.09 | Train/Valid Acc.: 67.65%/67.35% \n",
            "33/50 | Cost: 15547.39 | Train/Valid Acc.: 67.90%/67.51% \n",
            "34/50 | Cost: 15346.22 | Train/Valid Acc.: 68.05%/67.84% \n",
            "35/50 | Cost: 15165.56 | Train/Valid Acc.: 68.37%/68.03% \n",
            "36/50 | Cost: 14923.50 | Train/Valid Acc.: 68.85%/68.48% \n",
            "37/50 | Cost: 14824.13 | Train/Valid Acc.: 68.93%/68.65% \n",
            "38/50 | Cost: 14606.86 | Train/Valid Acc.: 69.62%/69.24% \n",
            "39/50 | Cost: 14444.65 | Train/Valid Acc.: 69.83%/69.37% \n",
            "40/50 | Cost: 14251.87 | Train/Valid Acc.: 70.20%/69.57% \n",
            "41/50 | Cost: 14206.68 | Train/Valid Acc.: 70.39%/69.83% \n",
            "42/50 | Cost: 14141.27 | Train/Valid Acc.: 70.77%/70.31% \n",
            "43/50 | Cost: 13886.52 | Train/Valid Acc.: 70.99%/70.54% \n",
            "44/50 | Cost: 13803.78 | Train/Valid Acc.: 71.23%/70.64% \n",
            "45/50 | Cost: 13613.85 | Train/Valid Acc.: 71.58%/71.03% \n",
            "46/50 | Cost: 13533.76 | Train/Valid Acc.: 71.77%/71.21% \n",
            "47/50 | Cost: 13249.06 | Train/Valid Acc.: 72.20%/71.64% \n",
            "48/50 | Cost: 13369.46 | Train/Valid Acc.: 72.17%/71.56% \n",
            "49/50 | Cost: 13238.75 | Train/Valid Acc.: 72.29%/71.70% \n",
            "50/50 | Cost: 13031.60 | Train/Valid Acc.: 72.82%/72.19% \n",
            "1/50 | Cost: 32314.39 | Train/Valid Acc.: 21.39%/21.12% \n",
            "2/50 | Cost: 31409.32 | Train/Valid Acc.: 23.18%/23.17% \n",
            "3/50 | Cost: 30274.65 | Train/Valid Acc.: 39.19%/38.70% \n",
            "4/50 | Cost: 29284.09 | Train/Valid Acc.: 44.81%/44.56% \n",
            "5/50 | Cost: 28396.76 | Train/Valid Acc.: 48.41%/48.54% \n",
            "6/50 | Cost: 27536.66 | Train/Valid Acc.: 48.53%/48.69% \n",
            "7/50 | Cost: 26882.18 | Train/Valid Acc.: 53.73%/54.19% \n",
            "8/50 | Cost: 26260.49 | Train/Valid Acc.: 54.13%/54.57% \n",
            "9/50 | Cost: 25657.32 | Train/Valid Acc.: 55.07%/55.34% \n",
            "10/50 | Cost: 25072.65 | Train/Valid Acc.: 57.30%/57.21% \n",
            "11/50 | Cost: 24566.49 | Train/Valid Acc.: 56.96%/56.87% \n",
            "12/50 | Cost: 24172.90 | Train/Valid Acc.: 58.71%/58.47% \n",
            "13/50 | Cost: 23665.52 | Train/Valid Acc.: 59.31%/59.48% \n",
            "14/50 | Cost: 23296.33 | Train/Valid Acc.: 59.06%/59.13% \n",
            "15/50 | Cost: 22833.02 | Train/Valid Acc.: 59.31%/59.52% \n",
            "16/50 | Cost: 22331.00 | Train/Valid Acc.: 60.43%/60.55% \n",
            "17/50 | Cost: 21797.50 | Train/Valid Acc.: 60.83%/61.04% \n",
            "18/50 | Cost: 21297.68 | Train/Valid Acc.: 61.61%/61.73% \n",
            "19/50 | Cost: 20819.34 | Train/Valid Acc.: 61.37%/61.56% \n",
            "20/50 | Cost: 20305.24 | Train/Valid Acc.: 62.53%/62.37% \n",
            "21/50 | Cost: 19869.98 | Train/Valid Acc.: 62.99%/63.01% \n",
            "22/50 | Cost: 19411.25 | Train/Valid Acc.: 63.23%/63.25% \n",
            "23/50 | Cost: 18932.15 | Train/Valid Acc.: 63.90%/64.02% \n",
            "24/50 | Cost: 18462.57 | Train/Valid Acc.: 64.37%/64.26% \n",
            "25/50 | Cost: 18125.01 | Train/Valid Acc.: 64.69%/64.46% \n",
            "26/50 | Cost: 17744.67 | Train/Valid Acc.: 65.63%/65.56% \n",
            "27/50 | Cost: 17322.52 | Train/Valid Acc.: 66.39%/66.23% \n",
            "28/50 | Cost: 16999.06 | Train/Valid Acc.: 66.38%/66.46% \n",
            "29/50 | Cost: 16662.92 | Train/Valid Acc.: 67.16%/66.95% \n",
            "30/50 | Cost: 16380.28 | Train/Valid Acc.: 67.55%/67.36% \n",
            "31/50 | Cost: 16110.44 | Train/Valid Acc.: 67.86%/67.69% \n",
            "32/50 | Cost: 15809.80 | Train/Valid Acc.: 68.54%/68.38% \n",
            "33/50 | Cost: 15579.91 | Train/Valid Acc.: 68.84%/68.61% \n",
            "34/50 | Cost: 15344.82 | Train/Valid Acc.: 69.70%/69.38% \n",
            "35/50 | Cost: 15015.90 | Train/Valid Acc.: 69.84%/69.45% \n",
            "36/50 | Cost: 14865.95 | Train/Valid Acc.: 69.84%/69.60% \n",
            "37/50 | Cost: 14657.23 | Train/Valid Acc.: 70.47%/70.28% \n",
            "38/50 | Cost: 14445.33 | Train/Valid Acc.: 70.72%/70.65% \n",
            "39/50 | Cost: 14163.40 | Train/Valid Acc.: 71.27%/70.94% \n",
            "40/50 | Cost: 14070.21 | Train/Valid Acc.: 71.31%/71.27% \n",
            "41/50 | Cost: 13830.07 | Train/Valid Acc.: 71.89%/71.65% \n",
            "42/50 | Cost: 13721.59 | Train/Valid Acc.: 72.00%/71.77% \n",
            "43/50 | Cost: 13523.13 | Train/Valid Acc.: 72.37%/72.29% \n",
            "44/50 | Cost: 13349.40 | Train/Valid Acc.: 72.52%/72.51% \n",
            "45/50 | Cost: 13261.34 | Train/Valid Acc.: 72.89%/72.65% \n",
            "46/50 | Cost: 13093.06 | Train/Valid Acc.: 73.19%/72.99% \n",
            "47/50 | Cost: 12992.17 | Train/Valid Acc.: 73.18%/73.23% \n",
            "48/50 | Cost: 12883.34 | Train/Valid Acc.: 73.52%/73.37% \n",
            "49/50 | Cost: 12788.53 | Train/Valid Acc.: 73.78%/73.75% \n",
            "50/50 | Cost: 12633.76 | Train/Valid Acc.: 74.02%/73.91% \n",
            "1/50 | Cost: 31971.52 | Train/Valid Acc.: 22.88%/22.57% \n",
            "2/50 | Cost: 31082.21 | Train/Valid Acc.: 35.93%/35.25% \n",
            "3/50 | Cost: 29961.33 | Train/Valid Acc.: 44.41%/44.16% \n",
            "4/50 | Cost: 29074.18 | Train/Valid Acc.: 48.86%/48.74% \n",
            "5/50 | Cost: 28296.26 | Train/Valid Acc.: 51.87%/51.63% \n",
            "6/50 | Cost: 27530.73 | Train/Valid Acc.: 52.88%/52.88% \n",
            "7/50 | Cost: 26814.48 | Train/Valid Acc.: 56.06%/56.04% \n",
            "8/50 | Cost: 26166.16 | Train/Valid Acc.: 53.84%/53.85% \n",
            "9/50 | Cost: 25492.97 | Train/Valid Acc.: 55.26%/55.49% \n",
            "10/50 | Cost: 24926.23 | Train/Valid Acc.: 54.45%/54.42% \n",
            "11/50 | Cost: 24373.01 | Train/Valid Acc.: 56.33%/56.04% \n",
            "12/50 | Cost: 24024.04 | Train/Valid Acc.: 56.75%/56.68% \n",
            "13/50 | Cost: 23519.62 | Train/Valid Acc.: 56.71%/56.36% \n",
            "14/50 | Cost: 23020.11 | Train/Valid Acc.: 56.94%/56.49% \n",
            "15/50 | Cost: 22496.24 | Train/Valid Acc.: 56.84%/56.51% \n",
            "16/50 | Cost: 21973.41 | Train/Valid Acc.: 58.56%/58.17% \n",
            "17/50 | Cost: 21537.00 | Train/Valid Acc.: 58.26%/57.71% \n",
            "18/50 | Cost: 21025.14 | Train/Valid Acc.: 58.21%/57.63% \n",
            "19/50 | Cost: 20513.05 | Train/Valid Acc.: 59.22%/59.07% \n",
            "20/50 | Cost: 20064.55 | Train/Valid Acc.: 60.07%/59.91% \n",
            "21/50 | Cost: 19582.09 | Train/Valid Acc.: 61.05%/60.99% \n",
            "22/50 | Cost: 19144.58 | Train/Valid Acc.: 61.94%/61.81% \n",
            "23/50 | Cost: 18756.31 | Train/Valid Acc.: 62.61%/62.32% \n",
            "24/50 | Cost: 18286.48 | Train/Valid Acc.: 63.13%/62.90% \n",
            "25/50 | Cost: 18009.27 | Train/Valid Acc.: 63.69%/63.59% \n",
            "26/50 | Cost: 17547.50 | Train/Valid Acc.: 64.49%/64.33% \n",
            "27/50 | Cost: 17205.12 | Train/Valid Acc.: 65.03%/64.64% \n",
            "28/50 | Cost: 16895.68 | Train/Valid Acc.: 65.87%/65.66% \n",
            "29/50 | Cost: 16563.26 | Train/Valid Acc.: 66.29%/66.02% \n",
            "30/50 | Cost: 16253.08 | Train/Valid Acc.: 66.46%/66.24% \n",
            "31/50 | Cost: 16030.71 | Train/Valid Acc.: 66.92%/66.62% \n",
            "32/50 | Cost: 15754.98 | Train/Valid Acc.: 67.42%/67.10% \n",
            "33/50 | Cost: 15462.41 | Train/Valid Acc.: 67.99%/67.74% \n",
            "34/50 | Cost: 15200.61 | Train/Valid Acc.: 68.17%/67.98% \n",
            "35/50 | Cost: 15068.49 | Train/Valid Acc.: 68.43%/68.29% \n",
            "36/50 | Cost: 14764.93 | Train/Valid Acc.: 68.84%/68.57% \n",
            "37/50 | Cost: 14536.55 | Train/Valid Acc.: 69.18%/69.03% \n",
            "38/50 | Cost: 14383.39 | Train/Valid Acc.: 69.49%/69.38% \n",
            "39/50 | Cost: 14153.95 | Train/Valid Acc.: 69.81%/69.59% \n",
            "40/50 | Cost: 14005.38 | Train/Valid Acc.: 69.86%/69.85% \n",
            "41/50 | Cost: 13813.24 | Train/Valid Acc.: 70.34%/70.26% \n",
            "42/50 | Cost: 13752.20 | Train/Valid Acc.: 70.39%/70.30% \n",
            "43/50 | Cost: 13569.50 | Train/Valid Acc.: 70.74%/70.70% \n",
            "44/50 | Cost: 13455.41 | Train/Valid Acc.: 70.87%/70.59% \n",
            "45/50 | Cost: 13296.50 | Train/Valid Acc.: 71.03%/70.82% \n",
            "46/50 | Cost: 13183.19 | Train/Valid Acc.: 71.52%/71.39% \n",
            "47/50 | Cost: 12936.70 | Train/Valid Acc.: 71.65%/71.50% \n",
            "48/50 | Cost: 12908.90 | Train/Valid Acc.: 71.89%/71.78% \n",
            "49/50 | Cost: 12770.29 | Train/Valid Acc.: 71.69%/71.40% \n",
            "50/50 | Cost: 12712.06 | Train/Valid Acc.: 72.21%/72.05% \n",
            "1/50 | Cost: 29358.99 | Train/Valid Acc.: 38.06%/37.60% \n",
            "2/50 | Cost: 27627.10 | Train/Valid Acc.: 55.07%/54.29% \n",
            "3/50 | Cost: 26915.95 | Train/Valid Acc.: 53.99%/53.53% \n",
            "4/50 | Cost: 26590.06 | Train/Valid Acc.: 49.16%/48.99% \n",
            "5/50 | Cost: 26487.38 | Train/Valid Acc.: 47.20%/46.89% \n",
            "6/50 | Cost: 26368.27 | Train/Valid Acc.: 34.63%/34.35% \n",
            "7/50 | Cost: 26022.82 | Train/Valid Acc.: 34.02%/33.94% \n",
            "8/50 | Cost: 26120.08 | Train/Valid Acc.: 17.71%/17.79% \n",
            "9/50 | Cost: 26119.17 | Train/Valid Acc.: 13.97%/14.12% \n",
            "10/50 | Cost: 26035.10 | Train/Valid Acc.: 11.17%/11.17% \n",
            "11/50 | Cost: 26292.24 | Train/Valid Acc.: 11.17%/11.17% \n",
            "12/50 | Cost: 26261.36 | Train/Valid Acc.: 11.17%/11.17% \n",
            "13/50 | Cost: 26326.68 | Train/Valid Acc.: 11.17%/11.17% \n",
            "14/50 | Cost: 26330.23 | Train/Valid Acc.: 11.17%/11.17% \n",
            "15/50 | Cost: 26450.49 | Train/Valid Acc.: 11.17%/11.17% \n",
            "16/50 | Cost: 26583.64 | Train/Valid Acc.: 11.17%/11.17% \n",
            "17/50 | Cost: 26471.19 | Train/Valid Acc.: 11.17%/11.17% \n",
            "18/50 | Cost: 26512.20 | Train/Valid Acc.: 11.17%/11.17% \n",
            "19/50 | Cost: 26562.70 | Train/Valid Acc.: 11.17%/11.17% \n",
            "20/50 | Cost: 26706.19 | Train/Valid Acc.: 11.17%/11.17% \n",
            "21/50 | Cost: 26859.08 | Train/Valid Acc.: 11.17%/11.17% \n",
            "22/50 | Cost: 26749.47 | Train/Valid Acc.: 11.17%/11.17% \n",
            "23/50 | Cost: 26789.42 | Train/Valid Acc.: 11.17%/11.17% \n",
            "24/50 | Cost: 26825.31 | Train/Valid Acc.: 11.17%/11.17% \n",
            "25/50 | Cost: 26651.39 | Train/Valid Acc.: 11.17%/11.17% \n",
            "26/50 | Cost: 26863.48 | Train/Valid Acc.: 11.17%/11.17% \n",
            "27/50 | Cost: 27091.22 | Train/Valid Acc.: 11.17%/11.17% \n",
            "28/50 | Cost: 26964.29 | Train/Valid Acc.: 11.17%/11.17% \n",
            "29/50 | Cost: 27158.75 | Train/Valid Acc.: 11.17%/11.17% \n",
            "30/50 | Cost: 27019.96 | Train/Valid Acc.: 11.17%/11.17% \n",
            "31/50 | Cost: 27084.02 | Train/Valid Acc.: 11.17%/11.17% \n",
            "32/50 | Cost: 27271.82 | Train/Valid Acc.: 11.17%/11.17% \n",
            "33/50 | Cost: 27381.91 | Train/Valid Acc.: 11.17%/11.17% \n",
            "34/50 | Cost: 27391.96 | Train/Valid Acc.: 11.17%/11.17% \n",
            "35/50 | Cost: 27442.37 | Train/Valid Acc.: 11.17%/11.17% \n",
            "36/50 | Cost: 27641.94 | Train/Valid Acc.: 11.17%/11.17% \n",
            "37/50 | Cost: 27860.58 | Train/Valid Acc.: 11.17%/11.17% \n",
            "38/50 | Cost: 27875.27 | Train/Valid Acc.: 11.17%/11.17% \n",
            "39/50 | Cost: 27904.14 | Train/Valid Acc.: 11.17%/11.17% \n",
            "40/50 | Cost: 28341.62 | Train/Valid Acc.: 11.17%/11.17% \n",
            "41/50 | Cost: 28113.41 | Train/Valid Acc.: 11.17%/11.17% \n",
            "42/50 | Cost: 28294.54 | Train/Valid Acc.: 11.17%/11.17% \n",
            "43/50 | Cost: 28430.99 | Train/Valid Acc.: 11.17%/11.17% \n",
            "44/50 | Cost: 28635.56 | Train/Valid Acc.: 11.17%/11.17% \n",
            "45/50 | Cost: 28903.22 | Train/Valid Acc.: 11.17%/11.17% \n",
            "46/50 | Cost: 28885.00 | Train/Valid Acc.: 11.17%/11.17% \n",
            "47/50 | Cost: 29270.33 | Train/Valid Acc.: 11.17%/11.17% \n",
            "48/50 | Cost: 29508.76 | Train/Valid Acc.: 11.17%/11.17% \n",
            "49/50 | Cost: 29851.64 | Train/Valid Acc.: 11.17%/11.17% \n",
            "50/50 | Cost: 30152.14 | Train/Valid Acc.: 11.17%/11.17% \n",
            "1/50 | Cost: 29464.13 | Train/Valid Acc.: 33.40%/33.14% \n",
            "2/50 | Cost: 27825.14 | Train/Valid Acc.: 43.57%/43.41% \n",
            "3/50 | Cost: 27297.05 | Train/Valid Acc.: 41.24%/41.17% \n",
            "4/50 | Cost: 27003.20 | Train/Valid Acc.: 43.42%/43.24% \n",
            "5/50 | Cost: 26634.79 | Train/Valid Acc.: 25.50%/25.37% \n",
            "6/50 | Cost: 26493.63 | Train/Valid Acc.: 24.12%/24.42% \n",
            "7/50 | Cost: 26147.62 | Train/Valid Acc.: 17.64%/17.65% \n",
            "8/50 | Cost: 25928.20 | Train/Valid Acc.: 19.39%/19.05% \n",
            "9/50 | Cost: 26004.05 | Train/Valid Acc.: 14.48%/14.44% \n",
            "10/50 | Cost: 26157.25 | Train/Valid Acc.: 11.42%/11.42% \n",
            "11/50 | Cost: 26114.61 | Train/Valid Acc.: 11.42%/11.42% \n",
            "12/50 | Cost: 26160.49 | Train/Valid Acc.: 11.42%/11.42% \n",
            "13/50 | Cost: 26298.66 | Train/Valid Acc.: 11.42%/11.42% \n",
            "14/50 | Cost: 26498.57 | Train/Valid Acc.: 11.42%/11.42% \n",
            "15/50 | Cost: 26552.50 | Train/Valid Acc.: 11.42%/11.42% \n",
            "16/50 | Cost: 26513.66 | Train/Valid Acc.: 11.42%/11.42% \n",
            "17/50 | Cost: 26602.50 | Train/Valid Acc.: 11.42%/11.42% \n",
            "18/50 | Cost: 26638.96 | Train/Valid Acc.: 11.42%/11.42% \n",
            "19/50 | Cost: 26634.08 | Train/Valid Acc.: 11.42%/11.42% \n",
            "20/50 | Cost: 26684.89 | Train/Valid Acc.: 11.42%/11.42% \n",
            "21/50 | Cost: 26711.58 | Train/Valid Acc.: 11.42%/11.42% \n",
            "22/50 | Cost: 26798.86 | Train/Valid Acc.: 11.42%/11.42% \n",
            "23/50 | Cost: 26669.46 | Train/Valid Acc.: 11.42%/11.42% \n",
            "24/50 | Cost: 26928.07 | Train/Valid Acc.: 11.42%/11.42% \n",
            "25/50 | Cost: 26762.62 | Train/Valid Acc.: 11.42%/11.42% \n",
            "26/50 | Cost: 26946.06 | Train/Valid Acc.: 10.46%/10.46% \n",
            "27/50 | Cost: 26822.05 | Train/Valid Acc.: 11.42%/11.42% \n",
            "28/50 | Cost: 26827.61 | Train/Valid Acc.: 11.42%/11.42% \n",
            "29/50 | Cost: 27050.11 | Train/Valid Acc.: 11.42%/11.42% \n",
            "30/50 | Cost: 27102.45 | Train/Valid Acc.: 11.42%/11.42% \n",
            "31/50 | Cost: 27151.34 | Train/Valid Acc.: 11.42%/11.42% \n",
            "32/50 | Cost: 27345.43 | Train/Valid Acc.: 11.42%/11.42% \n",
            "33/50 | Cost: 27376.89 | Train/Valid Acc.: 11.42%/11.42% \n",
            "34/50 | Cost: 27342.46 | Train/Valid Acc.: 11.42%/11.42% \n",
            "35/50 | Cost: 27488.82 | Train/Valid Acc.: 11.42%/11.42% \n",
            "36/50 | Cost: 27625.02 | Train/Valid Acc.: 11.42%/11.42% \n",
            "37/50 | Cost: 27611.02 | Train/Valid Acc.: 11.42%/11.42% \n",
            "38/50 | Cost: 27762.08 | Train/Valid Acc.: 11.42%/11.42% \n",
            "39/50 | Cost: 27864.28 | Train/Valid Acc.: 11.42%/11.42% \n",
            "40/50 | Cost: 28058.67 | Train/Valid Acc.: 11.42%/11.42% \n",
            "41/50 | Cost: 28199.48 | Train/Valid Acc.: 11.42%/11.42% \n",
            "42/50 | Cost: 28336.15 | Train/Valid Acc.: 11.42%/11.42% \n",
            "43/50 | Cost: 28358.48 | Train/Valid Acc.: 11.42%/11.42% \n",
            "44/50 | Cost: 28593.32 | Train/Valid Acc.: 11.42%/11.42% \n",
            "45/50 | Cost: 28785.51 | Train/Valid Acc.: 11.42%/11.42% \n",
            "46/50 | Cost: 29087.52 | Train/Valid Acc.: 11.42%/11.42% \n",
            "47/50 | Cost: 29353.83 | Train/Valid Acc.: 11.42%/11.42% \n",
            "48/50 | Cost: 29583.81 | Train/Valid Acc.: 11.42%/11.42% \n",
            "49/50 | Cost: 30124.25 | Train/Valid Acc.: 11.42%/11.42% \n",
            "50/50 | Cost: 30382.25 | Train/Valid Acc.: 11.42%/11.42% \n",
            "1/50 | Cost: 29841.48 | Train/Valid Acc.: 32.67%/31.99% \n",
            "2/50 | Cost: 28162.71 | Train/Valid Acc.: 41.10%/40.67% \n",
            "3/50 | Cost: 27166.81 | Train/Valid Acc.: 46.85%/46.83% \n",
            "4/50 | Cost: 26948.01 | Train/Valid Acc.: 45.44%/45.32% \n",
            "5/50 | Cost: 26552.07 | Train/Valid Acc.: 47.55%/47.54% \n",
            "6/50 | Cost: 26274.20 | Train/Valid Acc.: 43.70%/43.67% \n",
            "7/50 | Cost: 26193.05 | Train/Valid Acc.: 24.53%/24.73% \n",
            "8/50 | Cost: 26117.29 | Train/Valid Acc.: 22.23%/22.36% \n",
            "9/50 | Cost: 25985.97 | Train/Valid Acc.: 16.31%/16.43% \n",
            "10/50 | Cost: 26111.11 | Train/Valid Acc.: 13.22%/13.10% \n",
            "11/50 | Cost: 26003.12 | Train/Valid Acc.: 11.17%/11.18% \n",
            "12/50 | Cost: 26282.06 | Train/Valid Acc.: 11.17%/11.18% \n",
            "13/50 | Cost: 26265.16 | Train/Valid Acc.: 11.17%/11.18% \n",
            "14/50 | Cost: 26372.64 | Train/Valid Acc.: 11.17%/11.18% \n",
            "15/50 | Cost: 26469.68 | Train/Valid Acc.: 11.17%/11.18% \n",
            "16/50 | Cost: 26506.05 | Train/Valid Acc.: 11.17%/11.18% \n",
            "17/50 | Cost: 26617.50 | Train/Valid Acc.: 11.17%/11.18% \n",
            "18/50 | Cost: 26430.66 | Train/Valid Acc.: 11.17%/11.18% \n",
            "19/50 | Cost: 26718.59 | Train/Valid Acc.: 11.17%/11.18% \n",
            "20/50 | Cost: 26697.01 | Train/Valid Acc.: 11.17%/11.18% \n",
            "21/50 | Cost: 26690.92 | Train/Valid Acc.: 11.17%/11.18% \n",
            "22/50 | Cost: 26744.94 | Train/Valid Acc.: 11.17%/11.18% \n",
            "23/50 | Cost: 26957.19 | Train/Valid Acc.: 11.17%/11.18% \n",
            "24/50 | Cost: 26897.46 | Train/Valid Acc.: 11.17%/11.18% \n",
            "25/50 | Cost: 26934.54 | Train/Valid Acc.: 11.17%/11.18% \n",
            "26/50 | Cost: 27081.54 | Train/Valid Acc.: 10.47%/10.47% \n",
            "27/50 | Cost: 27103.71 | Train/Valid Acc.: 11.17%/11.18% \n",
            "28/50 | Cost: 27130.24 | Train/Valid Acc.: 11.17%/11.18% \n",
            "29/50 | Cost: 27225.47 | Train/Valid Acc.: 11.17%/11.18% \n",
            "30/50 | Cost: 27174.93 | Train/Valid Acc.: 11.17%/11.18% \n",
            "31/50 | Cost: 27225.02 | Train/Valid Acc.: 11.17%/11.18% \n",
            "32/50 | Cost: 27397.05 | Train/Valid Acc.: 11.17%/11.18% \n",
            "33/50 | Cost: 27335.41 | Train/Valid Acc.: 11.17%/11.18% \n",
            "34/50 | Cost: 27493.63 | Train/Valid Acc.: 11.17%/11.18% \n",
            "35/50 | Cost: 27621.83 | Train/Valid Acc.: 11.17%/11.18% \n",
            "36/50 | Cost: 27547.02 | Train/Valid Acc.: 11.17%/11.18% \n",
            "37/50 | Cost: 27946.96 | Train/Valid Acc.: 11.17%/11.18% \n",
            "38/50 | Cost: 28157.22 | Train/Valid Acc.: 11.17%/11.18% \n",
            "39/50 | Cost: 28040.19 | Train/Valid Acc.: 11.17%/11.18% \n",
            "40/50 | Cost: 28060.80 | Train/Valid Acc.: 11.17%/11.18% \n",
            "41/50 | Cost: 28177.59 | Train/Valid Acc.: 11.17%/11.18% \n",
            "42/50 | Cost: 28496.67 | Train/Valid Acc.: 11.17%/11.18% \n",
            "43/50 | Cost: 28446.51 | Train/Valid Acc.: 11.17%/11.18% \n",
            "44/50 | Cost: 28620.41 | Train/Valid Acc.: 11.17%/11.18% \n",
            "45/50 | Cost: 28951.64 | Train/Valid Acc.: 11.17%/11.18% \n",
            "46/50 | Cost: 29239.15 | Train/Valid Acc.: 11.17%/11.18% \n",
            "47/50 | Cost: 29268.18 | Train/Valid Acc.: 11.17%/11.18% \n",
            "48/50 | Cost: 29780.23 | Train/Valid Acc.: 11.17%/11.18% \n",
            "49/50 | Cost: 30102.21 | Train/Valid Acc.: 11.17%/11.18% \n",
            "50/50 | Cost: 30562.01 | Train/Valid Acc.: 11.17%/11.18% \n",
            "1/50 | Cost: 32064.70 | Train/Valid Acc.: 12.98%/12.92% \n",
            "2/50 | Cost: 31564.10 | Train/Valid Acc.: 20.00%/19.88% \n",
            "3/50 | Cost: 31218.46 | Train/Valid Acc.: 43.89%/43.21% \n",
            "4/50 | Cost: 30824.24 | Train/Valid Acc.: 22.04%/21.48% \n",
            "5/50 | Cost: 30629.48 | Train/Valid Acc.: 24.44%/23.89% \n",
            "6/50 | Cost: 30485.17 | Train/Valid Acc.: 26.52%/25.84% \n",
            "7/50 | Cost: 30378.09 | Train/Valid Acc.: 26.12%/25.33% \n",
            "8/50 | Cost: 30393.08 | Train/Valid Acc.: 28.97%/28.28% \n",
            "9/50 | Cost: 30413.62 | Train/Valid Acc.: 22.61%/22.08% \n",
            "10/50 | Cost: 30484.41 | Train/Valid Acc.: 27.08%/26.44% \n",
            "11/50 | Cost: 30636.09 | Train/Valid Acc.: 20.83%/20.38% \n",
            "12/50 | Cost: 30771.57 | Train/Valid Acc.: 20.73%/20.46% \n",
            "13/50 | Cost: 30751.12 | Train/Valid Acc.: 22.79%/22.14% \n",
            "14/50 | Cost: 30780.90 | Train/Valid Acc.: 23.83%/23.57% \n",
            "15/50 | Cost: 30768.64 | Train/Valid Acc.: 28.97%/28.99% \n",
            "16/50 | Cost: 30676.98 | Train/Valid Acc.: 27.90%/27.85% \n",
            "17/50 | Cost: 30572.11 | Train/Valid Acc.: 23.56%/23.69% \n",
            "18/50 | Cost: 30514.80 | Train/Valid Acc.: 22.81%/22.90% \n",
            "19/50 | Cost: 30236.64 | Train/Valid Acc.: 26.33%/26.54% \n",
            "20/50 | Cost: 30126.92 | Train/Valid Acc.: 23.55%/23.47% \n",
            "21/50 | Cost: 29950.92 | Train/Valid Acc.: 26.17%/26.38% \n",
            "22/50 | Cost: 29691.22 | Train/Valid Acc.: 25.40%/25.64% \n",
            "23/50 | Cost: 29473.77 | Train/Valid Acc.: 26.03%/26.29% \n",
            "24/50 | Cost: 29204.63 | Train/Valid Acc.: 26.38%/26.50% \n",
            "25/50 | Cost: 28969.73 | Train/Valid Acc.: 28.58%/28.71% \n",
            "26/50 | Cost: 28691.92 | Train/Valid Acc.: 29.20%/29.25% \n",
            "27/50 | Cost: 28370.42 | Train/Valid Acc.: 28.63%/28.62% \n",
            "28/50 | Cost: 28097.95 | Train/Valid Acc.: 29.51%/29.71% \n",
            "29/50 | Cost: 27824.03 | Train/Valid Acc.: 29.28%/29.31% \n",
            "30/50 | Cost: 27458.79 | Train/Valid Acc.: 30.86%/30.94% \n",
            "31/50 | Cost: 27333.63 | Train/Valid Acc.: 32.16%/32.21% \n",
            "32/50 | Cost: 26987.36 | Train/Valid Acc.: 31.13%/31.49% \n",
            "33/50 | Cost: 26778.39 | Train/Valid Acc.: 31.80%/32.02% \n",
            "34/50 | Cost: 26581.80 | Train/Valid Acc.: 31.92%/32.06% \n",
            "35/50 | Cost: 26395.88 | Train/Valid Acc.: 33.36%/33.49% \n",
            "36/50 | Cost: 26156.26 | Train/Valid Acc.: 33.30%/33.19% \n",
            "37/50 | Cost: 25842.46 | Train/Valid Acc.: 33.29%/33.25% \n"
          ]
        }
      ],
      "source": [
        "#parameters = None # CHANGE IT\n",
        "#grid = None # CHANGE IT\n",
        "#best_classifier = None # CHANGE IT\n",
        "#best_score = None # CHANGE IT\n",
        "\n",
        "\n",
        "\n",
        "from scipy.stats import randint\n",
        "param_dist = {\n",
        "    'classifier__l1': np.arange(0.001, 0.011, 0.001),\n",
        "    'classifier__l2': np.arange(0.001, 0.006, 0.001),\n",
        "    'classifier__n_hidden': np.arange(30, 110, 10),\n",
        "    'classifier__eta': np.arange(0.0001, 0.001, 0.002),\n",
        "    'classifier__init_technique': ['normal', 'xavier', 'he']\n",
        "}\n",
        "clf = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=10, cv=3, random_state=RANDOM_STATE)\n",
        "clf.fit(X_train, y_train)\n",
        "best_classifier = clf.best_estimator_\n",
        "best_score = clf.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zisqj2Qk6318"
      },
      "outputs": [],
      "source": [
        "# PRINT YOUR SCORES HERE\n",
        "print(best_classifier, best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9gSkzvszU1K"
      },
      "source": [
        "# END"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "652c03699ff13e80c7cc80288635a3de490fdf0df8057fb27f23cbe0dec25fa0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
